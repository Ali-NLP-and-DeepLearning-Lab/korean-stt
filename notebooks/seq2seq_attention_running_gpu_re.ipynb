{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 80\n",
    "fs = 44100\n",
    "frame_length_ms=50\n",
    "frame_shift_ms=25\n",
    "nsc = int(fs * frame_length_ms / 1000)\n",
    "nov = nsc - int(fs * frame_shift_ms / 1000)\n",
    "nhop = int(fs * frame_shift_ms / 1000)\n",
    "eps = 1e-8\n",
    "db_ref = 160\n",
    "\n",
    "meta_path = \"D:/korean-single-speaker-speech-dataset/transcript.v.1.2.txt\"\n",
    "data_folder = \"D:/korean-single-speaker-speech-dataset/kss\"\n",
    "\n",
    "with open(meta_path, encoding='utf-8') as f:\n",
    "    metadata = np.array([line.strip().split('|') for line in f])\n",
    "#     hours = sum((int(x[2]) for x in metadata)) * frame_shift_ms / (3600 * 1000)\n",
    "#     log('Loaded metadata for %d examples (%.2f hours)' % (len(metadata), hours))\n",
    "\n",
    "# metadata = metadata[:32, :2]\n",
    "\n",
    "max_sequence_len = max(list(map(len, metadata[:, 1])))\n",
    "\n",
    "error_jamos = [5868, 5998, 6046, 6155, 6202, \n",
    "               6654, 6890, 7486, 7502, 7744, \n",
    "               7765, 8267, 9069, 9927, 10437, \n",
    "               10515, 10533, 10606, 10610, 12777]\n",
    "\n",
    "print(metadata.shape)\n",
    "metadata = np.delete(metadata, error_jamos, axis = 0)\n",
    "print(metadata.shape)\n",
    "\n",
    "dataset_size = len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_length = list()\n",
    "file_length = list()\n",
    "division_length = list()\n",
    "unicode_jamo_list = list()\n",
    "\n",
    "for i in range(len(metadata)):\n",
    "    character_length.append(len(metadata[i, 3]))\n",
    "    file_length.append(float(metadata[i, 4]))\n",
    "    division_length.append(float(metadata[i, 4]) * 1000 / len(metadata[i, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_name_list = []\n",
    "\n",
    "for data in metadata:\n",
    "    wave_name_list.append(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unicode_jamo_list = list()\n",
    "for unicode in range(0x1100, 0x1113):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "    \n",
    "for unicode in range(0x1161, 0x1176):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "    \n",
    "for unicode in range(0x11A8, 0x11C3):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "    \n",
    "unicode_jamo_list += [' ', '!', ',', '.', '?', '<sos>', '<eos>']\n",
    "    \n",
    "unicode_jamo_list.sort()\n",
    "\n",
    "print(unicode_jamo_list)\n",
    "print(len(unicode_jamo_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, vocabs):\n",
    "        self.vocabs = vocabs\n",
    "        \n",
    "    def word2num(self, sentence):\n",
    "        tokens = list()\n",
    "        for char in sentence:\n",
    "            tokens.append(self.vocabs.index(char))    \n",
    "        return tokens\n",
    "        \n",
    "    def word2vec(self, sentence):\n",
    "        vectors = np.zeros((len(sentence), len(self.vocabs)))\n",
    "        for i, char in enumerate(sentence):\n",
    "            vectors[i, self.vocabs.index(char)] = 1   \n",
    "        return vectors\n",
    "    \n",
    "    def num2word(self, num):\n",
    "        output = list()\n",
    "        for i in num:\n",
    "            output.append(self.vocabs[i])\n",
    "        return output\n",
    "    \n",
    "    def num2vec(self, numbers):\n",
    "        vectors = np.zeros((len(numbers), len(self.vocabs)))\n",
    "        for i, num in enumerate(numbers):\n",
    "            vectors[i, num] = 1   \n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(unicode_jamo_list)\n",
    "jamo_tokens = tokenizer.word2num(unicode_jamo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(jamo_tokens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_path_list = list()\n",
    "\n",
    "for i, wav_name in enumerate(tqdm(wave_name_list)):\n",
    "    \n",
    "    npy_name = wav_name.replace('.wav', '.npy')\n",
    "    wav_path = os.path.join(data_folder, wav_name)  \n",
    "    mel_path = os.path.join(data_folder + '/mel', npy_name)\n",
    "    mel_path_list.append(mel_path)\n",
    "    \n",
    "    if not os.path.isfile(mel_path):\n",
    "#         print(\"{}\".format(mel_path))\n",
    "        y, sr = librosa.core.load(wav_path)\n",
    "        f, t, Zxx = sp.signal.stft(y, fs=sr, nperseg=nsc, noverlap=nov)\n",
    "        Sxx = np.abs(Zxx)\n",
    "        Sxx = np.maximum(Sxx, eps)\n",
    "\n",
    "        # plt.figure(figsize=(20,20))\n",
    "        # plt.imshow(20*np.log10(Sxx), origin='lower')\n",
    "        # plt.colorbar()\n",
    "        # plt.show()\n",
    "\n",
    "        mel_filters = librosa.filters.mel(sr=fs, n_fft=nsc, n_mels=n_mels)\n",
    "        mel_specgram = np.matmul(mel_filters, Sxx)\n",
    "\n",
    "    #   log_specgram = 20*np.log10(Sxx)\n",
    "    #   norm_log_specgram = (log_specgram + db_ref) / db_ref\n",
    "\n",
    "        log_mel_specgram = 20 * np.log10(np.maximum(mel_specgram, eps))\n",
    "        norm_log_mel_specgram = (log_mel_specgram + db_ref) / db_ref\n",
    "\n",
    "    #   np.save(specgram_path, norm_log_specgram)\n",
    "        np.save(mel_path, norm_log_mel_specgram)\n",
    "    #   np.save(specgram_path, Sxx)\n",
    "\n",
    "    #     print(norm_log_mel_specgram.shape[1])\n",
    "\n",
    "    #     if i % 1000 == 0:\n",
    "    #         plt.figure(figsize=(8, 4))\n",
    "    #         plt.imshow(20 * np.log10(Sxx), origin='lower', aspect='auto')\n",
    "    #         plt.colorbar()\n",
    "    #         plt.show()\n",
    "\n",
    "    #         plt.figure(figsize=(8, 4))\n",
    "    #         plt.imshow(norm_log_mel_specgram, origin='lower', aspect='auto')\n",
    "    #         plt.colorbar()\n",
    "    #         plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = nn.Embedding(len(jamo_tokens), 256)\n",
    "\n",
    "# print(metadata[5031, 3])\n",
    "# print(metadata[5031, 2])\n",
    "# print(len(metadata[5031, 3]))\n",
    "\n",
    "# input_token = tokenizer.word2num(metadata[5031, 3])\n",
    "# input_tensor = torch.tensor(input_token)\n",
    "# plt.imshow(embedding_layer(input_tensor).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGRU(nn.Module):\n",
    "    def __init__(self, D_in, bidirectional=True):\n",
    "        super(ResidualGRU, self).__init__()\n",
    "        self.gru = nn.GRU(D_in, int(D_in/2), bidirectional=bidirectional, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "#         print(\"Residual Input: {}\".format(input_tensor.shape))\n",
    "        gru_output, _ = self.gru(input_tensor)\n",
    "        activated = self.relu(gru_output)  \n",
    "#         print(\"Residual Output: {}\".format(activated.shape))\n",
    "        output_tensor = torch.add(activated, input_tensor)\n",
    "        \n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.H = H\n",
    "        self.fc = torch.nn.Linear(D_in, H)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc_2 = torch.nn.Linear(H, H)\n",
    "        self.relu_2 = torch.nn.ReLU()\n",
    "        self.dropout_2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.gru = nn.GRU(H, D_out, bidirectional=True, batch_first=True)\n",
    "        self.relu_gru = torch.nn.ReLU()\n",
    "        \n",
    "        self.residual_gru_layers = nn.ModuleList([ResidualGRU(H, bidirectional=True) for i in range(3)])\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        output_tensor = self.fc(input_tensor)\n",
    "        output_tensor = self.relu(output_tensor)\n",
    "        output_tensor = self.dropout(output_tensor)\n",
    "        \n",
    "        output_tensor, _ = self.gru(output_tensor)\n",
    "        output_tensor = self.relu_gru(output_tensor)\n",
    "        \n",
    "        for layer in self.residual_gru_layers:\n",
    "            output_tensor = layer(output_tensor)\n",
    "        \n",
    "        output_tensor = self.fc_2(output_tensor)\n",
    "        output_tensor = self.relu_2(output_tensor)\n",
    "        output_tensor = self.dropout_2(output_tensor)\n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, H, D_out):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.H = H\n",
    "        self.fc_embed = nn.Linear(256, 1024)\n",
    "        self.relu_embed = torch.nn.ReLU()\n",
    "        self.dropout_embed = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.gru = nn.GRU(2 * H, H, batch_first=True)\n",
    "#         self.attention = AdditiveAttentionModule(D_out * 2)\n",
    "        self.attention = MultiplicativeAttentionModule(D_out * 2)\n",
    "        self.fc = nn.Linear(1024, 512)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 74)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_tensor, query):\n",
    "#         print(\"Input tensor shape in Decoder: {}\".format(input_tensor.shape))\n",
    "#         print(\"Hidden_tensor shape in Decoder: {}\".format(hidden_tensor.shape))\n",
    "        output_tensor = self.fc_embed(input_tensor)\n",
    "        output_tensor = self.relu_embed(output_tensor)\n",
    "        output_tensor = self.dropout_embed(output_tensor)\n",
    "    \n",
    "#         print(\"Output tensor shape in Decoder: {}\".format(output_tensor.shape))\n",
    "#         print(\"Output Tensor Shape: {}\".format(output_tensor.shape))\n",
    "        output_tensor, hidden_tensor = self.gru(output_tensor, hidden_tensor)\n",
    "        \n",
    "#         print(\"Hidden_tensor shape in Decoder: {}\".format(hidden_tensor.shape))\n",
    "#         print(\"Output tensor shape in Decoder: {}\".format(output_tensor.shape))\n",
    "    \n",
    "        context_vector, alpha = self.attention(query, output_tensor)\n",
    "        output_tensor = torch.cat([output_tensor, context_vector], dim=2)\n",
    "#         print('output_tensor: {}'.format(output_tensor.shape))\n",
    "#         print('output_tensor: {}'.format(context_vector.shape))\n",
    "        output_tensor = self.fc(output_tensor)\n",
    "        output_tensor = self.relu(output_tensor)\n",
    "        output_tensor = self.dropout(output_tensor)        \n",
    "        \n",
    "        output_tensor = self.fc2(output_tensor)\n",
    "        prediction_tensor = self.softmax(output_tensor)\n",
    "\n",
    "        return prediction_tensor, hidden_tensor, context_vector, alpha\n",
    "\n",
    "class AdditiveAttentionModule(torch.nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(AdditiveAttentionModule, self).__init__()\n",
    "        self.fc_alpha = nn.Linear(H, 1)\n",
    "        self.W = nn.Linear(H, H)\n",
    "        self.V = nn.Linear(H, H)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, query, key):\n",
    "        output_tensor = torch.tanh(torch.add(self.W(query), self.V(key)))\n",
    "        e = self.fc_alpha(output_tensor)\n",
    "        e_sig = self.sigmoid(e)\n",
    "        alpha = self.softmax(e_sig).transpose(1, 2)\n",
    "        context_vector = torch.bmm(alpha, query)\n",
    "        \n",
    "        return context_vector, alpha\n",
    "    \n",
    "class MultiplicativeAttentionModule(torch.nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(MultiplicativeAttentionModule, self).__init__()\n",
    "#         self.W = nn.Linear(H, H)\n",
    "#         self.V = nn.Linear(H, H)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, query, key):\n",
    "#         output_tensor = torch.tanh(torch.bmm(self.W(query), self.V(key)))\n",
    "#         print(\"Query shape: {}\".format(query.shape))\n",
    "#         print(\"Key shape: {}\".format(key.shape))\n",
    "        output_tensor = torch.bmm(query, key.transpose(1, 2))\n",
    "#         print(\"Output shape: {}\".format(output_tensor.shape))\n",
    "#         e_sig = self.sigmoid(output_tensor)\n",
    "        alpha = self.softmax(output_tensor).transpose(1, 2)\n",
    "#         print('Alpha shape: {}'.format(alpha.shape))\n",
    "        context_vector = torch.bmm(alpha, query)\n",
    "        \n",
    "        return context_vector, alpha\n",
    "\n",
    "class Mel2SeqNet():\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Mel2SeqNet, self).__init__()\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.H = H\n",
    "        self.encoder = Encoder(D_in, H, D_out).to(device)\n",
    "        self.embedding_layer = nn.Embedding(len(jamo_tokens), 256).to(device)\n",
    "        self.decoder = Decoder(H, D_out).to(device)\n",
    "#         self.encoder_optimizer = optim.SGD(self.encoder.parameters(), lr=0.01)\n",
    "#         self.decoder_optimizer = optim.SGD(self.decoder.parameters(), lr=0.01)\n",
    "#         self.embedding_optimizer = optim.SGD(self.embedding_layer.parameters(), lr=0.01)\n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=0.01)\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=0.01)\n",
    "        self.embedding_optimizer = optim.Adam(self.embedding_layer.parameters(), lr=0.01)\n",
    "\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def train(self, input_tensor, ground_truth, loss_mask):\n",
    "        \n",
    "        batch_size = input_tensor.shape[0]\n",
    "\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        self.embedding_optimizer.zero_grad()\n",
    "\n",
    "        encoded_tensor = self.encoder(input_tensor)\n",
    "        decoder_hidden = encoded_tensor[:, -1, :].view(1, batch_size, self.H).contiguous()\n",
    "#         decoder_hidden = encoded_tensor[:, -1, :].transpose(0, 1)\n",
    "        \n",
    "        pred_tensor_list = list()\n",
    "        att_weight_list = list()\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        decoder_input = self.embedding_layer(torch.tensor(tokenizer.word2num(['<sos>'] * batch_size)).to(device)).view([batch_size, 1, -1])\n",
    "        \n",
    "#         print(encoded_tensor.shape)\n",
    "#         print(decoder_hidden.shape)\n",
    "#         print(decoder_input.shape)\n",
    "        \n",
    "        for i in range(ground_truth.shape[1]):\n",
    "            \n",
    "            pred_tensor, decoder_hidden, context_vector, att_weight = self.decoder(decoder_input, decoder_hidden, encoded_tensor)\n",
    "            pred_tensor_list.append(pred_tensor)\n",
    "            att_weight_list.append(att_weight)\n",
    "            \n",
    "#             print('pred_tensor shape: {}'.format(pred_tensor.shape))\n",
    "            truth = ground_truth[:, i]\n",
    "            truth = truth.type(torch.cuda.LongTensor)\n",
    "            \n",
    "            loss += torch.dot(loss_mask[:, i], self.criterion(pred_tensor.view([batch_size, -1]), truth)) / batch_size\n",
    "            decoder_input = self.embedding_layer(truth).view([batch_size, 1, -1])\n",
    "#             pred_tensor_list.append(torch.tensor(tokenizer.num2vec(truth)).view(batch_size, 1, -1))\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        self.embedding_optimizer.step()\n",
    "        \n",
    "        prediction = torch.cat(pred_tensor_list, dim=1)\n",
    "        attention_matrix = torch.cat(att_weight_list, dim=1)\n",
    "        \n",
    "        return prediction, attention_matrix, loss.item() / ground_truth.shape[1]\n",
    "    \n",
    "    def save(self, check_point_name):\n",
    "        torch.save({\n",
    "            'embedding_layer_state_dict': self.embedding_layer.state_dict(),\n",
    "            'encoder_state_dict': self.encoder.state_dict(),\n",
    "            'decoder_state_dict': self.decoder.state_dict(),\n",
    "            'embedding_optimizer_state_dict': self.embedding_optimizer.state_dict(),\n",
    "            'encoder_optimizer_state_dict': self.encoder_optimizer.state_dict(),\n",
    "            'decoder_optimizer_state_dict': self.decoder_optimizer.state_dict(),\n",
    "            }, check_point_name)\n",
    "    \n",
    "    def load(self, check_point_name):\n",
    "        checkpoint = torch.load(check_point_name)\n",
    "        self.embedding_layer.load_state_dict(checkpoint['embedding_layer_state_dict'])\n",
    "        self.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        self.decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "        self.embedding_optimizer.load_state_dict(checkpoint['embedding_optimizer_state_dict'])\n",
    "        self.encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "        self.decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "    \n",
    "        self.embedding_layer.eval()\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        self.embedding_layer.train()\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "    \n",
    "net = Mel2SeqNet(80, 512, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")      \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Preloader():\n",
    "#     def __init__(self, mel_path_list):\n",
    "#         super(Preloader).__init__()\n",
    "#         self.mel_path_list = mel_path_list\n",
    "#         self.tensor_input_list = [None] * len(mel_path_list)\n",
    "    \n",
    "#     def load(self, i):\n",
    "#         norm_log_mel_specgram = np.load(self.mel_path_list[i])\n",
    "#         input_spectrogram = norm_log_mel_specgram.T\n",
    "#         tensor_input = torch.tensor(input_spectrogram).view(1, input_spectrogram.shape[0], input_spectrogram.shape[1])\n",
    "#         self.tensor_input_list[i] = tensor_input\n",
    "        \n",
    "#     def get(self, i):\n",
    "#         if type(self.tensor_input_list[i]) == type(None):\n",
    "#             self.load(i)\n",
    "#         return self.tensor_input_list[i]  \n",
    "    \n",
    "#     def get_batch(self):\n",
    "        \n",
    "#         return batched_tensor, indxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batched_Preloader():\n",
    "    def __init__(self, mel_path_list, ground_truth_list, batch_size):\n",
    "        super(Batched_Preloader).__init__()\n",
    "        self.mel_path_list = mel_path_list\n",
    "        self.total_num_input = len(mel_path_list)\n",
    "        self.tensor_input_list = [None] * self.total_num_input\n",
    "        self.ground_truth_list = ground_truth_list\n",
    "        self.sentence_length_list = np.asarray(list(map(len, ground_truth_list)))\n",
    "        self.shuffle_step = 4\n",
    "        self.loading_sequence = None\n",
    "        self.end_flag = True\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def load(self, i):\n",
    "        norm_log_mel_specgram = np.load(self.mel_path_list[i])\n",
    "        input_spectrogram = norm_log_mel_specgram.T\n",
    "        tensor_input = torch.tensor(input_spectrogram).view(1, input_spectrogram.shape[0], input_spectrogram.shape[1])\n",
    "        self.tensor_input_list[i] = tensor_input\n",
    "        \n",
    "    def get(self, i):\n",
    "        if type(self.tensor_input_list[i]) == type(None):\n",
    "            self.load(i)\n",
    "        return self.tensor_input_list[i]  \n",
    "    \n",
    "    def initialize_batch(self):\n",
    "        loading_sequence = np.argsort(self.sentence_length_list)\n",
    "        bundle = np.stack([self.sentence_length_list[loading_sequence], loading_sequence])\n",
    "        \n",
    "        for seq_len in range(self.shuffle_step, np.max(self.sentence_length_list), self.shuffle_step):\n",
    "            idxs = np.where((bundle[0, :] > seq_len) & (bundle[0, :] <= seq_len + self.shuffle_step))[0]\n",
    "            idxs_origin = copy.deepcopy(idxs)\n",
    "            random.shuffle(idxs)\n",
    "            bundle[:, idxs_origin] = bundle[:, idxs]\n",
    "            \n",
    "        loading_sequence = bundle[1, :]\n",
    "        \n",
    "        self.loading_sequence = loading_sequence\n",
    "        self.current_loading_index = 0\n",
    "        self.end_flag = False\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def get_batch(self):\n",
    "        \n",
    "        tensor_list = list()\n",
    "        ground_truth_list = list()\n",
    "        tensor_size_list = list()\n",
    "        ground_truth_size_list = list()\n",
    "        \n",
    "        count = 0\n",
    "        max_seq_len = 0\n",
    "        max_sen_len = 0\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            if self.current_loading_index >= self.total_num_input:\n",
    "                self.end_flag = True\n",
    "                break\n",
    "            \n",
    "            tensor = self.get(self.loading_sequence[self.current_loading_index])\n",
    "            tensor_list.append(tensor)\n",
    "            tensor_size_list.append(tensor.shape[1])\n",
    "            \n",
    "            ground_truth = self.ground_truth_list[self.loading_sequence[self.current_loading_index]]\n",
    "            ground_truth_list.append(ground_truth)\n",
    "            ground_truth_size_list.append(len(ground_truth))\n",
    "            \n",
    "            \n",
    "            if (tensor.shape[1] > max_seq_len):\n",
    "                max_seq_len = tensor.shape[1]\n",
    "            if (len(ground_truth) > max_sen_len):\n",
    "                max_sen_len = len(ground_truth)  \n",
    "            \n",
    "            self.current_loading_index += 1\n",
    "            count += 1\n",
    "            \n",
    "        batched_tensor = torch.zeros(count, max_seq_len, n_mels)\n",
    "        batched_ground_truth = torch.zeros(count, max_sen_len)\n",
    "        batched_loss_mask = torch.zeros(count, max_sen_len)\n",
    "        \n",
    "        for order in range(count):\n",
    "            batched_tensor[order, :tensor_size_list[order], :] = tensor_list[order]\n",
    "#             print(ground_truth_size_list[order])\n",
    "            batched_ground_truth[order, :ground_truth_size_list[order]] = torch.tensor(ground_truth_list[order])\n",
    "            batched_loss_mask[order, :ground_truth_size_list[order]] = torch.ones(ground_truth_size_list[order])\n",
    "        \n",
    "        return batched_tensor, batched_ground_truth, batched_loss_mask\n",
    "        \n",
    "#         return batched_tensor, ground_truth_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_list = [(tokenizer.word2num(list(metadata[i, 3]) + ['<eos>'])) for i in range(len(metadata))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preloader = Preloader(mel_path_list)\n",
    "preloader = Batched_Preloader(mel_path_list, ground_truth_list, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCH = 30\n",
    "REPEAT = 1\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    try:\n",
    "        net.load('check_point_test_adam')\n",
    "    except:\n",
    "        print(\"Loading Error\")\n",
    "    preloader.initialize_batch()\n",
    "    counter = 0\n",
    "    loss_list = list()\n",
    "    print(datetime.now().strftime('%m-%d %H:%M:%S'))\n",
    "    \n",
    "    while preloader.end_flag == False:\n",
    "        tensor_input, ground_truth, loss_mask = preloader.get_batch()\n",
    "        \n",
    "        for i in range(REPEAT):\n",
    "            pred_tensor, attention_matrix, loss = net.train(tensor_input.to(device), ground_truth.to(device), loss_mask.to(device))\n",
    "        \n",
    "        counter += 1\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        if (counter % 100 == 0):\n",
    "            print('Calculated {} Batches'.format(counter))\n",
    "            print('Loss {}: {}'.format(counter, loss))\n",
    "            print(datetime.now().strftime('%m-%d %H:%M:%S'))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(attention_matrix[0].detach().cpu().numpy())\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            _, index = pred_tensor[0].max(-1)\n",
    "            sentence = tokenizer.num2word(index.view(-1))\n",
    "            print(''.join(sentence))\n",
    "            true_sentence = tokenizer.num2word(ground_truth[0, :].detach().numpy().astype(int))\n",
    "            print(''.join(true_sentence))\n",
    "            \n",
    "    net.save('check_point_test_adam')\n",
    "    print(\"Mean Loss: {}\".format(np.mean(np.asarray(loss_list))))\n",
    "    print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EPOCH = 1\n",
    "\n",
    "# for epoch in range(EPOCH):\n",
    "    \n",
    "#     net.load('check_point_test')\n",
    "    \n",
    "#     for i in tqdm(range(len(metadata))):\n",
    "#         tensor_input = preloader.get(i)\n",
    "#         ground_truth = ground_truth_list[i]\n",
    "        \n",
    "#         pred_tensor, attention_matrix, loss = net.train(tensor_input.to(device), ground_truth.to(device))\n",
    "        \n",
    "#         if (i % 100 == 0):\n",
    "#             print(datetime.datetime.now())\n",
    "#             print('Loss {}: {}'.format(i, loss))\n",
    "#             plt.figure()\n",
    "#             plt.imshow(attention_matrix[0].detach().cpu().numpy())\n",
    "#             plt.colorbar()\n",
    "#             plt.show()\n",
    "#             _, index = pred_tensor.max(-1)\n",
    "#             sentence = tokenizer.num2word(index.view(-1))\n",
    "#             print(''.join(sentence))\n",
    "#             print(metadata[i, 2])\n",
    "    \n",
    "#     net.save('check_point_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.imshow(attention_matrix[0].detach().cpu().numpy())\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(attention_matrix[0, 0, :].shape)\n",
    "# print(sum(attention_matrix[0, 0, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.save('check_point_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.load('check_point_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
