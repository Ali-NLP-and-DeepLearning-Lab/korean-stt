{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNy2clEZPKAv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa # librosa: Audio handling package\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import jamotools\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm # tqdm: Pakage for progress bar visualization\n",
    "from datetime import datetime\n",
    "\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "import Levenshtein as Lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "SBaEfy86PKA1",
    "outputId": "4cde26f5-f3e4-4589-871e-e204684d376e"
   },
   "outputs": [],
   "source": [
    "n_mels = 80\n",
    "fs = 16000\n",
    "frame_length_ms=50\n",
    "frame_shift_ms=25\n",
    "nsc = int(fs * frame_length_ms / 1000)\n",
    "nov = nsc - int(fs * frame_shift_ms / 1000)\n",
    "nhop = int(fs * frame_shift_ms / 1000)\n",
    "eps = 1e-8\n",
    "db_ref = 160\n",
    "\n",
    "# meta_path = 'gdrive/My Drive/korean-single-speaker-speech-dataset/transcript.v.1.2.txt'\n",
    "# data_folder = 'gdrive/My Drive/korean-single-speaker-speech-dataset/kss'\n",
    "\n",
    "# meta_path = \"D:/korean-single-speaker-speech-dataset/transcript.v.1.2.txt\"\n",
    "# data_folder = \"D:/korean-single-speaker-speech-dataset/kss\"\n",
    "\n",
    "data_folder = \"D:/nsml-dataset/train_data/\"\n",
    "label_path = \"D:/nsml-dataset/hackathon.labels\"\n",
    "\n",
    "data_list = glob.glob(data_folder + '*.csv')[0]\n",
    "\n",
    "wav_paths = list()\n",
    "script_paths = list()\n",
    "korean_script_paths = list()\n",
    "\n",
    "with open(data_list, 'r') as f:\n",
    "    for line in f:\n",
    "        # line: \"aaa.wav,aaa.label\"\n",
    "        wav_path, script_path = line.strip().split(',')\n",
    "        korean_script_path = script_path.replace('.label', '.script')\n",
    "        \n",
    "        wav_paths.append(os.path.join(data_folder, wav_path))\n",
    "        script_paths.append(os.path.join(data_folder, script_path))\n",
    "        korean_script_paths.append(os.path.join(data_folder, korean_script_path))\n",
    "\n",
    "dataset_size = len(wav_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(filepath, bos_id, eos_id):\n",
    "    key = filepath.split('/')[-1].split('.')[0]\n",
    "    script = target_dict[key]\n",
    "    tokens = script.split(' ')\n",
    "    result = list()\n",
    "    result.append(bos_id)\n",
    "    for i in range(len(tokens)):\n",
    "        if len(tokens[i]) > 0:\n",
    "            result.append(int(tokens[i]))\n",
    "    result.append(eos_id)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav_paths len: 29805\n",
      "script_paths len: 29805\n",
      "korean_script_paths len: 29805\n",
      "D:/nsml-dataset/train_data/41_0601_211_0_07930_02.wav\n",
      "D:/nsml-dataset/train_data/41_0601_211_0_07930_02.label\n",
      "D:/nsml-dataset/train_data/41_0601_211_0_07930_02.script\n"
     ]
    }
   ],
   "source": [
    "print(\"wav_paths len: {}\".format(len(wav_paths)))\n",
    "print(\"script_paths len: {}\".format(len(script_paths)))\n",
    "print(\"korean_script_paths len: {}\".format(len(korean_script_paths)))\n",
    "\n",
    "print(wav_paths[0])\n",
    "print(script_paths[0])\n",
    "print(korean_script_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(script_paths[1]) as f:\n",
    "    line = f.read()\n",
    "    line = line.strip()\n",
    "    result = list(map(int, line.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label(label_path):\n",
    "    char2index = dict() # [ch] = id\n",
    "    index2char = dict() # [id] = ch\n",
    "    with open(label_path, 'r', encoding='UTF-8') as f:\n",
    "    # with open(label_path, 'r') as f:\n",
    "        for no, line in enumerate(f):\n",
    "            if line[0] == '#': \n",
    "                continue\n",
    "\n",
    "            index, char, freq = line.strip().split('\\t')\n",
    "            char = char.strip()\n",
    "            if len(char) == 0:\n",
    "                char = ' '\n",
    "\n",
    "            char2index[char] = int(index)\n",
    "            index2char[int(index)] = char\n",
    "\n",
    "    return char2index, index2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2index, index2char = load_label(label_path)\n",
    "SOS_token = char2index['<s>']  # '<sos>' or '<s>'\n",
    "EOS_token = char2index['</s>']  # '<eos>' or '</s>'\n",
    "PAD_token = char2index['_']  # '-' or '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char2index len: 820\n",
      "index2char len: 820\n"
     ]
    }
   ],
   "source": [
    "print('char2index len: {}'.format(len(char2index)))\n",
    "\n",
    "print('index2char len: {}'.format(len(index2char)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ᄀ', 'ᄁ', 'ᄂ', 'ᄃ', 'ᄄ', 'ᄅ', 'ᄆ', 'ᄇ', 'ᄈ', 'ᄉ', 'ᄊ', 'ᄋ', 'ᄌ', 'ᄍ', 'ᄎ', 'ᄏ', 'ᄐ', 'ᄑ', 'ᄒ', 'ᅡ', 'ᅢ', 'ᅣ', 'ᅤ', 'ᅥ', 'ᅦ', 'ᅧ', 'ᅨ', 'ᅩ', 'ᅪ', 'ᅫ', 'ᅬ', 'ᅭ', 'ᅮ', 'ᅯ', 'ᅰ', 'ᅱ', 'ᅲ', 'ᅳ', 'ᅴ', 'ᅵ', 'ᆨ', 'ᆩ', 'ᆪ', 'ᆫ', 'ᆬ', 'ᆭ', 'ᆮ', 'ᆯ', 'ᆰ', 'ᆱ', 'ᆲ', 'ᆳ', 'ᆴ', 'ᆵ', 'ᆶ', 'ᆷ', 'ᆸ', 'ᆹ', 'ᆺ', 'ᆻ', 'ᆼ', 'ᆽ', 'ᆾ', 'ᆿ', 'ᇀ', 'ᇁ', 'ᇂ', ' ', '!', ',', '.', '?']\n"
     ]
    }
   ],
   "source": [
    "pure_jamo_list = list()\n",
    "\n",
    "# 초성\n",
    "for unicode in range(0x1100, 0x1113):\n",
    "    pure_jamo_list.append(chr(unicode))  # chr: Change hexadecimal to unicode\n",
    "# 중성\n",
    "for unicode in range(0x1161, 0x1176):\n",
    "    pure_jamo_list.append(chr(unicode))\n",
    "# 종성\n",
    "for unicode in range(0x11A8, 0x11C3):\n",
    "    pure_jamo_list.append(chr(unicode))\n",
    "\n",
    "pure_jamo_list += [' ', '!', ',', '.', '?']\n",
    "\n",
    "print(pure_jamo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cf312302fa44e0a7858f9fb2c8224e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29805), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "korean_script_list = list()\n",
    "jamo_script_list = list()\n",
    "\n",
    "# true_jamo_regex_not  = re.compile(u'[^, .?!\\u1100-\\u115e\\u1161-\\u11A7\\u11a8-\\u11ff]+')\n",
    "\n",
    "# valid_regex = re.compile(u'[,_ ^.?!？~<>:;/%()+A-Za-z0-9\\u1100-\\u115e\\u1161-\\u11A7\\u11a8-\\u11ff]+')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for file in tqdm(korean_script_paths):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        line = f.read()\n",
    "        line = line.strip()\n",
    "        korean_script_list.append(line)\n",
    "        jamo = list(jamotools.split_syllables(line, 'JAMO'))\n",
    "        \n",
    "#         print(len(jamo))\n",
    "        for i, c in enumerate(jamo):\n",
    "            if c not in pure_jamo_list:\n",
    "                jamo[i] = '*'\n",
    "        \n",
    "        jamo = ''.join(jamo)\n",
    "#         jamo_filtered = ''.join(true_jamo_regex.findall(jamo))\n",
    "#         jamo_filtered = re.sub(true_jamo_regex_not, '*', jamo)\n",
    "        jamo_script_list.append(jamo)\n",
    "        \n",
    "#         print(line)\n",
    "#         print(jamo) \n",
    "        \n",
    "#         if count == 100:\n",
    "#             break\n",
    "        \n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_starting_point(coef, thres = 0.1, margin = 10):\n",
    "    starting_point = 0\n",
    "    for i in range(len(coef) - 1):\n",
    "        if (coef[i] <= thres and coef[i+1] > thres):\n",
    "            starting_point = i\n",
    "            break\n",
    "            \n",
    "    starting_point = starting_point - margin\n",
    "    \n",
    "    if starting_point < 0:\n",
    "        starting_point = 0\n",
    "    \n",
    "    return starting_point\n",
    "\n",
    "def find_ending_point(coef, thres = 0.1, margin = 10):\n",
    "    for i in range(len(coef) - 1, 0, -1):\n",
    "        if (coef[i] <= thres and coef[i-1] > thres):\n",
    "            ending_point = i\n",
    "            break\n",
    "            \n",
    "    ending_point = ending_point + margin\n",
    "    \n",
    "    if ending_point > len(coef):\n",
    "        ending_point = len(coef)\n",
    "\n",
    "    return ending_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Threading_Batched_Preloader():\n",
    "    def __init__(self, wav_path_list, ground_truth_list, script_path_list, batch_size, is_train=True):\n",
    "        super(Threading_Batched_Preloader).__init__()\n",
    "        self.wav_path_list = wav_path_list\n",
    "        self.total_num_input = len(wav_path_list)\n",
    "        self.tensor_input_list = [None] * self.total_num_input\n",
    "        self.ground_truth_list = ground_truth_list\n",
    "        self.script_path_list = script_path_list\n",
    "        self.sentence_length_list = np.asarray(list(map(len, ground_truth_list)))\n",
    "        self.shuffle_step = 12\n",
    "        \n",
    "        self.shuffle_step = 4\n",
    "        \n",
    "        self.loading_sequence = None\n",
    "        self.end_flag = False\n",
    "        self.batch_size = batch_size\n",
    "        self.queue = queue.Queue(16)\n",
    "        self.thread_flags = list()\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    # Shuffle loading index and set end flag to false\n",
    "    def initialize_batch(self, thread_num):\n",
    "        loading_sequence = np.argsort(self.sentence_length_list)\n",
    "        bundle = np.stack([self.sentence_length_list[loading_sequence], loading_sequence])\n",
    "\n",
    "        for seq_len in range(self.shuffle_step, np.max(self.sentence_length_list), self.shuffle_step):\n",
    "            idxs = np.where((bundle[0, :] > seq_len) & (bundle[0, :] <= seq_len + self.shuffle_step))[0]\n",
    "            idxs_origin = copy.deepcopy(idxs)\n",
    "            random.shuffle(idxs)\n",
    "            bundle[:, idxs_origin] = bundle[:, idxs]\n",
    "            \n",
    "        loading_sequence = bundle[1, :]\n",
    "        loading_sequence_len = len(loading_sequence)\n",
    "        \n",
    "#         print(\"Loading Sequence Length: {}\".format(loading_sequence_len))\n",
    "        \n",
    "        thread_size = int(np.ceil(loading_sequence_len / thread_num))\n",
    "\n",
    "        load_idxs_list = list()\n",
    "        for i in range(thread_num):\n",
    "            start_idx = i * thread_size\n",
    "            end_idx = (i + 1) * thread_size\n",
    "\n",
    "            if end_idx > loading_sequence_len:\n",
    "                end_idx = loading_sequence_len\n",
    "\n",
    "            load_idxs_list.append(loading_sequence[start_idx:end_idx])\n",
    "            \n",
    "#         for i in range(thread_num):\n",
    "#             print(len(load_idxs_list[i]))\n",
    "\n",
    "        self.end_flag = False\n",
    "        \n",
    "        self.queue = queue.Queue(32)\n",
    "        self.thread_flags = [False] * thread_num\n",
    "        \n",
    "        self.thread_list = [Batching_Thread(self.wav_path_list, self.ground_truth_list, self.script_path_list, load_idxs_list[i], self.queue, self.batch_size, self.thread_flags, i, self.is_train) for i in range(thread_num)]\n",
    "\n",
    "        for thread in self.thread_list:\n",
    "            thread.start()\n",
    "        return\n",
    "\n",
    "    def check_thread_flags(self):\n",
    "        for flag in self.thread_flags:\n",
    "            if flag == False:\n",
    "                return False\n",
    "        \n",
    "        if (self.queue.empty):\n",
    "            self.end_flag = True\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_batch(self):\n",
    "        while not (self.check_thread_flags()):\n",
    "            batch = self.queue.get()\n",
    "\n",
    "            if (batch != None):\n",
    "                batched_tensor = batch[0]\n",
    "                batched_ground_truth = batch[1] \n",
    "                batched_loss_mask = batch[2]\n",
    "                ground_truth_size_list = batch[3]\n",
    "                batched_lev_truth = batch[4]\n",
    "                batched_lev_truth_loss_mask = batch[5]\n",
    "                \n",
    "                return batched_tensor, batched_ground_truth, batched_loss_mask, ground_truth_size_list, batched_lev_truth, batched_lev_truth_loss_mask\n",
    "\n",
    "        return None\n",
    "\n",
    "class Batching_Thread(threading.Thread):\n",
    "\n",
    "    def __init__(self, wav_path_list, ground_truth_list, script_path_list, load_idxs_list, queue, batch_size, thread_flags, id, is_train=True):\n",
    "        \n",
    "        threading.Thread.__init__(self)\n",
    "        self.wav_path_list = wav_path_list\n",
    "        self.ground_truth_list = ground_truth_list\n",
    "        self.script_path_list = script_path_list\n",
    "        self.load_idxs_list = load_idxs_list\n",
    "        self.list_len = len(load_idxs_list)\n",
    "        self.cur_idx = 0\n",
    "        self.id = id\n",
    "        self.queue = queue\n",
    "        self.batch_size = batch_size \n",
    "        self.thread_flags = thread_flags\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        while(self.cur_idx < self.list_len):\n",
    "            batch = self.batch()\n",
    "            success = False\n",
    "            while success == False:\n",
    "                try:\n",
    "                    self.queue.put(batch, True)\n",
    "                    success = True\n",
    "                except:\n",
    "                    print(\"Batching Failed in Thread ID: {}\".format(self.id))\n",
    "                    sleep(1)\n",
    "\n",
    "        self.thread_flags[self.id] = True\n",
    "        \n",
    "#         print(\"Thread {} finished\".foramt(self.id))\n",
    "\n",
    "        return \n",
    "\n",
    "\n",
    "    def batch(self):\n",
    "\n",
    "        tensor_list = list()\n",
    "        ground_truth_list = list()\n",
    "        tensor_size_list = list()\n",
    "        ground_truth_size_list = list()\n",
    "        lev_truth_list = list()\n",
    "        lev_truth_length_list = list()\n",
    "        \n",
    "        count = 0\n",
    "        max_seq_len = 0\n",
    "        max_sen_len = 0\n",
    "        max_lev_truth_len = 0\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            # If there is no more file, break and set end_flag true\n",
    "            if self.cur_idx >= self.list_len:\n",
    "                self.end_flag = True\n",
    "                break\n",
    "                \n",
    "            script_path = self.script_path_list[self.load_idxs_list[self.cur_idx]]\n",
    "            \n",
    "#             print(script_path)\n",
    "            \n",
    "            with open(script_path) as f:\n",
    "                line = f.read()\n",
    "                line = line.strip()\n",
    "                lev_truth = list(map(int, line.split(' ')))\n",
    "                \n",
    "            lev_truth_list.append(lev_truth)\n",
    "            lev_truth_length_list.append(len(lev_truth))\n",
    "            \n",
    "            wav_path = self.wav_path_list[self.load_idxs_list[self.cur_idx]]\n",
    "\n",
    "            tensor = self.create_mel(wav_path)\n",
    "            tensor_list.append(tensor)\n",
    "            tensor_size_list.append(tensor.shape[1])\n",
    "            \n",
    "            ground_truth = self.ground_truth_list[self.load_idxs_list[self.cur_idx]]\n",
    "            ground_truth_list.append(ground_truth)\n",
    "            ground_truth_size_list.append(len(ground_truth))\n",
    "            \n",
    "            if (tensor.shape[1] > max_seq_len):\n",
    "                max_seq_len = tensor.shape[1]\n",
    "            if (len(ground_truth) > max_sen_len):\n",
    "                max_sen_len = len(ground_truth)  \n",
    "            if (len(lev_truth) > max_lev_truth_len):\n",
    "                max_lev_truth_len = len(lev_truth)\n",
    "            \n",
    "            self.cur_idx += 1\n",
    "            count += 1\n",
    "            \n",
    "        batched_tensor = torch.zeros(count, max_seq_len + 5, n_mels)\n",
    "        batched_ground_truth = torch.zeros(count, max_sen_len)\n",
    "        batched_loss_mask = torch.zeros(count, max_sen_len)\n",
    "        ground_truth_size_list = torch.tensor(np.asarray(ground_truth_size_list), dtype=torch.long)\n",
    "        \n",
    "        batched_lev_truth = torch.zeros(count, max_lev_truth_len)\n",
    "        batched_lev_truth_loss_mask = torch.zeros(count, max_lev_truth_len)\n",
    "        \n",
    "        for order in range(count):\n",
    "            \n",
    "            target = tensor_list[order]\n",
    "            \n",
    "            if self.is_train:\n",
    "                pad_random = np.random.randint(0, 5)\n",
    "                # Time shift, add zeros in front of an image\n",
    "                if pad_random > 0:\n",
    "                    offset = torch.zeros(target.shape[0], pad_random, target.shape[2])\n",
    "                    target = torch.cat((offset, target), 1)\n",
    "                # Add random noise\n",
    "                target = target + (torch.rand(target.shape) - 0.5) / 20\n",
    "                # Value less than 0 or more than 1 is clamped to 0 and 1\n",
    "                target = torch.clamp(target, min=0.0, max=1.0)\n",
    "                batched_tensor[order, :tensor_size_list[order] + pad_random, :] = target\n",
    "            else:\n",
    "                batched_tensor[order, :tensor_size_list[order], :] = target\n",
    "\n",
    "#           batched_tensor[order, :tensor_size_list[order], :] = target\n",
    "            batched_ground_truth[order, :ground_truth_size_list[order]] = torch.tensor(ground_truth_list[order])\n",
    "            \n",
    "            # You do not need to know what loss mask is \n",
    "            batched_loss_mask[order, :ground_truth_size_list[order]] = torch.ones(ground_truth_size_list[order])\n",
    "        \n",
    "            batched_lev_truth[order, :lev_truth_length_list[order]] = torch.tensor(lev_truth_list[order])\n",
    "            batched_lev_truth_loss_mask[order, :lev_truth_length_list[order]] = torch.ones(lev_truth_length_list[order])\n",
    "        \n",
    "        return [batched_tensor, batched_ground_truth, batched_loss_mask, ground_truth_size_list, batched_lev_truth, batched_lev_truth_loss_mask]\n",
    "    \n",
    "    def create_mel(self, wav_path):  \n",
    "        y, sr = librosa.core.load(wav_path, sr=fs) \n",
    "        f, t, Zxx = sp.signal.stft(y, fs=sr, nperseg=nsc, noverlap=nov)\n",
    "        Sxx = np.abs(Zxx)\n",
    "        \n",
    "        # Cut-off paddings\n",
    "        coef = np.sum(Sxx, 0)\n",
    "        Sxx = Sxx[:, find_starting_point(coef):find_ending_point(coef)]\n",
    "\n",
    "        # mel_filters: (n_fft, n_mels)\n",
    "        mel_filters = librosa.filters.mel(sr=fs, n_fft=nsc, n_mels=n_mels)\n",
    "        mel_specgram = np.matmul(mel_filters, Sxx)\n",
    "\n",
    "        # log10(0) is minus infinite, so replace mel_specgram values smaller than 'eps' as 'eps' (1e-8)\n",
    "        log_mel_specgram = 20 * np.log10(np.maximum(mel_specgram, eps))\n",
    "        \n",
    "        # 20 * log10(eps) = 20 * -8 = -160\n",
    "        # -160 is the smallest value\n",
    "        # Add 160 and divide by 160 => Normalize value between 0 and 1\n",
    "        norm_log_mel_specgram = (log_mel_specgram + db_ref) / db_ref        \n",
    "        \n",
    "        # (F, T) -> (T, F)\n",
    "        input_spectrogram = norm_log_mel_specgram.T\n",
    "        # (T, F) -> (1, T, F)\n",
    "        # Inserted the first axis to make stacking easier\n",
    "        tensor_input = torch.tensor(input_spectrogram).view(1, input_spectrogram.shape[0], input_spectrogram.shape[1])\n",
    "        return tensor_input\n",
    "#         return torch.zeros(1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer maps numbers to characters, 8 -> 'ㄱ', 10 -> 'ㄴ'\n",
    "class Tokenizer():\n",
    "    def __init__(self, vocabs):\n",
    "        self.vocabs = vocabs\n",
    "        \n",
    "    def word2num(self, sentence):\n",
    "        tokens = list()\n",
    "        for char in sentence:\n",
    "            tokens.append(self.vocabs.index(char))    \n",
    "        return tokens\n",
    "        \n",
    "    def word2vec(self, sentence):\n",
    "        vectors = np.zeros((len(sentence), len(self.vocabs)))\n",
    "        for i, char in enumerate(sentence):\n",
    "            vectors[i, self.vocabs.index(char)] = 1   \n",
    "        return vectors\n",
    "    \n",
    "    def num2word(self, num):\n",
    "        output = list()\n",
    "        for i in num:\n",
    "            output.append(self.vocabs[i])\n",
    "        return output\n",
    "    \n",
    "    def num2vec(self, numbers):\n",
    "        vectors = np.zeros((len(numbers), len(self.vocabs)))\n",
    "        for i, num in enumerate(numbers):\n",
    "            vectors[i, num] = 1   \n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "9gbjg0_LPKA8",
    "outputId": "b2b29359-e867-4df6-e305-83de989ceef3"
   },
   "outputs": [],
   "source": [
    "# unicode_jamo_list = list()\n",
    "\n",
    "# # 초성\n",
    "# for unicode in range(0x1100, 0x1113):\n",
    "#     unicode_jamo_list.append(chr(unicode))  # chr: Change hexadecimal to unicode\n",
    "# # 중성\n",
    "# for unicode in range(0x1161, 0x1176):\n",
    "#     unicode_jamo_list.append(chr(unicode))\n",
    "# # 종성\n",
    "# for unicode in range(0x11A8, 0x11C3):\n",
    "#     unicode_jamo_list.append(chr(unicode))\n",
    "# for unicode in range(ord('A'), ord('Z') + 1):\n",
    "#     unicode_jamo_list.append(chr(unicode))\n",
    "# for unicode in range(ord('a'), ord('z') + 1):\n",
    "#     unicode_jamo_list.append(chr(unicode))\n",
    "# for unicode in range(ord('0'), ord('9') + 1):\n",
    "#     unicode_jamo_list.append(chr(unicode))\n",
    "\n",
    "# unicode_jamo_list += [' ', '\\\\', '!', '~', '^', '<', '>', ',', '.', \"'\", '?', '？', '/', '%', '(', ')', ':', ';', '+',\n",
    "#                       '-', '<s>', '</s>']\n",
    "# unicode_jamo_list.sort()\n",
    "# # '_' symbol represents \"blank\" in CTC loss system, \"blank\" has to be the index 0\n",
    "# unicode_jamo_list = ['_'] + unicode_jamo_list\n",
    "\n",
    "# tokenizer = Tokenizer(unicode_jamo_list)\n",
    "# jamo_tokens = tokenizer.word2num(unicode_jamo_list)\n",
    "\n",
    "unicode_jamo_list = list()\n",
    "\n",
    "# 초성\n",
    "for unicode in range(0x1100, 0x1113):\n",
    "    unicode_jamo_list.append(chr(unicode))  # chr: Change hexadecimal to unicode\n",
    "# 중성\n",
    "for unicode in range(0x1161, 0x1176):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "# 종성\n",
    "for unicode in range(0x11A8, 0x11C3):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "\n",
    "unicode_jamo_list += [' ', '!', ',', '.', '?', '<s>', '</s>', '*']\n",
    "\n",
    "unicode_jamo_list.sort()\n",
    "# '_' symbol represents \"blank\" in CTC loss system, \"blank\" has to be the index 0\n",
    "unicode_jamo_list = ['_'] + unicode_jamo_list\n",
    "\n",
    "tokenizer = Tokenizer(unicode_jamo_list)\n",
    "jamo_tokens = tokenizer.word2num(unicode_jamo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', ' ', '!', '*', ',', '.', '</s>', '<s>', '?', 'ᄀ', 'ᄁ', 'ᄂ', 'ᄃ', 'ᄄ', 'ᄅ', 'ᄆ', 'ᄇ', 'ᄈ', 'ᄉ', 'ᄊ', 'ᄋ', 'ᄌ', 'ᄍ', 'ᄎ', 'ᄏ', 'ᄐ', 'ᄑ', 'ᄒ', 'ᅡ', 'ᅢ', 'ᅣ', 'ᅤ', 'ᅥ', 'ᅦ', 'ᅧ', 'ᅨ', 'ᅩ', 'ᅪ', 'ᅫ', 'ᅬ', 'ᅭ', 'ᅮ', 'ᅯ', 'ᅰ', 'ᅱ', 'ᅲ', 'ᅳ', 'ᅴ', 'ᅵ', 'ᆨ', 'ᆩ', 'ᆪ', 'ᆫ', 'ᆬ', 'ᆭ', 'ᆮ', 'ᆯ', 'ᆰ', 'ᆱ', 'ᆲ', 'ᆳ', 'ᆴ', 'ᆵ', 'ᆶ', 'ᆷ', 'ᆸ', 'ᆹ', 'ᆺ', 'ᆻ', 'ᆼ', 'ᆽ', 'ᆾ', 'ᆿ', 'ᇀ', 'ᇁ', 'ᇂ']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]\n"
     ]
    }
   ],
   "source": [
    "print(unicode_jamo_list)\n",
    "print(jamo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_list = [(tokenizer.word2num(['<s>'] + list(jamo_script_list[i]) + ['</s>'])) for i in range(len(jamo_script_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% of the data will be used as train\n",
    "split_index = int(0.9 * len(wav_paths))\n",
    "\n",
    "wav_path_list_train = wav_paths[:split_index]\n",
    "ground_truth_list_train = ground_truth_list[:split_index]\n",
    "script_path_list_train = script_paths[:split_index]\n",
    "\n",
    "wav_path_list_eval = wav_paths[split_index:]\n",
    "ground_truth_list_eval = ground_truth_list[split_index:]\n",
    "script_path_list_eval = script_paths[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_thread = 3\n",
    "\n",
    "preloader_eval = Threading_Batched_Preloader(wav_path_list_eval, ground_truth_list_eval, script_path_list_eval, batch_size, is_train=False)\n",
    "preloader_train = Threading_Batched_Preloader(wav_path_list_train, ground_truth_list_train, script_path_list_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, ctc_loss, input_tensor, ground_truth, loss_mask, target_lengths):\n",
    "\n",
    "    # Shape of the input tensor (B, T, F)\n",
    "    # B: Number of a batch (8, 16, or 64 ...)\n",
    "    # T: Temporal length of an input\n",
    "    # F: Number of frequency band, 80\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    batch_size = input_tensor.shape[0]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pred_tensor = net(input_tensor)\n",
    "    \n",
    "    # Cast true sentence as Long data type, since CTC loss takes long tensor only\n",
    "    # Shape (B, S)\n",
    "    # S: Max length among true sentences \n",
    "    truth = ground_truth\n",
    "    truth = truth.type(torch.cuda.LongTensor)\n",
    "\n",
    "    input_lengths = torch.full(size=(batch_size,), fill_value=pred_tensor.shape[0], dtype=torch.long)\n",
    "\n",
    "    loss = ctc_loss(pred_tensor, truth, input_lengths, target_lengths)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return loss divided by true length because loss is sum of the character losses\n",
    "\n",
    "    return pred_tensor, loss.item() / ground_truth.shape[1]\n",
    "\n",
    "\n",
    "def evaluate(net, ctc_loss, input_tensor, ground_truth, loss_mask, target_lengths):\n",
    "\n",
    "    # Shape of the input tensor (B, T, F)\n",
    "    # B: Number of a batch (8, 16, or 64 ...)\n",
    "    # T: Temporal length of an input\n",
    "    # F: Number of frequency band, 80\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    batch_size = input_tensor.shape[0]\n",
    "    \n",
    "    pred_tensor = net(input_tensor)\n",
    "    \n",
    "    # Cast true sentence as Long data type, since CTC loss takes long tensor only\n",
    "    # Shape (B, S)\n",
    "    # S: Max length among true sentences \n",
    "    truth = ground_truth\n",
    "    truth = truth.type(torch.cuda.LongTensor)\n",
    "\n",
    "    input_lengths = torch.full(size=(batch_size,), fill_value=pred_tensor.shape[0], dtype=torch.long)\n",
    "\n",
    "    loss = ctc_loss(pred_tensor, truth, input_lengths, target_lengths)\n",
    "\n",
    "    # Return loss divided by true length because loss is sum of the character losses\n",
    "\n",
    "    return pred_tensor, loss.item() / ground_truth.shape[1]\n",
    "\n",
    "def save(model, optimizer, check_point_name):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, check_point_name)\n",
    "\n",
    "def load(model, optimizer, check_point_name):\n",
    "    checkpoint = torch.load(check_point_name)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OG2yNubVPKBM"
   },
   "outputs": [],
   "source": [
    "# Use GPU if GPU is available \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, D_in, H):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = torch.nn.Linear(D_in, H)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.gru = nn.GRU(H, int(H/2), bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # (B, T, F)\n",
    "        output_tensor = self.fc(input_tensor)\n",
    "        output_tensor = self.relu(output_tensor)\n",
    "        output_tensor = self.dropout(output_tensor)\n",
    "        # (B, T, H)\n",
    "        output_tensor, _ = self.gru(output_tensor)\n",
    "        return output_tensor\n",
    "    \n",
    "class CTC_Decoder(nn.Module):\n",
    "    def __init__(self, H, D_out, num_chars):\n",
    "        super(CTC_Decoder, self).__init__()\n",
    "        self.fc_embed = nn.Linear(H, H)\n",
    "        self.relu_embed = torch.nn.ReLU()\n",
    "        self.dropout_embed = nn.Dropout(p=0.2) \n",
    "        self.gru = nn.GRU(H, D_out, batch_first=True)\n",
    "        self.fc = nn.Linear(D_out, num_chars)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # (B, T, 2 * H/2)\n",
    "        output_tensor = self.fc_embed(input_tensor)\n",
    "        output_tensor = self.relu_embed(output_tensor)\n",
    "        output_tensor = self.dropout_embed(output_tensor) \n",
    "        # (B, T, H)\n",
    "        output_tensor,_ = self.gru(input_tensor)\n",
    "        # (B, T, H)\n",
    "        output_tensor = self.fc(output_tensor)\n",
    "        # (B, T, 75)\n",
    "        prediction_tensor = self.log_softmax(output_tensor)\n",
    "\n",
    "        return prediction_tensor\n",
    "\n",
    "class Mel2SeqNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, num_chars, device):\n",
    "        super(Mel2SeqNet, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(D_in, H).to(device)\n",
    "        self.decoder = CTC_Decoder(H, D_out, num_chars).to(device)\n",
    "        \n",
    "        # Initialize weights with random uniform numbers with range\n",
    "        for param in self.encoder.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "        for param in self.decoder.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "            \n",
    "    def forward(self, input_tensor):\n",
    "        batch_size = input_tensor.shape[0]\n",
    "        # (B, T, F) -> (B, T, H)\n",
    "        encoded_tensor = self.encoder(input_tensor)\n",
    "        # (B, T, H) -> (B, T, 75)\n",
    "        pred_tensor = self.decoder(encoded_tensor)\n",
    "        pred_tensor = pred_tensor.permute(1, 0, 2)\n",
    "        \n",
    "        return pred_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "############################################################################\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, int(hidden_size/2), bidirectional=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "          return torch.zeros(2, batch_size, int(self.hidden_size/2), device=device)\n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, embedded, hidden, encoder_outputs):\n",
    "\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # (1, B, H) + (1, B, H) = (1, B, 2H)        \n",
    "        concated_tensor = torch.cat((embedded, hidden), 2)\n",
    "        \n",
    "        key = self.attn(concated_tensor) # (1, B, H)\n",
    "        key = key.permute(1, 2, 0) # (B, H, 1)\n",
    "\n",
    "        attention_value = torch.bmm(encoder_outputs, key) # (B, L, 1)\n",
    "        attn_weights = F.softmax(attention_value, dim=1)\n",
    "        \n",
    "        attn_weights = attn_weights.permute(0, 2, 1) # (B, 1, L)\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_outputs) # (B, 1, H)\n",
    "        attn_applied = attn_applied.permute(1, 0, 2) # (1, B, H)\n",
    "        \n",
    "        output = torch.cat((embedded, attn_applied), 2) # (1, B, 2H)\n",
    "        output = self.attn_combine(output) # (1, B, H)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden) # (1, B, H)\n",
    "        output = F.log_softmax(self.out(output), dim=2) # (1, B, 74)\n",
    "        \n",
    "        return output.squeeze(0), hidden, attn_weights.squeeze(1)\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    \n",
    "class Seq2SeqNet(nn.Module):\n",
    "    def __init__(self, hidden_size, jamo_tokens, char2index, device):\n",
    "        super(Seq2SeqNet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.jamo_tokens = jamo_tokens\n",
    "        self.char2index = char2index\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(len(jamo_tokens), hidden_size).to(device) \n",
    "        self.embedding_layer_2 = nn.Embedding(len(char2index), hidden_size).to(device)\n",
    "        self.encoder = EncoderRNN(hidden_size).to(device)\n",
    "        self.decoder = AttnDecoderRNN(hidden_size,len(char2index), dropout_p=0.1).to(device)\n",
    "        \n",
    "        for param in self.encoder.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "        for param in self.embedding_layer.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "        for param in self.embedding_layer_2.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "        for param in self.decoder.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "            \n",
    "    def net_train(self, input_tensor, target_tensor, loss_mask, optimizer, criterion):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_size = input_tensor.shape[0]\n",
    "        input_length = input_tensor.shape[1]\n",
    "        target_length = target_tensor.shape[1]\n",
    "\n",
    "        input_tensor = input_tensor.long()\n",
    "        target_tensor = target_tensor.long()\n",
    "        \n",
    "        embedded_tensor = self.embedding_layer(input_tensor)\n",
    "        embedded_tensor = embedded_tensor.permute(1, 0, 2)\n",
    "\n",
    "        # (L, B)\n",
    "        target_tensor = target_tensor.permute(1, 0)\n",
    "        encoder_outputs = torch.zeros(input_length, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        encoder_outputs = torch.zeros(input_length, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        encoder_hidden = self.encoder.initHidden(batch_size)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            embedded_slice = embedded_tensor[ei].unsqueeze(0)\n",
    "            encoder_output, encoder_hidden = self.encoder(\n",
    "                embedded_slice, encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output\n",
    "\n",
    "        decoder_input_token = torch.tensor(([self.char2index['<s>']] * batch_size)).long().unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Override encoder hidden state\n",
    "        encoder_hidden = encoder_outputs[-1, :, :].unsqueeze(0)\n",
    "\n",
    "        # (L, B, H) -> (B, L, H)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        # Override encoder_hidden\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoder_attentions = torch.zeros([batch_size, input_length, target_length])\n",
    "        decoder_outputs = torch.zeros([batch_size, target_length, len(self.char2index)])\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_input = self.embedding_layer_2(decoder_input_token)\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            loss += torch.mean(criterion(decoder_output, target_tensor[di]) * loss_mask[:, di])\n",
    "\n",
    "#             if np.random.rand() < 0.8:        \n",
    "#                 decoder_input_token = target_tensor[di].unsqueeze(0)\n",
    "#             else:\n",
    "            decoder_input_token = torch.argmax(decoder_output, dim=1).unsqueeze(0)\n",
    "\n",
    "            decoder_attentions[:, :, di] = decoder_attention\n",
    "            decoder_outputs[:, di, :] = decoder_output\n",
    "          \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        return decoder_outputs, decoder_attentions, loss.item() / target_length\n",
    "    \n",
    "    \n",
    "    def net_eval(self, input_tensor, target_tensor, loss_mask, criterion):\n",
    "        \n",
    "        batch_size = input_tensor.shape[0]\n",
    "        input_length = input_tensor.shape[1]\n",
    "        target_length = target_tensor.shape[1]\n",
    "\n",
    "        input_tensor = input_tensor.long()\n",
    "        target_tensor = target_tensor.long()\n",
    "        \n",
    "        embedded_tensor = self.embedding_layer(input_tensor)\n",
    "        embedded_tensor = embedded_tensor.permute(1, 0, 2)\n",
    "\n",
    "        # (L, B)\n",
    "        target_tensor = target_tensor.permute(1, 0)\n",
    "        encoder_outputs = torch.zeros(input_length, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        encoder_outputs = torch.zeros(input_length, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        encoder_hidden = self.encoder.initHidden(batch_size)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            embedded_slice = embedded_tensor[ei].unsqueeze(0)\n",
    "            encoder_output, encoder_hidden = self.encoder(\n",
    "                embedded_slice, encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output\n",
    "\n",
    "        decoder_input_token = torch.tensor(([self.char2index['<s>']] * batch_size)).long().unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Override encoder hidden state\n",
    "        encoder_hidden = encoder_outputs[-1, :, :].unsqueeze(0)\n",
    "\n",
    "        # (L, B, H) -> (B, L, H)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        # Override encoder_hidden\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoder_attentions = torch.zeros([batch_size, input_length, target_length])\n",
    "        decoder_outputs = torch.zeros([batch_size, target_length, len(self.char2index)])\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_input = self.embedding_layer_2(decoder_input_token)\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            loss += torch.mean(criterion(decoder_output, target_tensor[di]) * loss_mask[:, di])\n",
    "\n",
    "#             if np.random.rand() < 0.8:        \n",
    "#                 decoder_input_token = target_tensor[di].unsqueeze(0)\n",
    "#             else:\n",
    "            decoder_input_token = torch.argmax(decoder_output, dim=1).unsqueeze(0)\n",
    "\n",
    "            decoder_attentions[:, :, di] = decoder_attention\n",
    "            decoder_outputs[:, di, :] = decoder_output\n",
    "        \n",
    "        return decoder_outputs, decoder_attentions, loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that interprets the CTC prediction result\n",
    "\n",
    "def Decode_CTC_Prediction(prediction):\n",
    "    CTC_pred = prediction.detach().cpu().numpy()\n",
    "    result = list()\n",
    "    last_elem = 0\n",
    "    for i, elem in enumerate(CTC_pred):\n",
    "        if elem != last_elem and elem != 0:\n",
    "            result.append(elem)\n",
    "        \n",
    "        last_elem = elem\n",
    "\n",
    "    result = np.asarray(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decode_Lev(tensor, index2char):\n",
    "    \n",
    "    str_output = list()\n",
    "    \n",
    "    for i in range(tensor.shape[0]):\n",
    "        _, lev_idxs = tensor[i, :, :].max(-1)\n",
    "        lev_idxs = lev_idxs.detach().cpu().numpy()\n",
    "        decoded_sentence = list()\n",
    "        for idx in lev_idxs:\n",
    "            decoded_sentence.append(index2char[idx])\n",
    "        str_output.append(''.join(decoded_sentence))\n",
    "        \n",
    "    return str_output\n",
    "\n",
    "def Decode_Lev_Truth(tensor, index2char):\n",
    "    \n",
    "    str_output = list()\n",
    "    \n",
    "    for i in range(tensor.shape[0]):\n",
    "        lev_idxs = tensor[i, :]\n",
    "        decoded_sentence = list()\n",
    "        for idx in lev_idxs:\n",
    "            decoded_sentence.append(index2char[idx])\n",
    "        str_output.append(''.join(decoded_sentence))\n",
    "        \n",
    "    return str_output\n",
    "        \n",
    "\n",
    "def Decode_Prediction_No_Filtering(pred_tensor, tokenizer):\n",
    "    decoded_list = list()\n",
    "    for i in range(pred_tensor.shape[1]):\n",
    "        _, CTC_index = pred_tensor[:, i, :].max(-1)\n",
    "        index = Decode_CTC_Prediction(CTC_index)\n",
    "        jamos = tokenizer.num2word(index)\n",
    "        sentence = jamotools.join_jamos(''.join(jamos))\n",
    "        decoded_list.append(sentence)\n",
    "    return decoded_list\n",
    "\n",
    "def Decode_Prediction(pred_tensor, tokenizer, char2index):\n",
    "    decoded_list = list()\n",
    "    for i in range(pred_tensor.shape[1]):\n",
    "        _, CTC_index = pred_tensor[:, i, :].max(-1)\n",
    "        index = Decode_CTC_Prediction(CTC_index)\n",
    "        jamos = tokenizer.num2word(index)\n",
    "        sentence = jamotools.join_jamos(''.join(jamos))\n",
    "        \n",
    "        not_com_jamo = re.compile(u'[^\\u3130-\\u3190]')\n",
    "        filtered_sentence = ''.join(not_com_jamo.findall(sentence))\n",
    "        filtered_sentence = filtered_sentence.replace('<s>', '')\n",
    "        filtered_sentence = filtered_sentence.replace('</s>', '')\n",
    "#         filtered_sentence = filtered_sentence.replace('<eos>', '')\n",
    "#         final_prediction = c2i_decoding(char2index, filtered_sentence)\n",
    "        \n",
    "        decoded_list.append(filtered_sentence)\n",
    "    return decoded_list\n",
    "    \n",
    "\n",
    "def lev_num_to_lev_string(lev_num_list, index2char):\n",
    "    lev_str_list = list()\n",
    "    for num_list in lev_num_list:\n",
    "        \n",
    "        temp = list()\n",
    "        for num in num_list:\n",
    "            temp.append(index2char[num])\n",
    "        \n",
    "        lev_str_list.append(''.join(temp))\n",
    "\n",
    "    return lev_str_list\n",
    "\n",
    "def char_distance(ref, hyp):\n",
    "    \n",
    "    ####################\n",
    "    hyp = hyp[:len(ref)]\n",
    "    \n",
    "    ref = ref.replace(' ', '') \n",
    "    hyp = hyp.replace(' ', '') \n",
    "\n",
    "    dist = Lev.distance(hyp, ref)\n",
    "    length = len(ref.replace(' ', ''))\n",
    "\n",
    "    return dist, length \n",
    "\n",
    "def char_distance_list(ref_list, hyp_list):\n",
    "\n",
    "    sum_dist = 0\n",
    "    sum_length = 0\n",
    "    \n",
    "    for ref, hyp in zip(ref_list, hyp_list):\n",
    "        dist, length = char_distance(ref, hyp)\n",
    "        sum_dist += dist\n",
    "        sum_length += length\n",
    "\n",
    "    return sum_dist, sum_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decode_Jamo_Prediction_And_Batch(pred_tensor):\n",
    "    decoded_idx_list = list()\n",
    "    \n",
    "    max_len = 0\n",
    "    \n",
    "    for i in range(pred_tensor.shape[1]):\n",
    "        _, CTC_index = pred_tensor[:, i, :].max(-1)\n",
    "        index = Decode_CTC_Prediction(CTC_index)\n",
    "        if len(index) > max_len:\n",
    "            max_len = len(index)\n",
    "        decoded_idx_list.append(index)\n",
    "        \n",
    "    batched_lev_input = torch.zeros(len(decoded_idx_list), max_len)\n",
    "    \n",
    "#     print('batched_lev_input shape: {}'.format(batched_lev_input.shape))\n",
    "        \n",
    "    for i, index in enumerate(decoded_idx_list):\n",
    "        batched_lev_input[i, :len(index)] = torch.tensor(index)\n",
    "        \n",
    "    return batched_lev_input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 6 * 10\n",
    "           \n",
    "# net = Mel2SeqNet(80, 512, 256)\n",
    "\n",
    "\n",
    "# net = Mel2SeqNet(80, 1024, 512, len(unicode_jamo_list), device)\n",
    "\n",
    "# keyword = 'NSML_pure_jamo_50ms_pad_cut'\n",
    "# net = Mel2SeqNet(80, 512, 256, len(unicode_jamo_list), device)\n",
    "\n",
    "# keyword = 'NSML_pure_jamo_50ms_pad_cut_1024'\n",
    "\n",
    "# keyword = 'NSML_pure_jamo_50ms_pad_cut_1024_copy'\n",
    "\n",
    "net = Mel2SeqNet(80, 512, 256, len(unicode_jamo_list), device)\n",
    "net_optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "ctc_loss = nn.CTCLoss().to(device)\n",
    "\n",
    "# try:\n",
    "#     load(net, net_optimizer, 'model_saved/{}'.format(keyword))\n",
    "# except:\n",
    "#     print('Failed to load model')\n",
    "\n",
    "keyword = 'NSML_jamo_seq2seq_from_scratch_3'\n",
    "\n",
    "jamo2char_net = Seq2SeqNet(256, jamo_tokens, char2index, device)\n",
    "jamo2char_optimizer = optim.Adam(jamo2char_net.parameters(), lr=0.001)\n",
    "seq2seq_criterion = nn.NLLLoss(reduction='none').to(device)\n",
    "\n",
    "train_loss_history = list()\n",
    "eval_loss_history = list()\n",
    "\n",
    "train_seq2seq_loss_history = list()\n",
    "eval_seq2seq_loss_history = list()\n",
    "\n",
    "train_seq2seq_loss_history_ref = list()\n",
    "eval_seq2seq_loss_history_ref = list()\n",
    "\n",
    "train_cer_history = list()\n",
    "eval_cer_history = list()\n",
    "\n",
    "train_cer_history_ref = list()\n",
    "eval_cer_history_ref = list()\n",
    "\n",
    "try:\n",
    "    train_cer_history = list(np.load('model_saved/train_cer_history_{}.npy'.format(keyword)))\n",
    "    eval_cer_history = list(np.load('model_saved/eval_cer_history_{}.npy'.format(keyword)))\n",
    "except:\n",
    "    print(\"No CER Record\")\n",
    "    \n",
    "try:\n",
    "    load(jamo2char_net, jamo2char_optimizer, 'model_saved/seq_{}'.format(keyword))\n",
    "    train_seq2seq_loss_history = list(np.load('model_saved/train_seq_loss_history_{}.npy'.format(keyword)))\n",
    "    eval_seq2seq_loss_history = list(np.load('model_saved/eval_seq_loss_history_{}.npy'.format(keyword)))\n",
    "except:\n",
    "    print('No Seq2Seq Loss Record')\n",
    "    \n",
    "try:\n",
    "#     load(net, net_optimizer, 'model_saved/{}'.format(keyword))\n",
    "    train_loss_history = list(np.load('model_saved/train_loss_history_{}.npy'.format(keyword)))\n",
    "    eval_loss_history = list(np.load('model_saved/eval_loss_history_{}.npy'.format(keyword)))\n",
    "except:\n",
    "    print(\"Loading {} Loss History Error\".format(keyword))\n",
    "\n",
    "# keyword = 'NSML_pure_jamo_50ms_pad_cut_1024_copy'\n",
    "\n",
    "min_loss_net = 1e+10\n",
    "min_loss_seq = 1e+10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    print((datetime.now().strftime('%m-%d %H:%M:%S')))\n",
    "    \n",
    "    try:\n",
    "        load(net, net_optimizer, 'model_saved/{}'.format(keyword))\n",
    "    except:\n",
    "        print('Could not found model_saved/{}'.format(keyword))\n",
    "        \n",
    "    try:\n",
    "        load(jamo2char_net, jamo2char_optimizer, 'model_saved/seq_{}'.format(keyword))\n",
    "    except:\n",
    "        print('Could not found model_saved/seq_{}'.format(keyword))\n",
    "\n",
    "    preloader_train.initialize_batch(num_thread)\n",
    "    loss_list_train = list()\n",
    "    seq2seq_loss_list_train = list()\n",
    "    seq2seq_loss_list_train_ref = list()\n",
    "\n",
    "    total_dist = 0\n",
    "    total_length = 1\n",
    "    total_dist_ref = 0\n",
    "    total_length_ref = 1\n",
    "    \n",
    "    count = 0\n",
    "    net.train()\n",
    "    jamo2char_net.train()\n",
    "    \n",
    "    while preloader_train.end_flag == False:\n",
    "        batch = preloader_train.get_batch()\n",
    "        # logger.info(\"Got Batch\")\n",
    "        if batch != None:\n",
    "            tensor_input, ground_truth, loss_mask, length_list, lev_truth, lev_truth_loss_mask = batch\n",
    "            pred_tensor, loss = train(net, net_optimizer, ctc_loss, tensor_input.to(device),\n",
    "                                      ground_truth.to(device), loss_mask.to(device), length_list.to(device))\n",
    "            loss_list_train.append(loss)\n",
    "\n",
    "            jamo_result = Decode_Prediction_No_Filtering(pred_tensor, tokenizer)\n",
    "            \n",
    "            true_string_list = Decode_Lev_Truth(lev_truth.detach().cpu().numpy(), index2char)\n",
    "            \n",
    "            lev_input_ref = ground_truth\n",
    "            lev_pred_ref, attentions_ref, seq2seq_loss_ref = jamo2char_net.net_train(lev_input_ref.to(device), lev_truth.to(device), lev_truth_loss_mask.to(device), jamo2char_optimizer, seq2seq_criterion)\n",
    "            pred_string_list_ref = Decode_Lev(lev_pred_ref, index2char)\n",
    "            seq2seq_loss_list_train_ref.append(seq2seq_loss_ref)\n",
    "            dist_ref, length_ref = char_distance_list(true_string_list, pred_string_list_ref)\n",
    "            \n",
    "            pred_string_list = [None]\n",
    "            \n",
    "            if (loss < 0.05):\n",
    "                lev_input = Decode_Jamo_Prediction_And_Batch(pred_tensor)\n",
    "                lev_pred, attentions, seq2seq_loss = jamo2char_net.net_train(lev_input.to(device), lev_truth.to(device), lev_truth_loss_mask.to(device), jamo2char_optimizer, seq2seq_criterion)\n",
    "                pred_string_list = Decode_Lev(lev_pred, index2char)\n",
    "                seq2seq_loss_list_train.append(seq2seq_loss)  \n",
    "                dist, length = char_distance_list(true_string_list, pred_string_list)\n",
    "                                \n",
    "\n",
    "            total_dist_ref += dist_ref\n",
    "            total_length_ref += length_ref\n",
    "            \n",
    "            total_dist += dist\n",
    "            total_length += length\n",
    "            \n",
    "            count += 1\n",
    "            if count % 200 == 0:\n",
    "                print(\"Count {} | {} => {}\".format(count, true_string_list[0], pred_string_list_ref[0]))\n",
    "                \n",
    "                plt.figure(figsize = (3, 3))\n",
    "                plt.imshow(attentions_ref[0].detach().cpu().numpy().T)\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"Count {} | {} => {} => {}\".format(count, true_string_list[0], jamo_result[0], pred_string_list[0]))\n",
    "                plt.figure(figsize = (3, 3))\n",
    "                plt.imshow(pred_tensor[:, 0, :].detach().cpu().numpy().T)\n",
    "                plt.show()\n",
    "                \n",
    "                if pred_string_list[0] is not None:\n",
    "                    plt.figure(figsize = (3, 3))\n",
    "                    plt.imshow(attentions[0].detach().cpu().numpy().T)\n",
    "                    plt.show()\n",
    "\n",
    "    train_cer = total_dist / total_length\n",
    "    train_loss = np.mean(np.asarray(loss_list_train))\n",
    "    train_seq2seq_loss = np.mean(np.asarray(seq2seq_loss_list_train))\n",
    "    \n",
    "    train_cer_ref = total_dist_ref / total_length_ref\n",
    "    train_seq2seq_loss_ref = np.mean(np.asarray(seq2seq_loss_list_train_ref))\n",
    "    \n",
    "    print((datetime.now().strftime('%m-%d %H:%M:%S')))\n",
    "    print(\"Mean Train Loss: {}\".format(train_loss))\n",
    "    print(\"Mean Train Seq2Seq Loss: {}\".format(train_seq2seq_loss))\n",
    "    print(\"Train CER: {}\".format(train_cer))\n",
    "    print(\"Mean Train Reference Seq2Seq Loss: {}\".format(train_seq2seq_loss_ref))\n",
    "    print(\"Train Reference CER: {}\".format(train_cer_ref))\n",
    "    \n",
    "    train_loss_history.append(train_loss)\n",
    "    train_cer_history.append(train_cer)\n",
    "    train_cer_history_ref.append(train_cer_ref)\n",
    "    \n",
    "    train_seq2seq_loss_history.append(train_seq2seq_loss)\n",
    "    train_seq2seq_loss_history_ref.append(train_seq2seq_loss_ref)\n",
    "    \n",
    "    ###########################################################\n",
    "    \n",
    "    preloader_eval.initialize_batch(num_thread)\n",
    "    loss_list_eval = list()\n",
    "    seq2seq_loss_list_eval = list()\n",
    "    seq2seq_loss_list_eval_ref = list()\n",
    "\n",
    "    total_dist = 0\n",
    "    total_length = 1\n",
    "    total_dist_ref = 0\n",
    "    total_length_ref = 1\n",
    "    \n",
    "    net.eval()\n",
    "    jamo2char_net.eval()\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while preloader_eval.end_flag == False:\n",
    "        batch = preloader_eval.get_batch()\n",
    "        if batch != None:\n",
    "            tensor_input, ground_truth, loss_mask, length_list, lev_truth, lev_truth_loss_mask = batch\n",
    "            pred_tensor, loss = evaluate(net, ctc_loss, tensor_input.to(device),\n",
    "                                      ground_truth.to(device), loss_mask.to(device), length_list.to(device))\n",
    "            loss_list_eval.append(loss)\n",
    "            \n",
    "            jamo_result = Decode_Prediction_No_Filtering(pred_tensor, tokenizer)\n",
    "            \n",
    "            true_string_list = Decode_Lev_Truth(lev_truth.detach().cpu().numpy(), index2char)\n",
    "            \n",
    "            lev_input_ref = ground_truth\n",
    "            lev_pred_ref, attentions_ref, seq2seq_loss_ref = jamo2char_net.net_eval(lev_input_ref.to(device), lev_truth.to(device), lev_truth_loss_mask.to(device), seq2seq_criterion)\n",
    "            pred_string_list_ref = Decode_Lev(lev_pred_ref, index2char)\n",
    "            seq2seq_loss_list_eval_ref.append(seq2seq_loss_ref)\n",
    "            dist_ref, length_ref = char_distance_list(true_string_list, pred_string_list_ref)\n",
    "            \n",
    "            pred_string_list = [None]\n",
    "            \n",
    "            if (loss < 0.05):\n",
    "                lev_input = Decode_Jamo_Prediction_And_Batch(pred_tensor)\n",
    "                lev_pred, attentions, seq2seq_loss = jamo2char_net.net_eval(lev_input.to(device), lev_truth.to(device), lev_truth_loss_mask.to(device), seq2seq_criterion)\n",
    "                pred_string_list = Decode_Lev(lev_pred, index2char)\n",
    "                seq2seq_loss_list_eval.append(seq2seq_loss)  \n",
    "                dist, length = char_distance_list(true_string_list, pred_string_list)\n",
    "                                \n",
    "            total_dist_ref += dist_ref\n",
    "            total_length_ref += length_ref\n",
    "            \n",
    "            total_dist += dist\n",
    "            total_length += length\n",
    "            \n",
    "            count += 1\n",
    "            if count % 50 == 0:\n",
    "                print(\"Eval Count {} | {} => {}\".format(count, true_string_list[0], pred_string_list_ref[0]))\n",
    "                \n",
    "                plt.figure(figsize = (3, 3))\n",
    "                plt.imshow(attentions_ref[0].detach().cpu().numpy().T)\n",
    "                plt.show()\n",
    "                \n",
    "                print(\"Eval Count {} | {} => {} => {}\".format(count, true_string_list[0], jamo_result[0], pred_string_list[0]))\n",
    "                plt.figure(figsize = (3, 3))\n",
    "                plt.imshow(pred_tensor[:, 0, :].detach().cpu().numpy().T)\n",
    "                plt.show()\n",
    "                \n",
    "                if pred_string_list[0] is not None:\n",
    "                    plt.figure(figsize = (3, 3))\n",
    "                    plt.imshow(attentions[0].detach().cpu().numpy().T)\n",
    "                    plt.show()\n",
    "            \n",
    "    eval_cer = total_dist / total_length\n",
    "    eval_loss = np.mean(np.asarray(loss_list_eval))\n",
    "    eval_seq2seq_loss = np.mean(np.asarray(seq2seq_loss_list_eval)) ##############\n",
    "    \n",
    "    eval_cer_ref = total_dist_ref / total_length_ref\n",
    "    eval_seq2seq_loss_ref = np.mean(np.asarray(seq2seq_loss_list_eval_ref)) ##############\n",
    "    \n",
    "\n",
    "    print((datetime.now().strftime('%m-%d %H:%M:%S')))\n",
    "    print(\"Mean Evaluation Loss: {}\".format(eval_loss))\n",
    "    print(\"Mean Evaluation Seq2Seq Loss: {}\".format(eval_seq2seq_loss)) ##################\n",
    "    print(\"Evaluation CER: {}\".format(eval_cer))\n",
    "    print(\"Mean Evaluation Reference Seq2Seq Loss: {}\".format(eval_seq2seq_loss_ref)) ##################\n",
    "    print(\"Evaluation Reference CER: {}\".format(eval_cer_ref))\n",
    "    \n",
    "    \n",
    "    eval_loss_history.append(eval_loss)\n",
    "    eval_cer_history.append(eval_cer)\n",
    "    eval_cer_history_ref.append(eval_cer_ref)\n",
    "    \n",
    "    eval_seq2seq_loss_history.append(eval_seq2seq_loss) ##################\n",
    "    eval_seq2seq_loss_history_ref.append(eval_seq2seq_loss_ref) ##################\n",
    "    \n",
    "    #####\n",
    "    \n",
    "#     if min_loss_net > train_loss:\n",
    "#         min_loss_net = train_loss\n",
    "\n",
    "    save(net, net_optimizer, 'model_saved/{}'.format(keyword))\n",
    "    \n",
    "    if min_loss_seq > train_seq2seq_loss:\n",
    "        min_loss_seq = train_seq2seq_loss\n",
    "        save(jamo2char_net, jamo2char_optimizer, 'model_saved/seq_{}'.format(keyword))\n",
    "    \n",
    "    np.save('model_saved/train_loss_history_{}'.format(keyword), train_loss_history)\n",
    "    np.save('model_saved/eval_loss_history_{}'.format(keyword), eval_loss_history)\n",
    "    \n",
    "    np.save('model_saved/train_seq_loss_history_{}'.format(keyword), train_seq2seq_loss_history)\n",
    "    np.save('model_saved/eval_seq_loss_history_{}'.format(keyword), eval_seq2seq_loss_history)\n",
    "    \n",
    "    np.save('model_saved/train_cer_history_{}'.format(keyword), train_cer_history)\n",
    "    np.save('model_saved/eval_cer_history_{}'.format(keyword), eval_cer_history)\n",
    "            \n",
    "    #####    \n",
    "    \n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.subplot(131)\n",
    "    plt.plot(train_loss_history)\n",
    "    plt.plot(eval_loss_history)\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.plot(train_seq2seq_loss_history)\n",
    "    plt.plot(eval_seq2seq_loss_history)\n",
    "    \n",
    "    plt.plot(train_seq2seq_loss_history_ref)\n",
    "    plt.plot(eval_seq2seq_loss_history_ref)\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.plot(train_cer_history)\n",
    "    plt.plot(eval_cer_history)\n",
    "    \n",
    "    plt.plot(train_cer_history_ref)\n",
    "    plt.plot(eval_cer_history_ref)\n",
    "    \n",
    "    plt.show()\n",
    "        \n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CTC_best_result_on_Colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
