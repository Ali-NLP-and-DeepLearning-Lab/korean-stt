{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12853, 5)\n",
      "(12833, 5)\n"
     ]
    }
   ],
   "source": [
    "n_mels = 80\n",
    "fs = 44100\n",
    "frame_length_ms=50\n",
    "frame_shift_ms=25\n",
    "nsc = int(fs * frame_length_ms / 1000)\n",
    "nov = nsc - int(fs * frame_shift_ms / 1000)\n",
    "nhop = int(fs * frame_shift_ms / 1000)\n",
    "eps = 1e-8\n",
    "db_ref = 160\n",
    "\n",
    "meta_path = \"D:/korean-single-speaker-speech-dataset/transcript.v.1.2.txt\"\n",
    "data_folder = \"D:/korean-single-speaker-speech-dataset/kss\"\n",
    "\n",
    "with open(meta_path, encoding='utf-8') as f:\n",
    "    metadata = np.array([line.strip().split('|') for line in f])\n",
    "#     hours = sum((int(x[2]) for x in metadata)) * frame_shift_ms / (3600 * 1000)\n",
    "#     log('Loaded metadata for %d examples (%.2f hours)' % (len(metadata), hours))\n",
    "\n",
    "# metadata = metadata[:32, :2]\n",
    "\n",
    "max_sequence_len = max(list(map(len, metadata[:, 1])))\n",
    "\n",
    "error_jamos = [5868, 5998, 6046, 6155, 6202, \n",
    "               6654, 6890, 7486, 7502, 7744, \n",
    "               7765, 8267, 9069, 9927, 10437, \n",
    "               10515, 10533, 10606, 10610, 12777]\n",
    "\n",
    "print(metadata.shape)\n",
    "metadata = np.delete(metadata, error_jamos, axis = 0)\n",
    "print(metadata.shape)\n",
    "\n",
    "dataset_size = len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_length = list()\n",
    "file_length = list()\n",
    "division_length = list()\n",
    "unicode_jamo_list = list()\n",
    "\n",
    "for i in range(len(metadata)):\n",
    "    character_length.append(len(metadata[i, 3]))\n",
    "    file_length.append(float(metadata[i, 4]))\n",
    "    division_length.append(float(metadata[i, 4]) * 1000 / len(metadata[i, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_name_list = []\n",
    "\n",
    "for data in metadata:\n",
    "    wave_name_list.append(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', ',', '.', '<eos>', '<sos>', '?', 'ᄀ', 'ᄁ', 'ᄂ', 'ᄃ', 'ᄄ', 'ᄅ', 'ᄆ', 'ᄇ', 'ᄈ', 'ᄉ', 'ᄊ', 'ᄋ', 'ᄌ', 'ᄍ', 'ᄎ', 'ᄏ', 'ᄐ', 'ᄑ', 'ᄒ', 'ᅡ', 'ᅢ', 'ᅣ', 'ᅤ', 'ᅥ', 'ᅦ', 'ᅧ', 'ᅨ', 'ᅩ', 'ᅪ', 'ᅫ', 'ᅬ', 'ᅭ', 'ᅮ', 'ᅯ', 'ᅰ', 'ᅱ', 'ᅲ', 'ᅳ', 'ᅴ', 'ᅵ', 'ᆨ', 'ᆩ', 'ᆪ', 'ᆫ', 'ᆬ', 'ᆭ', 'ᆮ', 'ᆯ', 'ᆰ', 'ᆱ', 'ᆲ', 'ᆳ', 'ᆴ', 'ᆵ', 'ᆶ', 'ᆷ', 'ᆸ', 'ᆹ', 'ᆺ', 'ᆻ', 'ᆼ', 'ᆽ', 'ᆾ', 'ᆿ', 'ᇀ', 'ᇁ', 'ᇂ']\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "unicode_jamo_list = list()\n",
    "for unicode in range(0x1100, 0x1113):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "    \n",
    "for unicode in range(0x1161, 0x1176):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "    \n",
    "for unicode in range(0x11A8, 0x11C3):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "    \n",
    "unicode_jamo_list += [' ', '!', ',', '.', '?', '<sos>', '<eos>']\n",
    "    \n",
    "unicode_jamo_list.sort()\n",
    "\n",
    "print(unicode_jamo_list)\n",
    "print(len(unicode_jamo_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, vocabs):\n",
    "        self.vocabs = vocabs\n",
    "        \n",
    "    def word2num(self, sentence):\n",
    "        tokens = list()\n",
    "        for char in sentence:\n",
    "            tokens.append(self.vocabs.index(char))    \n",
    "        return tokens\n",
    "        \n",
    "    def word2vec(self, sentence):\n",
    "        vectors = np.zeros((len(sentence), len(self.vocabs)))\n",
    "        for i, char in enumerate(sentence):\n",
    "            vectors[i, self.vocabs.index(char)] = 1   \n",
    "        return vectors\n",
    "    \n",
    "    def num2word(self, num):\n",
    "        output = list()\n",
    "        for i in num:\n",
    "            output.append(self.vocabs[i])\n",
    "        return output\n",
    "    \n",
    "    def num2vec(self, numbers):\n",
    "        vectors = np.zeros((len(numbers), len(self.vocabs)))\n",
    "        for i, num in enumerate(numbers):\n",
    "            vectors[i, num] = 1   \n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(unicode_jamo_list)\n",
    "jamo_tokens = tokenizer.word2num(unicode_jamo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(jamo_tokens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02d0749e35745088994ebaa61f2ea83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12833), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mel_path_list = list()\n",
    "\n",
    "for i, wav_name in enumerate(tqdm(wave_name_list)):\n",
    "    \n",
    "    npy_name = wav_name.replace('.wav', '.npy')\n",
    "    wav_path = os.path.join(data_folder, wav_name)  \n",
    "    mel_path = os.path.join(data_folder + '/mel', npy_name)\n",
    "    mel_path_list.append(mel_path)\n",
    "    \n",
    "    if not os.path.isfile(mel_path):\n",
    "#         print(\"{}\".format(mel_path))\n",
    "        y, sr = librosa.core.load(wav_path)\n",
    "        f, t, Zxx = sp.signal.stft(y, fs=sr, nperseg=nsc, noverlap=nov)\n",
    "        Sxx = np.abs(Zxx)\n",
    "        Sxx = np.maximum(Sxx, eps)\n",
    "\n",
    "        # plt.figure(figsize=(20,20))\n",
    "        # plt.imshow(20*np.log10(Sxx), origin='lower')\n",
    "        # plt.colorbar()\n",
    "        # plt.show()\n",
    "\n",
    "        mel_filters = librosa.filters.mel(sr=fs, n_fft=nsc, n_mels=n_mels)\n",
    "        mel_specgram = np.matmul(mel_filters, Sxx)\n",
    "\n",
    "    #   log_specgram = 20*np.log10(Sxx)\n",
    "    #   norm_log_specgram = (log_specgram + db_ref) / db_ref\n",
    "\n",
    "        log_mel_specgram = 20 * np.log10(np.maximum(mel_specgram, eps))\n",
    "        norm_log_mel_specgram = (log_mel_specgram + db_ref) / db_ref\n",
    "\n",
    "    #   np.save(specgram_path, norm_log_specgram)\n",
    "        np.save(mel_path, norm_log_mel_specgram)\n",
    "    #   np.save(specgram_path, Sxx)\n",
    "\n",
    "    #     print(norm_log_mel_specgram.shape[1])\n",
    "\n",
    "    #     if i % 1000 == 0:\n",
    "    #         plt.figure(figsize=(8, 4))\n",
    "    #         plt.imshow(20 * np.log10(Sxx), origin='lower', aspect='auto')\n",
    "    #         plt.colorbar()\n",
    "    #         plt.show()\n",
    "\n",
    "    #         plt.figure(figsize=(8, 4))\n",
    "    #         plt.imshow(norm_log_mel_specgram, origin='lower', aspect='auto')\n",
    "    #         plt.colorbar()\n",
    "    #         plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer = nn.Embedding(len(jamo_tokens), 256)\n",
    "\n",
    "# print(metadata[5031, 3])\n",
    "# print(metadata[5031, 2])\n",
    "# print(len(metadata[5031, 3]))\n",
    "\n",
    "# input_token = tokenizer.word2num(metadata[5031, 3])\n",
    "# input_tensor = torch.tensor(input_token)\n",
    "# plt.imshow(embedding_layer(input_tensor).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGRU(nn.Module):\n",
    "    def __init__(self, D_in, bidirectional=True):\n",
    "        super(ResidualGRU, self).__init__()\n",
    "        self.gru = nn.GRU(D_in, int(D_in/2), bidirectional=bidirectional, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "#         print(\"Residual Input: {}\".format(input_tensor.shape))\n",
    "        gru_output, _ = self.gru(input_tensor)\n",
    "        activated = self.relu(gru_output)  \n",
    "#         print(\"Residual Output: {}\".format(activated.shape))\n",
    "        output_tensor = torch.add(activated, input_tensor)\n",
    "        \n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.H = H\n",
    "        \n",
    "        self.fc = torch.nn.Linear(D_in, H)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc_2 = torch.nn.Linear(H, H)\n",
    "        self.relu_2 = torch.nn.ReLU()\n",
    "        self.dropout_2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.gru = nn.GRU(H, D_out, bidirectional=True, batch_first=True)\n",
    "        self.relu_gru = torch.nn.ReLU()\n",
    "        \n",
    "        self.residual_gru_layers = nn.ModuleList([ResidualGRU(H, bidirectional=True) for i in range(3)])\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "                        \n",
    "        output_tensor = self.fc(input_tensor)\n",
    "        output_tensor = self.relu(output_tensor)\n",
    "        output_tensor = self.dropout(output_tensor)      \n",
    "        \n",
    "        output_tensor, _ = self.gru(output_tensor)\n",
    "        output_tensor = self.relu_gru(output_tensor)\n",
    "        \n",
    "        for layer in self.residual_gru_layers:\n",
    "            output_tensor = layer(output_tensor)\n",
    "        \n",
    "        output_tensor = self.fc_2(output_tensor)\n",
    "        output_tensor = self.relu_2(output_tensor)\n",
    "        output_tensor = self.dropout_2(output_tensor)\n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, H, D_out):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.H = H\n",
    "        self.fc_embed = nn.Linear(256, 1024)\n",
    "        self.relu_embed = torch.nn.ReLU()\n",
    "        self.dropout_embed = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.gru = nn.GRU(2 * H, H, batch_first=True)\n",
    "#         self.attention = AdditiveAttentionModule(D_out * 2)\n",
    "        self.attention = MultiplicativeAttentionModule(D_out * 2)\n",
    "        self.fc = nn.Linear(1024, 512)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 74)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_tensor, query):\n",
    "#         print(\"Input tensor shape in Decoder: {}\".format(input_tensor.shape))\n",
    "#         print(\"Hidden_tensor shape in Decoder: {}\".format(hidden_tensor.shape))\n",
    "        output_tensor = self.fc_embed(input_tensor)\n",
    "        output_tensor = self.relu_embed(output_tensor)\n",
    "        output_tensor = self.dropout_embed(output_tensor)\n",
    "    \n",
    "#         print(\"Output tensor shape in Decoder: {}\".format(output_tensor.shape))\n",
    "#         print(\"Output Tensor Shape: {}\".format(output_tensor.shape))\n",
    "        output_tensor, hidden_tensor = self.gru(output_tensor, hidden_tensor)\n",
    "        \n",
    "#         print(\"Hidden_tensor shape in Decoder: {}\".format(hidden_tensor.shape))\n",
    "#         print(\"Output tensor shape in Decoder: {}\".format(output_tensor.shape))\n",
    "    \n",
    "        context_vector, alpha = self.attention(query, output_tensor)\n",
    "        output_tensor = torch.cat([output_tensor, context_vector], dim=2)\n",
    "#         print('output_tensor: {}'.format(output_tensor.shape))\n",
    "#         print('output_tensor: {}'.format(context_vector.shape))\n",
    "        output_tensor = self.fc(output_tensor)\n",
    "        output_tensor = self.relu(output_tensor)\n",
    "        output_tensor = self.dropout(output_tensor)        \n",
    "        \n",
    "        output_tensor = self.fc2(output_tensor)\n",
    "        prediction_tensor = self.softmax(output_tensor)\n",
    "\n",
    "        return prediction_tensor, hidden_tensor, context_vector, alpha\n",
    "\n",
    "class AdditiveAttentionModule(torch.nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(AdditiveAttentionModule, self).__init__()\n",
    "        self.fc_alpha = nn.Linear(H, 1)\n",
    "        self.W = nn.Linear(H, H)\n",
    "        self.V = nn.Linear(H, H)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, query, key):\n",
    "        output_tensor = torch.tanh(torch.add(self.W(query), self.V(key)))\n",
    "        e = self.fc_alpha(output_tensor)\n",
    "        e_sig = self.sigmoid(e)\n",
    "        alpha = self.softmax(e_sig).transpose(1, 2)\n",
    "        context_vector = torch.bmm(alpha, query)\n",
    "        \n",
    "        return context_vector, alpha\n",
    "    \n",
    "class MultiplicativeAttentionModule(torch.nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(MultiplicativeAttentionModule, self).__init__()\n",
    "#         self.W = nn.Linear(H, H)\n",
    "#         self.V = nn.Linear(H, H)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, query, key):\n",
    "#         output_tensor = torch.tanh(torch.bmm(self.W(query), self.V(key)))\n",
    "#         print(\"Query shape: {}\".format(query.shape))\n",
    "#         print(\"Key shape: {}\".format(key.shape))\n",
    "        output_tensor = torch.bmm(query, key.transpose(1, 2))\n",
    "#         print(\"Output shape: {}\".format(output_tensor.shape))\n",
    "#         e_sig = self.sigmoid(output_tensor)\n",
    "        alpha = self.softmax(output_tensor).transpose(1, 2)\n",
    "#         print('Alpha shape: {}'.format(alpha.shape))\n",
    "        context_vector = torch.bmm(alpha, query)\n",
    "        \n",
    "        return context_vector, alpha\n",
    "\n",
    "class Mel2SeqNet():\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Mel2SeqNet, self).__init__()\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.H = H\n",
    "        \n",
    "        self.encoder = Encoder(D_in, H, D_out).to(device)\n",
    "        self.embedding_layer = nn.Embedding(len(jamo_tokens), 256).to(device)\n",
    "        self.decoder = Decoder(H, D_out).to(device)\n",
    "#         self.encoder_optimizer = optim.SGD(self.encoder.parameters(), lr=0.01)\n",
    "#         self.decoder_optimizer = optim.SGD(self.decoder.parameters(), lr=0.01)\n",
    "#         self.embedding_optimizer = optim.SGD(self.embedding_layer.parameters(), lr=0.01)\n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=0.1)\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=0.1)\n",
    "        self.embedding_optimizer = optim.Adam(self.embedding_layer.parameters(), lr=0.1)\n",
    "\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def train(self, input_tensor, ground_truth, loss_mask):\n",
    "        \n",
    "        batch_size = input_tensor.shape[0]\n",
    "\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        self.embedding_optimizer.zero_grad()\n",
    "\n",
    "        encoded_tensor = self.encoder(input_tensor)\n",
    "        decoder_hidden = encoded_tensor[:, -1, :].view(1, batch_size, self.H).contiguous()\n",
    "#         decoder_hidden = encoded_tensor[:, -1, :].transpose(0, 1)\n",
    "        \n",
    "        pred_tensor_list = list()\n",
    "        att_weight_list = list()\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        decoder_input = self.embedding_layer(torch.tensor(tokenizer.word2num(['<sos>'] * batch_size)).to(device)).view([batch_size, 1, -1])\n",
    "        \n",
    "#         print(encoded_tensor.shape)\n",
    "#         print(decoder_hidden.shape)\n",
    "#         print(decoder_input.shape)\n",
    "        \n",
    "        for i in range(ground_truth.shape[1]):\n",
    "            \n",
    "            pred_tensor, decoder_hidden, context_vector, att_weight = self.decoder(decoder_input, decoder_hidden, encoded_tensor)\n",
    "            pred_tensor_list.append(pred_tensor)\n",
    "            att_weight_list.append(att_weight)\n",
    "            \n",
    "#             print('pred_tensor shape: {}'.format(pred_tensor.shape))\n",
    "            truth = ground_truth[:, i]\n",
    "            truth = truth.type(torch.cuda.LongTensor)\n",
    "            \n",
    "            loss += torch.dot(loss_mask[:, i], self.criterion(pred_tensor.view([batch_size, -1]), truth)) / batch_size\n",
    "            decoder_input = self.embedding_layer(truth).view([batch_size, 1, -1])\n",
    "#             pred_tensor_list.append(torch.tensor(tokenizer.num2vec(truth)).view(batch_size, 1, -1))\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        self.embedding_optimizer.step()\n",
    "        \n",
    "        prediction = torch.cat(pred_tensor_list, dim=1)\n",
    "        attention_matrix = torch.cat(att_weight_list, dim=1)\n",
    "        \n",
    "        return prediction, attention_matrix, loss.item() / ground_truth.shape[1]\n",
    "    \n",
    "    def save(self, check_point_name):\n",
    "        torch.save({\n",
    "            'embedding_layer_state_dict': self.embedding_layer.state_dict(),\n",
    "            'encoder_state_dict': self.encoder.state_dict(),\n",
    "            'decoder_state_dict': self.decoder.state_dict(),\n",
    "            'embedding_optimizer_state_dict': self.embedding_optimizer.state_dict(),\n",
    "            'encoder_optimizer_state_dict': self.encoder_optimizer.state_dict(),\n",
    "            'decoder_optimizer_state_dict': self.decoder_optimizer.state_dict(),\n",
    "            }, check_point_name)\n",
    "    \n",
    "    def load(self, check_point_name):\n",
    "        checkpoint = torch.load(check_point_name)\n",
    "        self.embedding_layer.load_state_dict(checkpoint['embedding_layer_state_dict'])\n",
    "        self.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        self.decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "        self.embedding_optimizer.load_state_dict(checkpoint['embedding_optimizer_state_dict'])\n",
    "        self.encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "        self.decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "    \n",
    "        self.embedding_layer.eval()\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        self.embedding_layer.train()\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "    \n",
    "net = Mel2SeqNet(80, 512, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")      \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Preloader():\n",
    "#     def __init__(self, mel_path_list):\n",
    "#         super(Preloader).__init__()\n",
    "#         self.mel_path_list = mel_path_list\n",
    "#         self.tensor_input_list = [None] * len(mel_path_list)\n",
    "    \n",
    "#     def load(self, i):\n",
    "#         norm_log_mel_specgram = np.load(self.mel_path_list[i])\n",
    "#         input_spectrogram = norm_log_mel_specgram.T\n",
    "#         tensor_input = torch.tensor(input_spectrogram).view(1, input_spectrogram.shape[0], input_spectrogram.shape[1])\n",
    "#         self.tensor_input_list[i] = tensor_input\n",
    "        \n",
    "#     def get(self, i):\n",
    "#         if type(self.tensor_input_list[i]) == type(None):\n",
    "#             self.load(i)\n",
    "#         return self.tensor_input_list[i]  \n",
    "    \n",
    "#     def get_batch(self):\n",
    "        \n",
    "#         return batched_tensor, indxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batched_Preloader():\n",
    "    def __init__(self, mel_path_list, ground_truth_list, batch_size):\n",
    "        super(Batched_Preloader).__init__()\n",
    "        self.mel_path_list = mel_path_list\n",
    "        self.total_num_input = len(mel_path_list)\n",
    "        self.tensor_input_list = [None] * self.total_num_input\n",
    "        self.ground_truth_list = ground_truth_list\n",
    "        self.sentence_length_list = np.asarray(list(map(len, ground_truth_list)))\n",
    "        self.shuffle_step = 4\n",
    "        self.loading_sequence = None\n",
    "        self.end_flag = True\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def load(self, i):\n",
    "        norm_log_mel_specgram = np.load(self.mel_path_list[i])\n",
    "        input_spectrogram = norm_log_mel_specgram.T\n",
    "        tensor_input = torch.tensor(input_spectrogram).view(1, input_spectrogram.shape[0], input_spectrogram.shape[1])\n",
    "        self.tensor_input_list[i] = tensor_input\n",
    "        \n",
    "    def get(self, i):\n",
    "        if type(self.tensor_input_list[i]) == type(None):\n",
    "            self.load(i)\n",
    "        return self.tensor_input_list[i]  \n",
    "    \n",
    "    def initialize_batch(self):\n",
    "        loading_sequence = np.argsort(self.sentence_length_list)\n",
    "        bundle = np.stack([self.sentence_length_list[loading_sequence], loading_sequence])\n",
    "        \n",
    "        for seq_len in range(self.shuffle_step, np.max(self.sentence_length_list), self.shuffle_step):\n",
    "            idxs = np.where((bundle[0, :] > seq_len) & (bundle[0, :] <= seq_len + self.shuffle_step))[0]\n",
    "            idxs_origin = copy.deepcopy(idxs)\n",
    "            random.shuffle(idxs)\n",
    "            bundle[:, idxs_origin] = bundle[:, idxs]\n",
    "            \n",
    "        loading_sequence = bundle[1, :]\n",
    "        \n",
    "        self.loading_sequence = loading_sequence\n",
    "        self.current_loading_index = 0\n",
    "        self.end_flag = False\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def get_batch(self):\n",
    "        \n",
    "        tensor_list = list()\n",
    "        ground_truth_list = list()\n",
    "        tensor_size_list = list()\n",
    "        ground_truth_size_list = list()\n",
    "        \n",
    "        count = 0\n",
    "        max_seq_len = 0\n",
    "        max_sen_len = 0\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            if self.current_loading_index >= self.total_num_input:\n",
    "                self.end_flag = True\n",
    "                break\n",
    "            \n",
    "            tensor = self.get(self.loading_sequence[self.current_loading_index])\n",
    "            tensor_list.append(tensor)\n",
    "            tensor_size_list.append(tensor.shape[1])\n",
    "            \n",
    "            ground_truth = self.ground_truth_list[self.loading_sequence[self.current_loading_index]]\n",
    "            ground_truth_list.append(ground_truth)\n",
    "            ground_truth_size_list.append(len(ground_truth))\n",
    "            \n",
    "            \n",
    "            if (tensor.shape[1] > max_seq_len):\n",
    "                max_seq_len = tensor.shape[1]\n",
    "            if (len(ground_truth) > max_sen_len):\n",
    "                max_sen_len = len(ground_truth)  \n",
    "            \n",
    "            self.current_loading_index += 1\n",
    "            count += 1\n",
    "            \n",
    "        batched_tensor = torch.zeros(count, max_seq_len, n_mels)\n",
    "        batched_ground_truth = torch.zeros(count, max_sen_len)\n",
    "        batched_loss_mask = torch.zeros(count, max_sen_len)\n",
    "        \n",
    "        for order in range(count):\n",
    "            batched_tensor[order, :tensor_size_list[order], :] = tensor_list[order]\n",
    "#             print(ground_truth_size_list[order])\n",
    "            batched_ground_truth[order, :ground_truth_size_list[order]] = torch.tensor(ground_truth_list[order])\n",
    "            batched_loss_mask[order, :ground_truth_size_list[order]] = torch.ones(ground_truth_size_list[order])\n",
    "        \n",
    "        return batched_tensor, batched_ground_truth, batched_loss_mask\n",
    "        \n",
    "#         return batched_tensor, ground_truth_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_list = [(tokenizer.word2num(list(metadata[i, 3]) + ['<eos>'])) for i in range(len(metadata))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preloader = Preloader(mel_path_list)\n",
    "preloader = Batched_Preloader(mel_path_list, ground_truth_list, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08-26 20:28:24\n",
      "Calculated 100 Batches\n",
      "Loss 100: 4.049277305603027\n",
      "08-26 20:28:45\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋᄋ\n",
      "제 아내는 인형처럼 예뻐요.<eos> \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-d92ccacd28ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREPEAT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mpred_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-fc51e1d1e41f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_tensor, ground_truth, loss_mask)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;31m#             pred_tensor_list.append(torch.tensor(tokenizer.num2vec(truth)).view(batch_size, 1, -1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCH = 30\n",
    "REPEAT = 1\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    try:\n",
    "        net.load('check_point_test_adam')\n",
    "    except:\n",
    "        print(\"Loading Error\")\n",
    "    preloader.initialize_batch()\n",
    "    counter = 0\n",
    "    loss_list = list()\n",
    "    print(datetime.now().strftime('%m-%d %H:%M:%S'))\n",
    "    \n",
    "    while preloader.end_flag == False:\n",
    "        tensor_input, ground_truth, loss_mask = preloader.get_batch()\n",
    "        \n",
    "        for i in range(REPEAT):\n",
    "            pred_tensor, attention_matrix, loss = net.train(tensor_input.to(device), ground_truth.to(device), loss_mask.to(device))\n",
    "        \n",
    "        counter += 1\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        if (counter % 100 == 0):\n",
    "            print('Calculated {} Batches'.format(counter))\n",
    "            print('Loss {}: {}'.format(counter, loss))\n",
    "            print(datetime.now().strftime('%m-%d %H:%M:%S'))\n",
    "\n",
    "            plt.figure()\n",
    "            plt.imshow(attention_matrix[0].detach().cpu().numpy())\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "            _, index = pred_tensor[0].max(-1)\n",
    "            sentence = tokenizer.num2word(index.view(-1))\n",
    "            print(''.join(sentence))\n",
    "            true_sentence = tokenizer.num2word(ground_truth[0, :].detach().numpy().astype(int))\n",
    "            print(''.join(true_sentence))\n",
    "            \n",
    "    net.save('check_point_test_adam')\n",
    "    print(\"Mean Loss: {}\".format(np.mean(np.asarray(loss_list))))\n",
    "    print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EPOCH = 1\n",
    "\n",
    "# for epoch in range(EPOCH):\n",
    "    \n",
    "#     net.load('check_point_test')\n",
    "    \n",
    "#     for i in tqdm(range(len(metadata))):\n",
    "#         tensor_input = preloader.get(i)\n",
    "#         ground_truth = ground_truth_list[i]\n",
    "        \n",
    "#         pred_tensor, attention_matrix, loss = net.train(tensor_input.to(device), ground_truth.to(device))\n",
    "        \n",
    "#         if (i % 100 == 0):\n",
    "#             print(datetime.datetime.now())\n",
    "#             print('Loss {}: {}'.format(i, loss))\n",
    "#             plt.figure()\n",
    "#             plt.imshow(attention_matrix[0].detach().cpu().numpy())\n",
    "#             plt.colorbar()\n",
    "#             plt.show()\n",
    "#             _, index = pred_tensor.max(-1)\n",
    "#             sentence = tokenizer.num2word(index.view(-1))\n",
    "#             print(''.join(sentence))\n",
    "#             print(metadata[i, 2])\n",
    "    \n",
    "#     net.save('check_point_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.imshow(attention_matrix[0].detach().cpu().numpy())\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(attention_matrix[0, 0, :].shape)\n",
    "# print(sum(attention_matrix[0, 0, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.save('check_point_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.load('check_point_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
