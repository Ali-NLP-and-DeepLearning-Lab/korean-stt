{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNy2clEZPKAv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa # librosa: Audio handling package\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import jamotools\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm # tqdm: Pakage for progress bar visualization\n",
    "from datetime import datetime\n",
    "\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "import Levenshtein as Lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "SBaEfy86PKA1",
    "outputId": "4cde26f5-f3e4-4589-871e-e204684d376e"
   },
   "outputs": [],
   "source": [
    "n_mels = 80\n",
    "fs = 16000\n",
    "frame_length_ms=50\n",
    "frame_shift_ms=25\n",
    "nsc = int(fs * frame_length_ms / 1000)\n",
    "nov = nsc - int(fs * frame_shift_ms / 1000)\n",
    "nhop = int(fs * frame_shift_ms / 1000)\n",
    "eps = 1e-8\n",
    "db_ref = 160\n",
    "\n",
    "# meta_path = 'gdrive/My Drive/korean-single-speaker-speech-dataset/transcript.v.1.2.txt'\n",
    "# data_folder = 'gdrive/My Drive/korean-single-speaker-speech-dataset/kss'\n",
    "\n",
    "# meta_path = \"D:/korean-single-speaker-speech-dataset/transcript.v.1.2.txt\"\n",
    "# data_folder = \"D:/korean-single-speaker-speech-dataset/kss\"\n",
    "\n",
    "data_folder = \"D:/nsml-dataset/train_data/\"\n",
    "label_path = \"D:/nsml-dataset/hackathon.labels\"\n",
    "\n",
    "data_list = glob.glob(data_folder + '*.csv')[0]\n",
    "\n",
    "wav_paths = list()\n",
    "script_paths = list()\n",
    "korean_script_paths = list()\n",
    "\n",
    "with open(data_list, 'r') as f:\n",
    "    for line in f:\n",
    "        # line: \"aaa.wav,aaa.label\"\n",
    "        wav_path, script_path = line.strip().split(',')\n",
    "        korean_script_path = script_path.replace('.label', '.script')\n",
    "        \n",
    "        wav_paths.append(os.path.join(data_folder, wav_path))\n",
    "        script_paths.append(os.path.join(data_folder, script_path))\n",
    "        korean_script_paths.append(os.path.join(data_folder, korean_script_path))\n",
    "\n",
    "dataset_size = len(wav_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(filepath, bos_id, eos_id):\n",
    "    key = filepath.split('/')[-1].split('.')[0]\n",
    "    script = target_dict[key]\n",
    "    tokens = script.split(' ')\n",
    "    result = list()\n",
    "    result.append(bos_id)\n",
    "    for i in range(len(tokens)):\n",
    "        if len(tokens[i]) > 0:\n",
    "            result.append(int(tokens[i]))\n",
    "    result.append(eos_id)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav_paths len: 29805\n",
      "script_paths len: 29805\n",
      "korean_script_paths len: 29805\n",
      "D:/nsml-dataset/train_data/41_0601_211_0_07930_02.wav\n",
      "D:/nsml-dataset/train_data/41_0601_211_0_07930_02.label\n",
      "D:/nsml-dataset/train_data/41_0601_211_0_07930_02.script\n"
     ]
    }
   ],
   "source": [
    "print(\"wav_paths len: {}\".format(len(wav_paths)))\n",
    "print(\"script_paths len: {}\".format(len(script_paths)))\n",
    "print(\"korean_script_paths len: {}\".format(len(korean_script_paths)))\n",
    "\n",
    "print(wav_paths[0])\n",
    "print(script_paths[0])\n",
    "print(korean_script_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(script_paths[1]) as f:\n",
    "    line = f.read()\n",
    "    line = line.strip()\n",
    "    result = list(map(int, line.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_label(label_path):\n",
    "    char2index = dict() # [ch] = id\n",
    "    index2char = dict() # [id] = ch\n",
    "    with open(label_path, 'r', encoding='UTF-8') as f:\n",
    "    # with open(label_path, 'r') as f:\n",
    "        for no, line in enumerate(f):\n",
    "            if line[0] == '#': \n",
    "                continue\n",
    "\n",
    "            index, char, freq = line.strip().split('\\t')\n",
    "            char = char.strip()\n",
    "            if len(char) == 0:\n",
    "                char = ' '\n",
    "\n",
    "            char2index[char] = int(index)\n",
    "            index2char[int(index)] = char\n",
    "\n",
    "    return char2index, index2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2index, index2char = load_label(label_path)\n",
    "SOS_token = char2index['<s>']  # '<sos>' or '<s>'\n",
    "EOS_token = char2index['</s>']  # '<eos>' or '</s>'\n",
    "PAD_token = char2index['_']  # '-' or '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char2index len: 820\n",
      "index2char len: 820\n"
     ]
    }
   ],
   "source": [
    "print('char2index len: {}'.format(len(char2index)))\n",
    "\n",
    "print('index2char len: {}'.format(len(index2char)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ᄀ', 'ᄁ', 'ᄂ', 'ᄃ', 'ᄄ', 'ᄅ', 'ᄆ', 'ᄇ', 'ᄈ', 'ᄉ', 'ᄊ', 'ᄋ', 'ᄌ', 'ᄍ', 'ᄎ', 'ᄏ', 'ᄐ', 'ᄑ', 'ᄒ', 'ᅡ', 'ᅢ', 'ᅣ', 'ᅤ', 'ᅥ', 'ᅦ', 'ᅧ', 'ᅨ', 'ᅩ', 'ᅪ', 'ᅫ', 'ᅬ', 'ᅭ', 'ᅮ', 'ᅯ', 'ᅰ', 'ᅱ', 'ᅲ', 'ᅳ', 'ᅴ', 'ᅵ', 'ᆨ', 'ᆩ', 'ᆪ', 'ᆫ', 'ᆬ', 'ᆭ', 'ᆮ', 'ᆯ', 'ᆰ', 'ᆱ', 'ᆲ', 'ᆳ', 'ᆴ', 'ᆵ', 'ᆶ', 'ᆷ', 'ᆸ', 'ᆹ', 'ᆺ', 'ᆻ', 'ᆼ', 'ᆽ', 'ᆾ', 'ᆿ', 'ᇀ', 'ᇁ', 'ᇂ', ' ', '!', ',', '.', '?']\n"
     ]
    }
   ],
   "source": [
    "pure_jamo_list = list()\n",
    "\n",
    "# 초성\n",
    "for unicode in range(0x1100, 0x1113):\n",
    "    pure_jamo_list.append(chr(unicode))  # chr: Change hexadecimal to unicode\n",
    "# 중성\n",
    "for unicode in range(0x1161, 0x1176):\n",
    "    pure_jamo_list.append(chr(unicode))\n",
    "# 종성\n",
    "for unicode in range(0x11A8, 0x11C3):\n",
    "    pure_jamo_list.append(chr(unicode))\n",
    "\n",
    "pure_jamo_list += [' ', '!', ',', '.', '?']\n",
    "\n",
    "print(pure_jamo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff964df449fd42c381647770355118bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=29805), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "korean_script_list = list()\n",
    "jamo_script_list = list()\n",
    "\n",
    "# true_jamo_regex_not  = re.compile(u'[^, .?!\\u1100-\\u115e\\u1161-\\u11A7\\u11a8-\\u11ff]+')\n",
    "\n",
    "# valid_regex = re.compile(u'[,_ ^.?!？~<>:;/%()+A-Za-z0-9\\u1100-\\u115e\\u1161-\\u11A7\\u11a8-\\u11ff]+')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for file in tqdm(korean_script_paths):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        line = f.read()\n",
    "        line = line.strip()\n",
    "        korean_script_list.append(line)\n",
    "        jamo = list(jamotools.split_syllables(line, 'JAMO'))\n",
    "        \n",
    "#         print(len(jamo))\n",
    "        for i, c in enumerate(jamo):\n",
    "            if c not in pure_jamo_list:\n",
    "                jamo[i] = '*'\n",
    "        \n",
    "        jamo = ''.join(jamo)\n",
    "#         jamo_filtered = ''.join(true_jamo_regex.findall(jamo))\n",
    "#         jamo_filtered = re.sub(true_jamo_regex_not, '*', jamo)\n",
    "        jamo_script_list.append(jamo)\n",
    "        \n",
    "#         print(line)\n",
    "#         print(jamo) \n",
    "        \n",
    "#         if count == 100:\n",
    "#             break\n",
    "        \n",
    "#         count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_starting_point(coef, thres = 0.1, margin = 10):\n",
    "    starting_point = 0\n",
    "    for i in range(len(coef) - 1):\n",
    "        if (coef[i] <= thres and coef[i+1] > thres):\n",
    "            starting_point = i\n",
    "            break\n",
    "            \n",
    "    starting_point = starting_point - margin\n",
    "    \n",
    "    if starting_point < 0:\n",
    "        starting_point = 0\n",
    "    \n",
    "    return starting_point\n",
    "\n",
    "def find_ending_point(coef, thres = 0.1, margin = 10):\n",
    "    for i in range(len(coef) - 1, 0, -1):\n",
    "        if (coef[i] <= thres and coef[i-1] > thres):\n",
    "            ending_point = i\n",
    "            break\n",
    "            \n",
    "    ending_point = ending_point + margin\n",
    "    \n",
    "    if ending_point > len(coef):\n",
    "        ending_point = len(coef)\n",
    "\n",
    "    return ending_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Threading_Batched_Preloader():\n",
    "    def __init__(self, wav_path_list, ground_truth_list, script_path_list, batch_size, is_train=True):\n",
    "        super(Threading_Batched_Preloader).__init__()\n",
    "        self.wav_path_list = wav_path_list\n",
    "        self.total_num_input = len(wav_path_list)\n",
    "        self.tensor_input_list = [None] * self.total_num_input\n",
    "        self.ground_truth_list = ground_truth_list\n",
    "        self.script_path_list = script_path_list\n",
    "        self.sentence_length_list = np.asarray(list(map(len, ground_truth_list)))\n",
    "        self.shuffle_step = 12\n",
    "        \n",
    "        self.shuffle_step = 4\n",
    "        \n",
    "        self.loading_sequence = None\n",
    "        self.end_flag = False\n",
    "        self.batch_size = batch_size\n",
    "        self.queue = queue.Queue(16)\n",
    "        self.thread_flags = list()\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    # Shuffle loading index and set end flag to false\n",
    "    def initialize_batch(self, thread_num):\n",
    "        loading_sequence = np.argsort(self.sentence_length_list)\n",
    "        bundle = np.stack([self.sentence_length_list[loading_sequence], loading_sequence])\n",
    "\n",
    "        for seq_len in range(self.shuffle_step, np.max(self.sentence_length_list), self.shuffle_step):\n",
    "            idxs = np.where((bundle[0, :] > seq_len) & (bundle[0, :] <= seq_len + self.shuffle_step))[0]\n",
    "            idxs_origin = copy.deepcopy(idxs)\n",
    "            random.shuffle(idxs)\n",
    "            bundle[:, idxs_origin] = bundle[:, idxs]\n",
    "            \n",
    "        loading_sequence = bundle[1, :]\n",
    "        loading_sequence_len = len(loading_sequence)\n",
    "        \n",
    "#         print(\"Loading Sequence Length: {}\".format(loading_sequence_len))\n",
    "        \n",
    "        thread_size = int(np.ceil(loading_sequence_len / thread_num))\n",
    "\n",
    "        load_idxs_list = list()\n",
    "        for i in range(thread_num):\n",
    "            start_idx = i * thread_size\n",
    "            end_idx = (i + 1) * thread_size\n",
    "\n",
    "            if end_idx > loading_sequence_len:\n",
    "                end_idx = loading_sequence_len\n",
    "\n",
    "            load_idxs_list.append(loading_sequence[start_idx:end_idx])\n",
    "            \n",
    "#         for i in range(thread_num):\n",
    "#             print(len(load_idxs_list[i]))\n",
    "\n",
    "        self.end_flag = False\n",
    "        \n",
    "        self.queue = queue.Queue(32)\n",
    "        self.thread_flags = [False] * thread_num\n",
    "        \n",
    "        self.thread_list = [Batching_Thread(self.wav_path_list, self.ground_truth_list, self.script_path_list, load_idxs_list[i], self.queue, self.batch_size, self.thread_flags, i, self.is_train) for i in range(thread_num)]\n",
    "\n",
    "        for thread in self.thread_list:\n",
    "            thread.start()\n",
    "        return\n",
    "\n",
    "    def check_thread_flags(self):\n",
    "        for flag in self.thread_flags:\n",
    "            if flag == False:\n",
    "                return False\n",
    "        \n",
    "        if (self.queue.empty):\n",
    "            self.end_flag = True\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_batch(self):\n",
    "        while not (self.check_thread_flags()):\n",
    "            batch = self.queue.get()\n",
    "\n",
    "            if (batch != None):\n",
    "                batched_tensor = batch[0]\n",
    "                batched_ground_truth = batch[1] \n",
    "                batched_loss_mask = batch[2]\n",
    "                ground_truth_size_list = batch[3]\n",
    "                batched_lev_truth = batch[4]\n",
    "                batched_lev_truth_loss_mask = batch[5]\n",
    "                \n",
    "                return batched_tensor, batched_ground_truth, batched_loss_mask, ground_truth_size_list, batched_lev_truth, batched_lev_truth_loss_mask\n",
    "\n",
    "        return None\n",
    "\n",
    "class Batching_Thread(threading.Thread):\n",
    "\n",
    "    def __init__(self, wav_path_list, ground_truth_list, script_path_list, load_idxs_list, queue, batch_size, thread_flags, id, is_train=True):\n",
    "        \n",
    "        threading.Thread.__init__(self)\n",
    "        self.wav_path_list = wav_path_list\n",
    "        self.ground_truth_list = ground_truth_list\n",
    "        self.script_path_list = script_path_list\n",
    "        self.load_idxs_list = load_idxs_list\n",
    "        self.list_len = len(load_idxs_list)\n",
    "        self.cur_idx = 0\n",
    "        self.id = id\n",
    "        self.queue = queue\n",
    "        self.batch_size = batch_size \n",
    "        self.thread_flags = thread_flags\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def run(self):\n",
    "\n",
    "        while(self.cur_idx < self.list_len):\n",
    "            batch = self.batch()\n",
    "            success = False\n",
    "            while success == False:\n",
    "                try:\n",
    "                    self.queue.put(batch, True)\n",
    "                    success = True\n",
    "                except:\n",
    "                    print(\"Batching Failed in Thread ID: {}\".format(self.id))\n",
    "                    sleep(1)\n",
    "\n",
    "        self.thread_flags[self.id] = True\n",
    "        \n",
    "#         print(\"Thread {} finished\".foramt(self.id))\n",
    "\n",
    "        return \n",
    "\n",
    "\n",
    "    def batch(self):\n",
    "\n",
    "        tensor_list = list()\n",
    "        ground_truth_list = list()\n",
    "        tensor_size_list = list()\n",
    "        ground_truth_size_list = list()\n",
    "        lev_truth_list = list()\n",
    "        lev_truth_length_list = list()\n",
    "        \n",
    "        count = 0\n",
    "        max_seq_len = 0\n",
    "        max_sen_len = 0\n",
    "        max_lev_truth_len = 0\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            # If there is no more file, break and set end_flag true\n",
    "            if self.cur_idx >= self.list_len:\n",
    "                self.end_flag = True\n",
    "                break\n",
    "                \n",
    "            script_path = self.script_path_list[self.load_idxs_list[self.cur_idx]]\n",
    "            \n",
    "#             print(script_path)\n",
    "            \n",
    "            with open(script_path) as f:\n",
    "                line = f.read()\n",
    "                line = line.strip()\n",
    "                lev_truth = list(map(int, line.split(' ')))\n",
    "                \n",
    "            lev_truth_list.append(lev_truth)\n",
    "            lev_truth_length_list.append(len(lev_truth))\n",
    "            \n",
    "            wav_path = self.wav_path_list[self.load_idxs_list[self.cur_idx]]\n",
    "\n",
    "            tensor = self.create_mel(wav_path)\n",
    "            tensor_list.append(tensor)\n",
    "            tensor_size_list.append(tensor.shape[1])\n",
    "            \n",
    "            ground_truth = self.ground_truth_list[self.load_idxs_list[self.cur_idx]]\n",
    "            ground_truth_list.append(ground_truth)\n",
    "            ground_truth_size_list.append(len(ground_truth))\n",
    "            \n",
    "            if (tensor.shape[1] > max_seq_len):\n",
    "                max_seq_len = tensor.shape[1]\n",
    "            if (len(ground_truth) > max_sen_len):\n",
    "                max_sen_len = len(ground_truth)  \n",
    "            if (len(lev_truth) > max_lev_truth_len):\n",
    "                max_lev_truth_len = len(lev_truth)\n",
    "            \n",
    "            self.cur_idx += 1\n",
    "            count += 1\n",
    "            \n",
    "        batched_tensor = torch.zeros(count, max_seq_len + 5, n_mels)\n",
    "        batched_ground_truth = torch.zeros(count, max_sen_len)\n",
    "        batched_loss_mask = torch.zeros(count, max_sen_len)\n",
    "        ground_truth_size_list = torch.tensor(np.asarray(ground_truth_size_list), dtype=torch.long)\n",
    "        \n",
    "        batched_lev_truth = torch.zeros(count, max_lev_truth_len)\n",
    "        batched_lev_truth_loss_mask = torch.zeros(count, max_lev_truth_len)\n",
    "        \n",
    "        for order in range(count):\n",
    "            \n",
    "            target = tensor_list[order]\n",
    "            \n",
    "            if self.is_train:\n",
    "                pad_random = np.random.randint(0, 5)\n",
    "                # Time shift, add zeros in front of an image\n",
    "                if pad_random > 0:\n",
    "                    offset = torch.zeros(target.shape[0], pad_random, target.shape[2])\n",
    "                    target = torch.cat((offset, target), 1)\n",
    "                # Add random noise\n",
    "                target = target + (torch.rand(target.shape) - 0.5) / 20\n",
    "                # Value less than 0 or more than 1 is clamped to 0 and 1\n",
    "                target = torch.clamp(target, min=0.0, max=1.0)\n",
    "                batched_tensor[order, :tensor_size_list[order] + pad_random, :] = target\n",
    "            else:\n",
    "                batched_tensor[order, :tensor_size_list[order], :] = target\n",
    "\n",
    "#           batched_tensor[order, :tensor_size_list[order], :] = target\n",
    "            batched_ground_truth[order, :ground_truth_size_list[order]] = torch.tensor(ground_truth_list[order])\n",
    "            \n",
    "            # You do not need to know what loss mask is \n",
    "            batched_loss_mask[order, :ground_truth_size_list[order]] = torch.ones(ground_truth_size_list[order])\n",
    "        \n",
    "            batched_lev_truth[order, :lev_truth_length_list[order]] = torch.tensor(lev_truth_list[order])\n",
    "            batched_lev_truth_loss_mask[order, :lev_truth_length_list[order]] = torch.ones(lev_truth_length_list[order])\n",
    "        \n",
    "        return [batched_tensor, batched_ground_truth, batched_loss_mask, ground_truth_size_list, batched_lev_truth, batched_lev_truth_loss_mask]\n",
    "    \n",
    "    def create_mel(self, wav_path):  \n",
    "#         y, sr = librosa.core.load(wav_path, sr=fs) \n",
    "#         f, t, Zxx = sp.signal.stft(y, fs=sr, nperseg=nsc, noverlap=nov)\n",
    "#         Sxx = np.abs(Zxx)\n",
    "        \n",
    "#         # Cut-off paddings\n",
    "#         coef = np.sum(Sxx, 0)\n",
    "#         Sxx = Sxx[:, find_starting_point(coef):find_ending_point(coef)]\n",
    "\n",
    "#         # mel_filters: (n_fft, n_mels)\n",
    "#         mel_filters = librosa.filters.mel(sr=fs, n_fft=nsc, n_mels=n_mels)\n",
    "#         mel_specgram = np.matmul(mel_filters, Sxx)\n",
    "\n",
    "#         # log10(0) is minus infinite, so replace mel_specgram values smaller than 'eps' as 'eps' (1e-8)\n",
    "#         log_mel_specgram = 20 * np.log10(np.maximum(mel_specgram, eps))\n",
    "        \n",
    "#         # 20 * log10(eps) = 20 * -8 = -160\n",
    "#         # -160 is the smallest value\n",
    "#         # Add 160 and divide by 160 => Normalize value between 0 and 1\n",
    "#         norm_log_mel_specgram = (log_mel_specgram + db_ref) / db_ref        \n",
    "        \n",
    "#         # (F, T) -> (T, F)\n",
    "#         input_spectrogram = norm_log_mel_specgram.T\n",
    "#         # (T, F) -> (1, T, F)\n",
    "#         # Inserted the first axis to make stacking easier\n",
    "#         tensor_input = torch.tensor(input_spectrogram).view(1, input_spectrogram.shape[0], input_spectrogram.shape[1])\n",
    "#         return tensor_input\n",
    "        return torch.zeros(1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer maps numbers to characters, 8 -> 'ㄱ', 10 -> 'ㄴ'\n",
    "class Tokenizer():\n",
    "    def __init__(self, vocabs):\n",
    "        self.vocabs = vocabs\n",
    "        \n",
    "    def word2num(self, sentence):\n",
    "        tokens = list()\n",
    "        for char in sentence:\n",
    "            tokens.append(self.vocabs.index(char))    \n",
    "        return tokens\n",
    "        \n",
    "    def word2vec(self, sentence):\n",
    "        vectors = np.zeros((len(sentence), len(self.vocabs)))\n",
    "        for i, char in enumerate(sentence):\n",
    "            vectors[i, self.vocabs.index(char)] = 1   \n",
    "        return vectors\n",
    "    \n",
    "    def num2word(self, num):\n",
    "        output = list()\n",
    "        for i in num:\n",
    "            output.append(self.vocabs[i])\n",
    "        return output\n",
    "    \n",
    "    def num2vec(self, numbers):\n",
    "        vectors = np.zeros((len(numbers), len(self.vocabs)))\n",
    "        for i, num in enumerate(numbers):\n",
    "            vectors[i, num] = 1   \n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "9gbjg0_LPKA8",
    "outputId": "b2b29359-e867-4df6-e305-83de989ceef3"
   },
   "outputs": [],
   "source": [
    "# unicode_jamo_list = list()\n",
    "\n",
    "# # 초성\n",
    "# for unicode in range(0x1100, 0x1113):\n",
    "#     unicode_jamo_list.append(chr(unicode))  # chr: Change hexadecimal to unicode\n",
    "# # 중성\n",
    "# for unicode in range(0x1161, 0x1176):\n",
    "#     unicode_jamo_list.append(chr(unicode))\n",
    "# # 종성\n",
    "# for unicode in range(0x11A8, 0x11C3):\n",
    "#     unicode_jamo_list.append(chr(unicode))\n",
    "# for unicode in range(ord('A'), ord('Z') + 1):\n",
    "#     unicode_jamo_list.append(chr(unicode))\n",
    "# for unicode in range(ord('a'), ord('z') + 1):\n",
    "#     unicode_jamo_list.append(chr(unicode))\n",
    "# for unicode in range(ord('0'), ord('9') + 1):\n",
    "#     unicode_jamo_list.append(chr(unicode))\n",
    "\n",
    "# unicode_jamo_list += [' ', '\\\\', '!', '~', '^', '<', '>', ',', '.', \"'\", '?', '？', '/', '%', '(', ')', ':', ';', '+',\n",
    "#                       '-', '<s>', '</s>']\n",
    "# unicode_jamo_list.sort()\n",
    "# # '_' symbol represents \"blank\" in CTC loss system, \"blank\" has to be the index 0\n",
    "# unicode_jamo_list = ['_'] + unicode_jamo_list\n",
    "\n",
    "# tokenizer = Tokenizer(unicode_jamo_list)\n",
    "# jamo_tokens = tokenizer.word2num(unicode_jamo_list)\n",
    "\n",
    "unicode_jamo_list = list()\n",
    "\n",
    "# 초성\n",
    "for unicode in range(0x1100, 0x1113):\n",
    "    unicode_jamo_list.append(chr(unicode))  # chr: Change hexadecimal to unicode\n",
    "# 중성\n",
    "for unicode in range(0x1161, 0x1176):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "# 종성\n",
    "for unicode in range(0x11A8, 0x11C3):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "\n",
    "unicode_jamo_list += [' ', '!', ',', '.', '?', '<s>', '</s>', '*']\n",
    "\n",
    "unicode_jamo_list.sort()\n",
    "# '_' symbol represents \"blank\" in CTC loss system, \"blank\" has to be the index 0\n",
    "unicode_jamo_list = ['_'] + unicode_jamo_list\n",
    "\n",
    "tokenizer = Tokenizer(unicode_jamo_list)\n",
    "jamo_tokens = tokenizer.word2num(unicode_jamo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', ' ', '!', '*', ',', '.', '</s>', '<s>', '?', 'ᄀ', 'ᄁ', 'ᄂ', 'ᄃ', 'ᄄ', 'ᄅ', 'ᄆ', 'ᄇ', 'ᄈ', 'ᄉ', 'ᄊ', 'ᄋ', 'ᄌ', 'ᄍ', 'ᄎ', 'ᄏ', 'ᄐ', 'ᄑ', 'ᄒ', 'ᅡ', 'ᅢ', 'ᅣ', 'ᅤ', 'ᅥ', 'ᅦ', 'ᅧ', 'ᅨ', 'ᅩ', 'ᅪ', 'ᅫ', 'ᅬ', 'ᅭ', 'ᅮ', 'ᅯ', 'ᅰ', 'ᅱ', 'ᅲ', 'ᅳ', 'ᅴ', 'ᅵ', 'ᆨ', 'ᆩ', 'ᆪ', 'ᆫ', 'ᆬ', 'ᆭ', 'ᆮ', 'ᆯ', 'ᆰ', 'ᆱ', 'ᆲ', 'ᆳ', 'ᆴ', 'ᆵ', 'ᆶ', 'ᆷ', 'ᆸ', 'ᆹ', 'ᆺ', 'ᆻ', 'ᆼ', 'ᆽ', 'ᆾ', 'ᆿ', 'ᇀ', 'ᇁ', 'ᇂ']\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75]\n"
     ]
    }
   ],
   "source": [
    "print(unicode_jamo_list)\n",
    "print(jamo_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_list = [(tokenizer.word2num(['<s>'] + list(jamo_script_list[i]) + ['</s>'])) for i in range(len(jamo_script_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90% of the data will be used as train\n",
    "split_index = int(0.9 * len(wav_paths))\n",
    "\n",
    "wav_path_list_train = wav_paths[:split_index]\n",
    "ground_truth_list_train = ground_truth_list[:split_index]\n",
    "script_path_list_train = script_paths[:split_index]\n",
    "\n",
    "wav_path_list_eval = wav_paths[split_index:]\n",
    "ground_truth_list_eval = ground_truth_list[split_index:]\n",
    "script_path_list_eval = script_paths[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_thread = 3\n",
    "\n",
    "preloader_eval = Threading_Batched_Preloader(wav_path_list_eval, ground_truth_list_eval, script_path_list_eval, batch_size, is_train=False)\n",
    "preloader_train = Threading_Batched_Preloader(wav_path_list_train, ground_truth_list_train, script_path_list_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, optimizer, ctc_loss, input_tensor, ground_truth, loss_mask, target_lengths):\n",
    "\n",
    "    # Shape of the input tensor (B, T, F)\n",
    "    # B: Number of a batch (8, 16, or 64 ...)\n",
    "    # T: Temporal length of an input\n",
    "    # F: Number of frequency band, 80\n",
    "\n",
    "    net.train()\n",
    "    \n",
    "    batch_size = input_tensor.shape[0]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pred_tensor = net(input_tensor)\n",
    "    \n",
    "    # Cast true sentence as Long data type, since CTC loss takes long tensor only\n",
    "    # Shape (B, S)\n",
    "    # S: Max length among true sentences \n",
    "    truth = ground_truth\n",
    "    truth = truth.type(torch.cuda.LongTensor)\n",
    "\n",
    "    input_lengths = torch.full(size=(batch_size,), fill_value=pred_tensor.shape[0], dtype=torch.long)\n",
    "\n",
    "    loss = ctc_loss(pred_tensor, truth, input_lengths, target_lengths)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return loss divided by true length because loss is sum of the character losses\n",
    "\n",
    "    return pred_tensor, loss.item() / ground_truth.shape[1]\n",
    "\n",
    "\n",
    "def evaluate(net, ctc_loss, input_tensor, ground_truth, loss_mask, target_lengths):\n",
    "\n",
    "    # Shape of the input tensor (B, T, F)\n",
    "    # B: Number of a batch (8, 16, or 64 ...)\n",
    "    # T: Temporal length of an input\n",
    "    # F: Number of frequency band, 80\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    batch_size = input_tensor.shape[0]\n",
    "    \n",
    "    pred_tensor = net(input_tensor)\n",
    "    \n",
    "    # Cast true sentence as Long data type, since CTC loss takes long tensor only\n",
    "    # Shape (B, S)\n",
    "    # S: Max length among true sentences \n",
    "    truth = ground_truth\n",
    "    truth = truth.type(torch.cuda.LongTensor)\n",
    "\n",
    "    input_lengths = torch.full(size=(batch_size,), fill_value=pred_tensor.shape[0], dtype=torch.long)\n",
    "\n",
    "    loss = ctc_loss(pred_tensor, truth, input_lengths, target_lengths)\n",
    "\n",
    "    # Return loss divided by true length because loss is sum of the character losses\n",
    "\n",
    "    return pred_tensor, loss.item() / ground_truth.shape[1]\n",
    "\n",
    "def save(model, optimizer, check_point_name):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, check_point_name)\n",
    "\n",
    "def load(model, optimizer, check_point_name):\n",
    "    checkpoint = torch.load(check_point_name)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OG2yNubVPKBM"
   },
   "outputs": [],
   "source": [
    "# Use GPU if GPU is available \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, D_in, H):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = torch.nn.Linear(D_in, H)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.gru = nn.GRU(H, int(H/2), bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # (B, T, F)\n",
    "        output_tensor = self.fc(input_tensor)\n",
    "        output_tensor = self.relu(output_tensor)\n",
    "        output_tensor = self.dropout(output_tensor)\n",
    "        # (B, T, H)\n",
    "        output_tensor, _ = self.gru(output_tensor)\n",
    "        return output_tensor\n",
    "    \n",
    "class CTC_Decoder(nn.Module):\n",
    "    def __init__(self, H, D_out, num_chars):\n",
    "        super(CTC_Decoder, self).__init__()\n",
    "        self.fc_embed = nn.Linear(H, H)\n",
    "        self.relu_embed = torch.nn.ReLU()\n",
    "        self.dropout_embed = nn.Dropout(p=0.2) \n",
    "        self.gru = nn.GRU(H, D_out, batch_first=True)\n",
    "        self.fc = nn.Linear(D_out, num_chars)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # (B, T, 2 * H/2)\n",
    "        output_tensor = self.fc_embed(input_tensor)\n",
    "        output_tensor = self.relu_embed(output_tensor)\n",
    "        output_tensor = self.dropout_embed(output_tensor) \n",
    "        # (B, T, H)\n",
    "        output_tensor,_ = self.gru(input_tensor)\n",
    "        # (B, T, H)\n",
    "        output_tensor = self.fc(output_tensor)\n",
    "        # (B, T, 75)\n",
    "        prediction_tensor = self.log_softmax(output_tensor)\n",
    "\n",
    "        return prediction_tensor\n",
    "\n",
    "class Mel2SeqNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, num_chars, device):\n",
    "        super(Mel2SeqNet, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(D_in, H).to(device)\n",
    "        self.decoder = CTC_Decoder(H, D_out, num_chars).to(device)\n",
    "        \n",
    "        # Initialize weights with random uniform numbers with range\n",
    "        for param in self.encoder.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "        for param in self.decoder.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "            \n",
    "    def forward(self, input_tensor):\n",
    "        batch_size = input_tensor.shape[0]\n",
    "        # (B, T, F) -> (B, T, H)\n",
    "        encoded_tensor = self.encoder(input_tensor)\n",
    "        # (B, T, H) -> (B, T, 75)\n",
    "        pred_tensor = self.decoder(encoded_tensor)\n",
    "        pred_tensor = pred_tensor.permute(1, 0, 2)\n",
    "        \n",
    "        return pred_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "############################################################################\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, int(hidden_size/2), bidirectional=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "          return torch.zeros(2, batch_size, int(self.hidden_size/2), device=device)\n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, embedded, hidden, encoder_outputs):\n",
    "\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # (1, B, H) + (1, B, H) = (1, B, 2H)        \n",
    "        concated_tensor = torch.cat((embedded, hidden), 2)\n",
    "        \n",
    "        key = self.attn(concated_tensor) # (1, B, H)\n",
    "        key = key.permute(1, 2, 0) # (B, H, 1)\n",
    "\n",
    "        attention_value = torch.bmm(encoder_outputs, key) # (B, L, 1)\n",
    "        attn_weights = F.softmax(attention_value, dim=1)\n",
    "        \n",
    "        attn_weights = attn_weights.permute(0, 2, 1) # (B, 1, L)\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_outputs) # (B, 1, H)\n",
    "        attn_applied = attn_applied.permute(1, 0, 2) # (1, B, H)\n",
    "        \n",
    "        output = torch.cat((embedded, attn_applied), 2) # (1, B, 2H)\n",
    "        output = self.attn_combine(output) # (1, B, H)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden) # (1, B, H)\n",
    "        output = F.log_softmax(self.out(output), dim=2) # (1, B, 74)\n",
    "        \n",
    "        return output.squeeze(0), hidden, attn_weights.squeeze(1)\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "    \n",
    "class Seq2SeqNet(nn.Module):\n",
    "    def __init__(self, hidden_size, jamo_tokens, char2index, device):\n",
    "        super(Seq2SeqNet, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.jamo_tokens = jamo_tokens\n",
    "        self.char2index = char2index\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(len(jamo_tokens), hidden_size).to(device) \n",
    "        self.embedding_layer_2 = nn.Embedding(len(char2index), hidden_size).to(device)\n",
    "        self.encoder = EncoderRNN(hidden_size).to(device)\n",
    "        self.decoder = AttnDecoderRNN(hidden_size,len(char2index), dropout_p=0.1).to(device)\n",
    "        \n",
    "        for param in self.encoder.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "        for param in self.embedding_layer.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "        for param in self.embedding_layer_2.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "        for param in self.decoder.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "            \n",
    "    def net_train(self, input_tensor, target_tensor, loss_mask, optimizer, criterion):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_size = input_tensor.shape[0]\n",
    "        input_length = input_tensor.shape[1]\n",
    "        target_length = target_tensor.shape[1]\n",
    "\n",
    "        input_tensor = input_tensor.long()\n",
    "        target_tensor = target_tensor.long()\n",
    "        \n",
    "        embedded_tensor = self.embedding_layer(input_tensor)\n",
    "        embedded_tensor = embedded_tensor.permute(1, 0, 2)\n",
    "\n",
    "        # (L, B)\n",
    "        target_tensor = target_tensor.permute(1, 0)\n",
    "        encoder_outputs = torch.zeros(input_length, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        encoder_outputs = torch.zeros(input_length, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        encoder_hidden = self.encoder.initHidden(batch_size)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            embedded_slice = embedded_tensor[ei].unsqueeze(0)\n",
    "            encoder_output, encoder_hidden = self.encoder(\n",
    "                embedded_slice, encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output\n",
    "\n",
    "        decoder_input_token = torch.tensor(([self.char2index['<s>']] * batch_size)).long().unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Override encoder hidden state\n",
    "        encoder_hidden = encoder_outputs[-1, :, :].unsqueeze(0)\n",
    "\n",
    "        # (L, B, H) -> (B, L, H)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        # Override encoder_hidden\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoder_attentions = torch.zeros([batch_size, input_length, target_length])\n",
    "        decoder_outputs = torch.zeros([batch_size, target_length, len(self.char2index)])\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_input = self.embedding_layer_2(decoder_input_token)\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            loss += torch.mean(criterion(decoder_output, target_tensor[di]) * loss_mask[:, di])\n",
    "\n",
    "#             if np.random.rand() < 0.8:        \n",
    "#                 decoder_input_token = target_tensor[di].unsqueeze(0)\n",
    "#             else:\n",
    "            decoder_input_token = torch.argmax(decoder_output, dim=1).unsqueeze(0)\n",
    "\n",
    "            decoder_attentions[:, :, di] = decoder_attention\n",
    "            decoder_outputs[:, di, :] = decoder_output\n",
    "          \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        return decoder_outputs, decoder_attentions, loss.item() / target_length\n",
    "    \n",
    "    \n",
    "    def net_eval(self, input_tensor, target_tensor, loss_mask, criterion):\n",
    "        \n",
    "        batch_size = input_tensor.shape[0]\n",
    "        input_length = input_tensor.shape[1]\n",
    "        target_length = target_tensor.shape[1]\n",
    "\n",
    "        input_tensor = input_tensor.long()\n",
    "        target_tensor = target_tensor.long()\n",
    "        \n",
    "        embedded_tensor = self.embedding_layer(input_tensor)\n",
    "        embedded_tensor = embedded_tensor.permute(1, 0, 2)\n",
    "\n",
    "        # (L, B)\n",
    "        target_tensor = target_tensor.permute(1, 0)\n",
    "        encoder_outputs = torch.zeros(input_length, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        encoder_outputs = torch.zeros(input_length, batch_size, self.hidden_size, device=self.device)\n",
    "\n",
    "        encoder_hidden = self.encoder.initHidden(batch_size)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            embedded_slice = embedded_tensor[ei].unsqueeze(0)\n",
    "            encoder_output, encoder_hidden = self.encoder(\n",
    "                embedded_slice, encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output\n",
    "\n",
    "        decoder_input_token = torch.tensor(([self.char2index['<s>']] * batch_size)).long().unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Override encoder hidden state\n",
    "        encoder_hidden = encoder_outputs[-1, :, :].unsqueeze(0)\n",
    "\n",
    "        # (L, B, H) -> (B, L, H)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "        # Override encoder_hidden\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoder_attentions = torch.zeros([batch_size, input_length, target_length])\n",
    "        decoder_outputs = torch.zeros([batch_size, target_length, len(self.char2index)])\n",
    "        \n",
    "        loss = 0\n",
    "\n",
    "        for di in range(target_length):\n",
    "            decoder_input = self.embedding_layer_2(decoder_input_token)\n",
    "\n",
    "            decoder_output, decoder_hidden, decoder_attention = self.decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "            loss += torch.mean(criterion(decoder_output, target_tensor[di]) * loss_mask[:, di])\n",
    "\n",
    "#             if np.random.rand() < 0.8:        \n",
    "#                 decoder_input_token = target_tensor[di].unsqueeze(0)\n",
    "#             else:\n",
    "            decoder_input_token = torch.argmax(decoder_output, dim=1).unsqueeze(0)\n",
    "\n",
    "            decoder_attentions[:, :, di] = decoder_attention\n",
    "            decoder_outputs[:, di, :] = decoder_output\n",
    "        \n",
    "        return decoder_outputs, decoder_attentions, loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that interprets the CTC prediction result\n",
    "\n",
    "def Decode_CTC_Prediction(prediction):\n",
    "    CTC_pred = prediction.detach().cpu().numpy()\n",
    "    result = list()\n",
    "    last_elem = 0\n",
    "    for i, elem in enumerate(CTC_pred):\n",
    "        if elem != last_elem and elem != 0:\n",
    "            result.append(elem)\n",
    "        \n",
    "        last_elem = elem\n",
    "\n",
    "    result = np.asarray(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decode_Lev(tensor, index2char):\n",
    "    \n",
    "    str_output = list()\n",
    "    \n",
    "    for i in range(tensor.shape[0]):\n",
    "        _, lev_idxs = tensor[i, :, :].max(-1)\n",
    "        lev_idxs = lev_idxs.detach().cpu().numpy()\n",
    "        decoded_sentence = list()\n",
    "        for idx in lev_idxs:\n",
    "            decoded_sentence.append(index2char[idx])\n",
    "        str_output.append(''.join(decoded_sentence))\n",
    "        \n",
    "    return str_output\n",
    "\n",
    "def Decode_Lev_Truth(tensor, index2char):\n",
    "    \n",
    "    str_output = list()\n",
    "    \n",
    "    for i in range(tensor.shape[0]):\n",
    "        lev_idxs = tensor[i, :]\n",
    "        decoded_sentence = list()\n",
    "        for idx in lev_idxs:\n",
    "            decoded_sentence.append(index2char[idx])\n",
    "        str_output.append(''.join(decoded_sentence))\n",
    "        \n",
    "    return str_output\n",
    "        \n",
    "\n",
    "def Decode_Prediction_No_Filtering(pred_tensor, tokenizer):\n",
    "    decoded_list = list()\n",
    "    for i in range(pred_tensor.shape[1]):\n",
    "        _, CTC_index = pred_tensor[:, i, :].max(-1)\n",
    "        index = Decode_CTC_Prediction(CTC_index)\n",
    "        jamos = tokenizer.num2word(index)\n",
    "        sentence = jamotools.join_jamos(''.join(jamos))\n",
    "        decoded_list.append(sentence)\n",
    "    return decoded_list\n",
    "\n",
    "def Decode_Prediction(pred_tensor, tokenizer, char2index):\n",
    "    decoded_list = list()\n",
    "    for i in range(pred_tensor.shape[1]):\n",
    "        _, CTC_index = pred_tensor[:, i, :].max(-1)\n",
    "        index = Decode_CTC_Prediction(CTC_index)\n",
    "        jamos = tokenizer.num2word(index)\n",
    "        sentence = jamotools.join_jamos(''.join(jamos))\n",
    "        \n",
    "        not_com_jamo = re.compile(u'[^\\u3130-\\u3190]')\n",
    "        filtered_sentence = ''.join(not_com_jamo.findall(sentence))\n",
    "        filtered_sentence = filtered_sentence.replace('<s>', '')\n",
    "        filtered_sentence = filtered_sentence.replace('</s>', '')\n",
    "#         filtered_sentence = filtered_sentence.replace('<eos>', '')\n",
    "#         final_prediction = c2i_decoding(char2index, filtered_sentence)\n",
    "        \n",
    "        decoded_list.append(filtered_sentence)\n",
    "    return decoded_list\n",
    "    \n",
    "\n",
    "def lev_num_to_lev_string(lev_num_list, index2char):\n",
    "    lev_str_list = list()\n",
    "    for num_list in lev_num_list:\n",
    "        \n",
    "        temp = list()\n",
    "        for num in num_list:\n",
    "            temp.append(index2char[num])\n",
    "        \n",
    "        lev_str_list.append(''.join(temp))\n",
    "\n",
    "    return lev_str_list\n",
    "\n",
    "def char_distance(ref, hyp):\n",
    "    \n",
    "    ####################\n",
    "    hyp = hyp[:len(ref)]\n",
    "    \n",
    "    ref = ref.replace(' ', '') \n",
    "    hyp = hyp.replace(' ', '') \n",
    "\n",
    "    dist = Lev.distance(hyp, ref)\n",
    "    length = len(ref.replace(' ', ''))\n",
    "\n",
    "    return dist, length \n",
    "\n",
    "def char_distance_list(ref_list, hyp_list):\n",
    "\n",
    "    sum_dist = 0\n",
    "    sum_length = 0\n",
    "    \n",
    "    for ref, hyp in zip(ref_list, hyp_list):\n",
    "        dist, length = char_distance(ref, hyp)\n",
    "        sum_dist += dist\n",
    "        sum_length += length\n",
    "\n",
    "    return sum_dist, sum_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decode_Jamo_Prediction_And_Batch(pred_tensor):\n",
    "    decoded_idx_list = list()\n",
    "    \n",
    "    max_len = 0\n",
    "    \n",
    "    for i in range(pred_tensor.shape[1]):\n",
    "        _, CTC_index = pred_tensor[:, i, :].max(-1)\n",
    "        index = Decode_CTC_Prediction(CTC_index)\n",
    "        if len(index) > max_len:\n",
    "            max_len = len(index)\n",
    "        decoded_idx_list.append(index)\n",
    "        \n",
    "    batched_lev_input = torch.zeros(len(decoded_idx_list), max_len)\n",
    "    \n",
    "#     print('batched_lev_input shape: {}'.format(batched_lev_input.shape))\n",
    "        \n",
    "    for i, index in enumerate(decoded_idx_list):\n",
    "        batched_lev_input[i, :len(index)] = torch.tensor(index)\n",
    "        \n",
    "    return batched_lev_input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CER Record\n",
      "No Seq2Seq Loss Record\n",
      "09-24 02:46:57\n",
      "Count 15 | 네 없습니다 __ =>          \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 30 | 감사합니다 ___ =>          \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 45 | 배달 되나요? ___ => 네          \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 60 | 메뉴 포장 되나요?  => 아          \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 75 | 언제까지 하죠? ___ => 아           \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 90 | 소주도 파시나요? __ => 네           \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 105 | 네 감사합니다 ___ => 아          \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 120 | 배달료가 얼마죠? __ => 주           \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 135 | 영업하고있나요? ____ => 주            \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 150 | 몇 시에 문 닫나요? _ => 아            \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 165 | 학생 할인 되나요? __ => 오약           \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 180 | 몇 시에 문 닫나요? _ => 주약           \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 195 | 예약 하려고 하는데요 __ => 예             \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 210 | 지하철 근처인가요? ___ => 지             \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 225 | 주차요금은 무료에요? _ => 아일           \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADsAAADCCAYAAAD3uLpiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAGOklEQVR4nO2dW4hVVRjHf3/HpiY1KrpQjqSBhBJBIdEFCjLBSqqHCoXCLuBTV4Iyeug16EJBUYhaQqIPZheii9KFCCoyu2l2Mbs4ZWl0paCc+vewdzWM5zhzzlprzmHN+sEwZ++99t7fj7XPPvt8e+3vyDbjhQmdDmAsKbK5UmRzZVzJThzLnfVO6HNfz5SW1umf9VNL7b8ZGOTHH/5Wo2VjKtvXM4XTj7i0pXXueebJltpftmBP02VBh7Gk+ZI+lrRd0tKQbY0FbctK6gEeBM4DZgOLJM2OFVgKQnr2VGC77R22/wTWAhfFCSsNIbJTgZ1DpgfqeV1LyAmq0Rlvn28VkpYASwAOmjA5YHfhhPTsADBtyHQ/8M3wRraX2Z5je07vhL6A3YUTIvsWMFPSDEm9wELg6ThhpaHtw9j2oKRrgReAHmCl7a3RIktA0EWF7WeBZyPFkpxybZwKDw7y13e7W1pnVu/BLbXvU/P+G1c9W2RzpcjmSpHNlSKbK0U2V4psrhTZXAlJkk+T9LKkbZK2SrohZmApCPnyPgjcbHuzpCnA25I22v4wUmzRabtnbe+yvbl+/SuwjYyT5P8haTpwMvBmg2X/J8lpLcUSm+ATlKTJwOPAjbZ/Gb58aJL8AA4M3V0QobcsD6ASXW17fZyQ0hFyNhawAthm+954IaUjpGfPBK4AzpH0bv13fqS4khBy++M1Gt/J61rKFVSuFNlcKbK5UmRzpcjmSpHNlSKbK0U2V2Ik3HokvSPpmRgBpSRGz95AlTPuekKzi/3ABcDyOOGkJbRn7wNuAf5u1kDSEkmbJG3ayx+BuwsjJJW6ANht++39tcslSX4mcKGkL6ie/DhH0mNRokpEyI2t22z3255ONWT+JduXR4ssAePqczbKXTzbrwCvxNhWSsZVzxbZXCmyuVJkc6XI5kqRzZUimytFNleK7GiRdKikdZI+qkeUnx4rsBSEZiruB563fUldvqGzA4pHoG1ZSYcAZwFXAtT1Zf6ME1YaQg7j44E9wCP1vZ7lkiYNb5RFkpzqqDgFeMj2ycBvwD41oXJJkg8AA7b/fS5gHZV81xKSJP8W2CnphHrWXKBrH3OB8LPxdcDq+ky8A7gqPKR0hNaWeReYEymW5JQrqFwpsrlSZHOlyOZKkc2VIpsrRTZXimyuhCbJb6pLrWyRtEbSQbECS0HIENypwPXAHNsnUpX9XRgrsBSEHsYTgT5JE6nuBuxTubqbCMkufg3cDXwF7AJ+tr1heLsskuSSDqMqpT8DOBaYJGmf8ca5JMnPBT63vcf2XmA9cEacsNIQIvsVcJqkg+vSK3Pp8kdeQt6zb1Ld8tgMfFBva1mkuJIQmiS/A7gjUizJKVdQuVJkc6XI5kqRzZUimytFNleKbK4U2VwZUVbSSkm7JW0ZMu9wSRslfVr/PyxtmHEYTc8+CswfNm8p8KLtmcCLNBh6242MKGv7VeCHYbMvAlbVr1cBF0eOKwntvmePtr0LqkLswFHNGmaRJB8tOSTJv5N0DED9v7Vf9usQ7co+DSyuXy8GnooTTlpG89GzBngdOEHSgKRrgDuBeZI+BebV013PiEly24uaLJobOZbklCuoXCmyuVJkc6XI5kqRzZUimytFNleKbK60myS/qy6x8r6kJyQdmjbMOLSbJN8InGj7JOAT4LbIcSWhrSS57Q22B+vJN4D+BLFFJ8Z79mrguQjbSU7QqFRJt1P9Du3q/bTpml8pDakatBhYAMy17WbtbC+jHod8iA5v2m4saEtW0nzgVuBs27/HDSkd7SbJHwCmABvrH+x8OHGcUWg3Sb4iQSzJGVdXUNrPuSX+zqQ9wJcNFh0BfN/i5pqtc5ztIxvufyxlmyFpk+2Wqg+1s864OoyLbAdo5+GnltfpivfsWNEtPTsmdFxW0nxJH0vaLmnEwWONkgmjxnbH/qgeHf+MqjpnL/AeMHuEdc6iqu+4pdX9dbpnTwW2295R11pdSzV6rilNRtyNik7LTgV2DpkeqOclodOyajAv2cdDp2UHgGlDpvtJWBGh07JvATMlzahLkC6kGj2Xhk6ejeuz6/lU6djPgNtH0X4NVamIvVRHxjWj3Ve5gsqVIpsrRTZXimyujCvZfwAsRfTAnoJ2pAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count 240 | 몇번버스 타면되요? __ => 아            \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-7ff31e831ea7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mlev_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mlev_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattentions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq2seq_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjamo2char_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlev_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlev_truth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlev_truth_loss_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjamo2char_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq2seq_criterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;31m#             print('lev_truth shape : {}'.format(lev_truth.shape))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-574d966419e7>\u001b[0m in \u001b[0;36mnet_train\u001b[1;34m(self, input_tensor, target_tensor, loss_mask, optimizer, criterion)\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0membedded_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedded_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mei\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             encoder_output, encoder_hidden = self.encoder(\n\u001b[1;32m--> 109\u001b[1;33m                 embedded_slice, encoder_hidden)\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[0mencoder_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mei\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-574d966419e7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[0msorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mrun_impl\u001b[1;34m(self, input, hx, batch_sizes)\u001b[0m\n\u001b[0;32m    677\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m             result = _VF.gru(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[1;32m--> 679\u001b[1;33m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[0;32m    680\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCH = 20 * 10\n",
    "           \n",
    "# net = Mel2SeqNet(80, 512, 256)\n",
    "\n",
    "\n",
    "# net = Mel2SeqNet(80, 1024, 512, len(unicode_jamo_list), device)\n",
    "\n",
    "# keyword = 'NSML_pure_jamo_50ms_pad_cut'\n",
    "# net = Mel2SeqNet(80, 512, 256, len(unicode_jamo_list), device)\n",
    "\n",
    "# keyword = 'NSML_pure_jamo_50ms_pad_cut_1024'\n",
    "\n",
    "keyword = 'NSML_pure_jamo_50ms_pad_cut_1024_copy'\n",
    "\n",
    "net = Mel2SeqNet(80, 1024, 512, len(unicode_jamo_list), device)\n",
    "net_optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "ctc_loss = nn.CTCLoss().to(device)\n",
    "\n",
    "keyword = 'NSML_seq2seq_both_model_256_0'\n",
    "\n",
    "jamo2char_net = Seq2SeqNet(256, jamo_tokens, char2index, device)\n",
    "jamo2char_optimizer = optim.Adam(jamo2char_net.parameters(), lr=0.001)\n",
    "seq2seq_criterion = nn.NLLLoss(reduction='none').to(device)\n",
    "\n",
    "# train_loss_history = list()\n",
    "# eval_loss_history = list()\n",
    "\n",
    "train_seq2seq_loss_history = list()\n",
    "eval_seq2seq_loss_history = list()\n",
    "\n",
    "train_cer_history = list()\n",
    "eval_cer_history = list()\n",
    "\n",
    "try:\n",
    "    train_cer_history = list(np.load('model_saved/train_cer_history_{}.npy'.format(keyword)))\n",
    "    eval_cer_history = list(np.load('model_saved/eval_cer_history_{}.npy'.format(keyword)))\n",
    "except:\n",
    "    print(\"No CER Record\")\n",
    "    \n",
    "try:\n",
    "    load(jamo2char_net, jamo2char_optimizer, 'model_saved/seq_{}'.format(keyword))\n",
    "    train_seq2seq_loss_history = list(np.load('model_saved/train_seq_loss_history_{}.npy'.format(keyword)))\n",
    "    eval_seq2seq_loss_history = list(np.load('model_saved/eval_seq_loss_history_{}.npy'.format(keyword)))\n",
    "except:\n",
    "    print('No Seq2Seq Loss Record')\n",
    "\n",
    "# try:\n",
    "#     load(net, net_optimizer, 'model_saved/{}'.format(keyword))\n",
    "#     train_loss_history = list(np.load('model_saved/train_loss_history_{}.npy'.format(keyword)))\n",
    "#     eval_loss_history = list(np.load('model_saved/eval_loss_history_{}.npy'.format(keyword)))\n",
    "# except:\n",
    "#     print(\"Loading {} Error\".format(keyword))\n",
    "\n",
    "# keyword = 'NSML_pure_jamo_50ms_pad_cut_1024_copy'\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "    print((datetime.now().strftime('%m-%d %H:%M:%S')))\n",
    "\n",
    "    preloader_train.initialize_batch(num_thread)\n",
    "#     loss_list_train = list()\n",
    "    seq2seq_loss_list_train = list()\n",
    "\n",
    "    total_dist = 0\n",
    "    total_length = 0\n",
    "    count = 0\n",
    "    net.eval()\n",
    "    jamo2char_net.train()\n",
    "    \n",
    "    while preloader_train.end_flag == False:\n",
    "        batch = preloader_train.get_batch()\n",
    "        # logger.info(\"Got Batch\")\n",
    "        if batch != None:\n",
    "            tensor_input, ground_truth, loss_mask, length_list, lev_truth, lev_truth_loss_mask = batch\n",
    "            pred_tensor, loss = evaluate(net, ctc_loss, tensor_input.to(device),\n",
    "                                      ground_truth.to(device), loss_mask.to(device), length_list.to(device))\n",
    "#             loss_list_train.append(loss)\n",
    "\n",
    "#             jamo_result = Decode_Prediction_No_Filtering(pred_tensor, tokenizer)\n",
    "            \n",
    "            lev_input = ground_truth\n",
    "            lev_pred, attentions, seq2seq_loss = jamo2char_net.net_train(lev_input.to(device), lev_truth.to(device), lev_truth_loss_mask.to(device), jamo2char_optimizer, seq2seq_criterion)\n",
    "            \n",
    "#             print('lev_truth shape : {}'.format(lev_truth.shape))\n",
    "#             print('lev_input shape : {}'.format(lev_input.shape))\n",
    "#             print('attentions shape : {}'.format(attentions.shape))\n",
    "            \n",
    "            lev_input = Decode_Jamo_Prediction_And_Batch(pred_tensor)\n",
    "            lev_pred, attentions, seq2seq_loss = jamo2char_net.net_train(lev_input.to(device), lev_truth.to(device), lev_truth_loss_mask.to(device), jamo2char_optimizer, seq2seq_criterion)\n",
    "            \n",
    "#             print('lev_pred shape : {}'.format(lev_pred.shape))\n",
    "#             print('attentions shape : {}'.format(attentions.shape))\n",
    "            \n",
    "            seq2seq_loss_list_train.append(seq2seq_loss)\n",
    "        \n",
    "            pred_string_list = Decode_Lev(lev_pred, index2char)\n",
    "            true_string_list = Decode_Lev_Truth(lev_truth.detach().cpu().numpy(), index2char)\n",
    "            dist, length = char_distance_list(true_string_list, pred_string_list)\n",
    "            \n",
    "            total_dist += dist\n",
    "            total_length += length\n",
    "            \n",
    "            count += 1\n",
    "            if count % 15 == 0:\n",
    "                print(\"Count {} | {} => {}\".format(count, true_string_list[0], pred_string_list[0]))\n",
    "                plt.figure(figsize = (3, 3))\n",
    "                plt.imshow(attentions[0].detach().cpu().numpy().T)\n",
    "                plt.show()\n",
    "            \n",
    "        \n",
    "    print(\"Count {} | {} => {}\".format(count, true_string_list[0], pred_string_list[0]))\n",
    "    plt.figure(figsize = (3, 3))\n",
    "    plt.imshow(attentions[0].detach().cpu().numpy().T)\n",
    "    plt.show()\n",
    "\n",
    "    train_cer = total_dist / total_length\n",
    "#     train_loss = np.mean(np.asarray(loss_list_train))\n",
    "    train_seq2seq_loss = np.mean(np.asarray(seq2seq_loss_list_train))\n",
    "    \n",
    "    print((datetime.now().strftime('%m-%d %H:%M:%S')))\n",
    "#     print(\"Mean Train Loss: {}\".format(train_loss))\n",
    "    print(\"Mean Train Seq2Seq Loss: {}\".format(train_seq2seq_loss))\n",
    "    print(\"Train CER: {}\".format(train_cer))\n",
    "    \n",
    "#     train_loss_history.append(train_loss)\n",
    "    train_cer_history.append(train_cer)\n",
    "    \n",
    "    train_seq2seq_loss_history.append(train_seq2seq_loss)\n",
    "    \n",
    "    ###########################################################\n",
    "    \n",
    "    preloader_eval.initialize_batch(num_thread)\n",
    "#     loss_list_eval = list()\n",
    "    seq2seq_loss_list_eval = list()\n",
    "\n",
    "    total_dist = 0\n",
    "    total_length = 0\n",
    "    \n",
    "    net.eval()\n",
    "    jamo2char_net.eval()\n",
    "    \n",
    "    while preloader_eval.end_flag == False:\n",
    "        batch = preloader_eval.get_batch()\n",
    "        if batch != None:\n",
    "            tensor_input, ground_truth, loss_mask, length_list, lev_truth, lev_truth_loss_mask = batch\n",
    "            pred_tensor, loss = evaluate(net, ctc_loss, tensor_input.to(device),\n",
    "                                      ground_truth.to(device), loss_mask.to(device), length_list.to(device))\n",
    "#             loss_list_eval.append(loss)\n",
    "            \n",
    "#             jamo_result = Decode_Prediction_No_Filtering(pred_tensor, tokenizer)\n",
    "            \n",
    "#             lev_input = Decode_Jamo_Prediction_And_Batch(pred_tensor)\n",
    "            lev_input = ground_truth\n",
    "            lev_pred, attentions, seq2seq_loss = jamo2char_net.net_eval(lev_input.to(device), lev_truth.to(device), lev_truth_loss_mask.to(device), seq2seq_criterion)\n",
    "        \n",
    "            lev_input = Decode_Jamo_Prediction_And_Batch(pred_tensor)\n",
    "            lev_pred, attentions, seq2seq_loss = jamo2char_net.net_eval(lev_input.to(device), lev_truth.to(device), lev_truth_loss_mask.to(device), seq2seq_criterion)\n",
    "            seq2seq_loss_list_eval.append(seq2seq_loss) ###############\n",
    "        \n",
    "            pred_string_list = Decode_Lev(lev_pred, index2char)\n",
    "            true_string_list = Decode_Lev_Truth(lev_truth.detach().cpu().numpy(), index2char)\n",
    "            dist, length = char_distance_list(true_string_list, pred_string_list)\n",
    "            \n",
    "            total_dist += dist\n",
    "            total_length += length\n",
    "            \n",
    "    print(\"Evaluation {} => {}\".format(true_string_list[0], pred_string_list[0]))\n",
    "    plt.figure(figsize = (3, 3))\n",
    "    plt.imshow(attentions[0].detach().cpu().numpy().T)\n",
    "    plt.show()\n",
    "            \n",
    "    eval_cer = total_dist / total_length\n",
    "#     eval_loss = np.mean(np.asarray(loss_list_eval))\n",
    "    \n",
    "    eval_seq2seq_loss = np.mean(np.asarray(seq2seq_loss_list_eval)) ##############\n",
    "    \n",
    "    print((datetime.now().strftime('%m-%d %H:%M:%S')))\n",
    "#     print(\"Mean Evaluation Loss: {}\".format(eval_loss))\n",
    "    print(\"Mean Evaluation Seq2Seq Loss: {}\".format(eval_seq2seq_loss)) ##################\n",
    "    print(\"Evaluation CER: {}\".format(eval_cer))\n",
    "    \n",
    "#     eval_loss_history.append(eval_loss)\n",
    "    eval_cer_history.append(eval_cer)\n",
    "    \n",
    "    eval_seq2seq_loss_history.append(eval_seq2seq_loss) ##################\n",
    "    \n",
    "    #####\n",
    "    \n",
    "#     save(net, net_optimizer, 'model_saved/{}'.format(keyword))\n",
    "    save(jamo2char_net, jamo2char_optimizer, 'model_saved/seq_{}'.format(keyword))\n",
    "    \n",
    "#     np.save('model_saved/train_loss_history_{}'.format(keyword), train_loss_history)\n",
    "#     np.save('model_saved/eval_loss_history_{}'.format(keyword), eval_loss_history)\n",
    "    \n",
    "    np.save('model_saved/train_seq_loss_history_{}'.format(keyword), train_seq2seq_loss_history)\n",
    "    np.save('model_saved/eval_seq_loss_history_{}'.format(keyword), eval_seq2seq_loss_history)\n",
    "    \n",
    "    np.save('model_saved/train_cer_history_{}'.format(keyword), train_cer_history)\n",
    "    np.save('model_saved/eval_cer_history_{}'.format(keyword), eval_cer_history)\n",
    "            \n",
    "    #####    \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.subplot(131)\n",
    "#     plt.plot(train_loss_history)\n",
    "#     plt.plot(eval_loss_history)\n",
    "\n",
    "    plt.subplot(132)\n",
    "    plt.plot(train_seq2seq_loss_history)\n",
    "    plt.plot(eval_seq2seq_loss_history)\n",
    "\n",
    "    plt.subplot(133)\n",
    "    plt.plot(train_cer_history)\n",
    "    plt.plot(eval_cer_history)\n",
    "    plt.show()\n",
    "        \n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Y9t1zv6cHp7y",
    "outputId": "e08fd2c2-ea3e-4479-e4c6-fe6dbe4e2a7a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EPOCH = 20 * 10\n",
    "           \n",
    "# # net = Mel2SeqNet(80, 512, 256)\n",
    "\n",
    "\n",
    "# # net = Mel2SeqNet(80, 1024, 512, len(unicode_jamo_list), device)\n",
    "\n",
    "# # keyword = 'NSML_pure_jamo_50ms_pad_cut'\n",
    "# # net = Mel2SeqNet(80, 512, 256, len(unicode_jamo_list), device)\n",
    "\n",
    "# # keyword = 'NSML_pure_jamo_50ms_pad_cut_1024'\n",
    "\n",
    "# keyword = 'NSML_pure_jamo_50ms_pad_cut_1024_copy'\n",
    "\n",
    "# net = Mel2SeqNet(80, 1024, 512, len(unicode_jamo_list), device)\n",
    "# net_optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "# ctc_loss = nn.CTCLoss().to(device)\n",
    "\n",
    "# jamo2char_net = Seq2SeqNet(256, jamo_tokens, char2index, device)\n",
    "# jamo2char_optimizer = optim.Adam(jamo2char_net.parameters(), lr=0.001)\n",
    "# seq2seq_criterion = nn.NLLLoss(reduction='none').to(device)\n",
    "\n",
    "# train_loss_history = list()\n",
    "# eval_loss_history = list()\n",
    "\n",
    "# train_seq2seq_loss_history = list()\n",
    "# eval_seq2seq_loss_history = list()\n",
    "\n",
    "# train_cer_history = list()\n",
    "# eval_cer_history = list()\n",
    "\n",
    "# try:\n",
    "#     train_cer_history = list(np.load('model_saved/train_cer_history_{}.npy'.format(keyword)))\n",
    "#     eval_cer_history = list(np.load('model_saved/eval_cer_history_{}.npy'.format(keyword)))\n",
    "# except:\n",
    "#     print(\"No CER Record\")\n",
    "    \n",
    "# try:\n",
    "#     load(jamo2char_net, jamo2char_optimizer, 'model_saved/seq_{}'.format(keyword))\n",
    "#     train_seq2seq_loss_history = list(np.load('model_saved/train_seq_loss_history_{}'.format(keyword)))\n",
    "#     eval_seq2seq_loss_history = list(np.load('model_saved/eval_seq_loss_history_{}'.format(keyword)))\n",
    "# except:\n",
    "#     print('No Seq2Seq Loss Record')\n",
    "\n",
    "# try:\n",
    "#     load(net, net_optimizer, 'model_saved/{}'.format(keyword))\n",
    "#     train_loss_history = list(np.load('model_saved/train_loss_history_{}.npy'.format(keyword)))\n",
    "#     eval_loss_history = list(np.load('model_saved/eval_loss_history_{}.npy'.format(keyword)))\n",
    "# except:\n",
    "#     print(\"Loading {} Error\".format(keyword))\n",
    "\n",
    "# # keyword = 'NSML_pure_jamo_50ms_pad_cut_1024_copy'\n",
    "\n",
    "# for epoch in range(EPOCH):\n",
    "\n",
    "#     print((datetime.now().strftime('%m-%d %H:%M:%S')))\n",
    "\n",
    "#     preloader_train.initialize_batch(num_thread)\n",
    "#     loss_list_train = list()\n",
    "#     seq2seq_loss_list_train = list()\n",
    "\n",
    "#     total_dist = 0\n",
    "#     total_length = 0\n",
    "#     count = 0\n",
    "#     net.train()\n",
    "#     jamo2char_net.train()\n",
    "    \n",
    "#     while preloader_train.end_flag == False:\n",
    "#         batch = preloader_train.get_batch()\n",
    "#         # logger.info(\"Got Batch\")\n",
    "#         if batch != None:\n",
    "#             tensor_input, ground_truth, loss_mask, length_list, lev_truth, lev_truth_loss_mask = batch\n",
    "#             pred_tensor, loss = train(net, net_optimizer, ctc_loss, tensor_input.to(device),\n",
    "#                                       ground_truth.to(device), loss_mask.to(device), length_list.to(device))\n",
    "#             loss_list_train.append(loss)\n",
    "            \n",
    "# #             print(pred_tensor.shape)\n",
    "\n",
    "#             jamo_result = Decode_Prediction_No_Filtering(pred_tensor, tokenizer)\n",
    "            \n",
    "#             lev_input = Decode_Jamo_Prediction_And_Batch(pred_tensor)\n",
    "#             lev_input = Decode_Jamo_Prediction_And_Batch(ground_truth)\n",
    "# #             print('lev_input shape: {}'.format(lev_input.shape))\n",
    "# #             print('lev_truth shape: {}'.format(lev_truth.shape))\n",
    "            \n",
    "#             lev_pred, attentions, seq2seq_loss = jamo2char_net.net_train(lev_input.to(device), lev_truth.to(device), lev_truth_loss_mask.to(device), jamo2char_optimizer, seq2seq_criterion)\n",
    "#             seq2seq_loss_list_train.append(seq2seq_loss)\n",
    "        \n",
    "#             pred_string_list = Decode_Lev(lev_pred, index2char)\n",
    "#             true_string_list = Decode_Lev_Truth(lev_truth.detach().cpu().numpy(), index2char)\n",
    "#             dist, length = char_distance_list(true_string_list, pred_string_list)\n",
    "            \n",
    "# #             print(true_string_list)\n",
    "# #             print(pred_string_list)\n",
    "#             count += 1\n",
    "    \n",
    "#             if count % 60 == 0:\n",
    "#                 print(\"Count {}, {} => {} => {}\".format(count, true_string_list[0], jamo_result[0], pred_string_list[0]))\n",
    "#                 plt.figure(figsize = (3, 3))\n",
    "#                 plt.imshow(attentions[0].detach().cpu().numpy().T)\n",
    "#                 plt.show()\n",
    "#             ###################\n",
    "            \n",
    "# #             lev_pred_list = Decode_Prediction(pred_tensor, tokenizer, char2index)\n",
    "# #             lev_str_list = lev_num_to_lev_string(lev_truth_list, index2char)\n",
    "# #             dist, length = char_distance_list(lev_str_list, lev_pred_list)\n",
    "#             total_dist += dist\n",
    "#             total_length += length\n",
    "\n",
    "#     train_cer = total_dist / total_length\n",
    "#     train_loss = np.mean(np.asarray(loss_list_train))\n",
    "#     train_seq2seq_loss = np.mean(np.asarray(seq2seq_loss_list_train))\n",
    "    \n",
    "#     print((datetime.now().strftime('%m-%d %H:%M:%S')))\n",
    "#     print(\"Mean Train Loss: {}\".format(train_loss))\n",
    "#     print(\"Mean Train Seq2Seq Loss: {}\".format(train_seq2seq_loss))\n",
    "#     print(\"Train CER: {}\".format(train_cer))\n",
    "    \n",
    "    \n",
    "#     train_loss_history.append(train_loss)\n",
    "#     train_cer_history.append(train_cer)\n",
    "    \n",
    "#     train_seq2seq_loss_history.append(train_seq2seq_loss)\n",
    "    \n",
    "#     ###########################################################\n",
    "    \n",
    "#     preloader_eval.initialize_batch(num_thread)\n",
    "#     loss_list_eval = list()\n",
    "#     seq2seq_loss_list_eval = list()\n",
    "\n",
    "#     total_dist = 0\n",
    "#     total_length = 0\n",
    "    \n",
    "#     net.eval()\n",
    "#     jamo2char_net.eval()\n",
    "    \n",
    "#     while preloader_eval.end_flag == False:\n",
    "#         batch = preloader_eval.get_batch()\n",
    "#         if batch != None:\n",
    "#             tensor_input, ground_truth, loss_mask, length_list, lev_truth, lev_truth_loss_mask = batch\n",
    "#             pred_tensor, loss = evaluate(net, net_optimizer, ctc_loss, tensor_input.to(device),\n",
    "#                                       ground_truth.to(device), loss_mask.to(device), length_list.to(device))\n",
    "#             loss_list_eval.append(loss)\n",
    "            \n",
    "#             jamo_result = Decode_Prediction_No_Filtering(pred_tensor, tokenizer)\n",
    "            \n",
    "#             lev_input = Decode_Jamo_Prediction_And_Batch(pred_tensor)\n",
    "#             lev_input = Decode_Jamo_Prediction_And_Batch(ground_truth)\n",
    "            \n",
    "#             lev_pred, attentions, seq2seq_loss = jamo2char_net.net_eval(lev_input.to(device), lev_truth.to(device), lev_truth_loss_mask.to(device), seq2seq_criterion)\n",
    "#             seq2seq_loss_list_eval.append(seq2seq_loss) ###############\n",
    "        \n",
    "#             pred_string_list = Decode_Lev(lev_pred, index2char)\n",
    "#             true_string_list = Decode_Lev_Truth(lev_truth.detach().cpu().numpy(), index2char)\n",
    "#             dist, length = char_distance_list(true_string_list, pred_string_list)\n",
    "            \n",
    "#             total_dist += dist\n",
    "#             total_length += length\n",
    "            \n",
    "#     eval_cer = total_dist / total_length\n",
    "#     eval_loss = np.mean(np.asarray(loss_list_eval))\n",
    "    \n",
    "#     eval_seq2seq_loss = np.mean(np.asarray(seq2seq_loss_list_eval)) ##############\n",
    "    \n",
    "#     print((datetime.now().strftime('%m-%d %H:%M:%S')))\n",
    "#     print(\"Mean Evaluation Loss: {}\".format(eval_loss))\n",
    "#     print(\"Mean Evaluation Seq2Seq Loss: {}\".format(eval_seq2seq_loss)) ##################\n",
    "#     print(\"Evaluation CER: {}\".format(eval_cer))\n",
    "    \n",
    "#     eval_loss_history.append(eval_loss)\n",
    "#     eval_cer_history.append(eval_cer)\n",
    "    \n",
    "#     eval_seq2seq_loss_history.append(eval_seq2seq_loss) ##################\n",
    "    \n",
    "#     #####\n",
    "    \n",
    "# #     save(net, net_optimizer, 'model_saved/{}'.format(keyword))\n",
    "#     save(jamo2char_net, jamo2char_optimizer, 'model_saved/seq_{}'.format(keyword))\n",
    "    \n",
    "#     np.save('model_saved/train_loss_history_{}'.format(keyword), train_loss_history)\n",
    "#     np.save('model_saved/eval_loss_history_{}'.format(keyword), eval_loss_history)\n",
    "    \n",
    "#     np.save('model_saved/train_seq_loss_history_{}'.format(keyword), train_seq2seq_loss_history)\n",
    "#     np.save('model_saved/eval_seq_loss_history_{}'.format(keyword), eval_seq2seq_loss_history)\n",
    "    \n",
    "#     np.save('model_saved/train_cer_history_{}'.format(keyword), train_cer_history)\n",
    "#     np.save('model_saved/eval_cer_history_{}'.format(keyword), eval_cer_history)\n",
    "            \n",
    "#     #####    \n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.subplot(131)\n",
    "#     plt.plot(train_loss_history)\n",
    "#     plt.plot(eval_loss_history)\n",
    "\n",
    "#     plt.subplot(132)\n",
    "#     plt.plot(train_seq2seq_loss_history)\n",
    "#     plt.plot(eval_seq2seq_loss_history)\n",
    "\n",
    "#     plt.subplot(133)\n",
    "#     plt.plot(train_cer_history)\n",
    "#     plt.plot(eval_cer_history)\n",
    "#     plt.show()\n",
    "        \n",
    "#     print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CTC_best_result_on_Colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
