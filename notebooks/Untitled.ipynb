{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, input_tensor, ground_truth, loss_mask, target_lengths):\n",
    "\n",
    "    # Shape of the input tensor (B, T, F)\n",
    "    # B: Number of a batch (8, 16, or 64 ...)\n",
    "    # T: Temporal length of an input\n",
    "    # F: Number of frequency band, 80\n",
    "\n",
    "    batch_size = input_tensor.shape[0]\n",
    "\n",
    "    self.encoder_optimizer.zero_grad()\n",
    "    self.decoder_optimizer.zero_grad()\n",
    "\n",
    "    # (B, T, F) -> (B, T, H)\n",
    "    encoded_tensor = self.encoder(input_tensor)\n",
    "\n",
    "    # (B, T, H) -> (B, T, 75)\n",
    "    pred_tensor = self.decoder(encoded_tensor)\n",
    "\n",
    "    # Cast true sentence as Long data type, since CTC loss takes long tensor only\n",
    "    # Shape (B, S)\n",
    "    # S: Max length among true sentences \n",
    "    truth = ground_truth\n",
    "    truth = truth.type(torch.cuda.LongTensor)\n",
    "\n",
    "    # CTC loss function takes tensor of the form (T, B, 75)\n",
    "    # Permute function changes axes of a tensor T <-> B\n",
    "    pred_tensor = pred_tensor.permute(1, 0, 2)\n",
    "\n",
    "    # CTC loss need to know the lenght of the true sentence\n",
    "    input_lengths = torch.full(size=(batch_size,), fill_value=pred_tensor.shape[0], dtype=torch.long)\n",
    "\n",
    "    # Calculate CTC Loss\n",
    "    loss = self.ctc_loss(pred_tensor, truth, input_lengths, target_lengths)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    self.encoder_optimizer.step()\n",
    "    self.decoder_optimizer.step()\n",
    "\n",
    "    # Return loss divided by true length because loss is sum of the character losses\n",
    "\n",
    "    return pred_tensor, loss.item() / ground_truth.shape[1]\n",
    "\n",
    "\n",
    "def test(self, input_tensor, ground_truth, loss_mask, target_lengths):\n",
    "\n",
    "    # Shape of the input tensor (B, T, F)\n",
    "    # B: Number of a batch (8, 16, or 64 ...)\n",
    "    # T: Temporal length of an input\n",
    "    # F: Number of frequency band, 80\n",
    "\n",
    "    batch_size = input_tensor.shape[0]\n",
    "\n",
    "    # (B, T, F) -> (B, T, H)\n",
    "    encoded_tensor = self.encoder(input_tensor)\n",
    "\n",
    "    # (B, T, H) -> (B, T, 75)\n",
    "    pred_tensor = self.decoder(encoded_tensor)\n",
    "\n",
    "    # Cast true sentence as Long data type, since CTC loss takes long tensor only\n",
    "    # Shape (B, S)\n",
    "    # S: Max length among true sentences \n",
    "    truth = ground_truth\n",
    "    truth = truth.type(torch.cuda.LongTensor)\n",
    "\n",
    "    # CTC loss function takes tensor of the form (T, B, 75)\n",
    "    # Permute function changes axes of a tensor T <-> B\n",
    "    pred_tensor = pred_tensor.permute(1, 0, 2)\n",
    "\n",
    "    # CTC loss need to know the lenght of the true sentence\n",
    "    input_lengths = torch.full(size=(batch_size,), fill_value=pred_tensor.shape[0], dtype=torch.long)\n",
    "\n",
    "    # Calculate CTC Loss\n",
    "    loss = self.ctc_loss(pred_tensor, truth, input_lengths, target_lengths)\n",
    "\n",
    "    # Return loss divided by true length because loss is sum of the character losses\n",
    "\n",
    "    return pred_tensor, loss.item() / ground_truth.shape[1]\n",
    "\n",
    "def save(self, check_point_name):\n",
    "    torch.save({\n",
    "        'encoder_state_dict': self.encoder.state_dict(),\n",
    "        'decoder_state_dict': self.decoder.state_dict(),\n",
    "        'encoder_optimizer_state_dict': self.encoder_optimizer.state_dict(),\n",
    "        'decoder_optimizer_state_dict': self.decoder_optimizer.state_dict(),\n",
    "        }, check_point_name)\n",
    "\n",
    "def load(self, check_point_name):\n",
    "    checkpoint = torch.load(check_point_name)\n",
    "    self.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    self.decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    self.encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    self.decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "\n",
    "def set_mode(self, mode):\n",
    "\n",
    "    # Must call .train() after loading if you want to train again\n",
    "    if mode == 'train':\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "\n",
    "    # Must call .eval() after loading if you do not want to use dropouts\n",
    "    elif mode == 'eval':\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid mode string: {}\".format(mode))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
