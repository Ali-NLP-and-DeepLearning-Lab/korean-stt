{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12853, 5)\n",
      "(12833, 5)\n"
     ]
    }
   ],
   "source": [
    "n_mels = 80\n",
    "fs = 44100\n",
    "frame_length_ms=50\n",
    "frame_shift_ms=25\n",
    "nsc = int(fs * frame_length_ms / 1000)\n",
    "nov = nsc - int(fs * frame_shift_ms / 1000)\n",
    "nhop = int(fs * frame_shift_ms / 1000)\n",
    "eps = 1e-8\n",
    "db_ref = 160\n",
    "\n",
    "meta_path = \"D:/korean-single-speaker-speech-dataset/transcript.v.1.2.txt\"\n",
    "data_folder = \"D:/korean-single-speaker-speech-dataset/kss\"\n",
    "\n",
    "with open(meta_path, encoding='utf-8') as f:\n",
    "    metadata = np.array([line.strip().split('|') for line in f])\n",
    "#     hours = sum((int(x[2]) for x in metadata)) * frame_shift_ms / (3600 * 1000)\n",
    "#     log('Loaded metadata for %d examples (%.2f hours)' % (len(metadata), hours))\n",
    "\n",
    "# metadata = metadata[:32, :2]\n",
    "\n",
    "max_sequence_len = max(list(map(len, metadata[:, 1])))\n",
    "\n",
    "error_jamos = [5868, 5998, 6046, 6155, 6202, \n",
    "               6654, 6890, 7486, 7502, 7744, \n",
    "               7765, 8267, 9069, 9927, 10437, \n",
    "               10515, 10533, 10606, 10610, 12777]\n",
    "\n",
    "print(metadata.shape)\n",
    "metadata = np.delete(metadata, error_jamos, axis = 0)\n",
    "print(metadata.shape)\n",
    "\n",
    "dataset_size = len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_length = list()\n",
    "file_length = list()\n",
    "division_length = list()\n",
    "unicode_jamo_list = list()\n",
    "\n",
    "for i in range(len(metadata)):\n",
    "    character_length.append(len(metadata[i, 3]))\n",
    "    file_length.append(float(metadata[i, 4]))\n",
    "    division_length.append(float(metadata[i, 4]) * 1000 / len(metadata[i, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_name_list = []\n",
    "\n",
    "for data in metadata:\n",
    "    wave_name_list.append(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', ' ', '!', ',', '.', '<eos>', '<sos>', '?', 'ᄀ', 'ᄁ', 'ᄂ', 'ᄃ', 'ᄄ', 'ᄅ', 'ᄆ', 'ᄇ', 'ᄈ', 'ᄉ', 'ᄊ', 'ᄋ', 'ᄌ', 'ᄍ', 'ᄎ', 'ᄏ', 'ᄐ', 'ᄑ', 'ᄒ', 'ᅡ', 'ᅢ', 'ᅣ', 'ᅤ', 'ᅥ', 'ᅦ', 'ᅧ', 'ᅨ', 'ᅩ', 'ᅪ', 'ᅫ', 'ᅬ', 'ᅭ', 'ᅮ', 'ᅯ', 'ᅰ', 'ᅱ', 'ᅲ', 'ᅳ', 'ᅴ', 'ᅵ', 'ᆨ', 'ᆩ', 'ᆪ', 'ᆫ', 'ᆬ', 'ᆭ', 'ᆮ', 'ᆯ', 'ᆰ', 'ᆱ', 'ᆲ', 'ᆳ', 'ᆴ', 'ᆵ', 'ᆶ', 'ᆷ', 'ᆸ', 'ᆹ', 'ᆺ', 'ᆻ', 'ᆼ', 'ᆽ', 'ᆾ', 'ᆿ', 'ᇀ', 'ᇁ', 'ᇂ']\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "unicode_jamo_list = list()\n",
    "for unicode in range(0x1100, 0x1113):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "    \n",
    "for unicode in range(0x1161, 0x1176):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "    \n",
    "for unicode in range(0x11A8, 0x11C3):\n",
    "    unicode_jamo_list.append(chr(unicode))\n",
    "    \n",
    "unicode_jamo_list += [' ', '!', ',', '.', '?', '<sos>', '<eos>']\n",
    "    \n",
    "unicode_jamo_list.sort()\n",
    "\n",
    "unicode_jamo_list = ['-'] + unicode_jamo_list\n",
    "\n",
    "print(unicode_jamo_list)\n",
    "print(len(unicode_jamo_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, vocabs):\n",
    "        self.vocabs = vocabs\n",
    "        \n",
    "    def word2num(self, sentence):\n",
    "        tokens = list()\n",
    "        for char in sentence:\n",
    "            tokens.append(self.vocabs.index(char))    \n",
    "        return tokens\n",
    "        \n",
    "    def word2vec(self, sentence):\n",
    "        vectors = np.zeros((len(sentence), len(self.vocabs)))\n",
    "        for i, char in enumerate(sentence):\n",
    "            vectors[i, self.vocabs.index(char)] = 1   \n",
    "        return vectors\n",
    "    \n",
    "    def num2word(self, num):\n",
    "        output = list()\n",
    "        for i in num:\n",
    "            output.append(self.vocabs[i])\n",
    "        return output\n",
    "    \n",
    "    def num2vec(self, numbers):\n",
    "        vectors = np.zeros((len(numbers), len(self.vocabs)))\n",
    "        for i, num in enumerate(numbers):\n",
    "            vectors[i, num] = 1   \n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(unicode_jamo_list)\n",
    "jamo_tokens = tokenizer.word2num(unicode_jamo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7139d482a934619bb1b49c03f298f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12833), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mel_path_list = list()\n",
    "\n",
    "for i, wav_name in enumerate(tqdm(wave_name_list)):\n",
    "    \n",
    "    npy_name = wav_name.replace('.wav', '.npy')\n",
    "    wav_path = os.path.join(data_folder, wav_name)  \n",
    "    mel_path = os.path.join(data_folder + '/mel', npy_name)\n",
    "    mel_path_list.append(mel_path)\n",
    "    \n",
    "    if not os.path.isfile(mel_path):\n",
    "#         print(\"{}\".format(mel_path))\n",
    "        y, sr = librosa.core.load(wav_path)\n",
    "        f, t, Zxx = sp.signal.stft(y, fs=sr, nperseg=nsc, noverlap=nov)\n",
    "        Sxx = np.abs(Zxx)\n",
    "        Sxx = np.maximum(Sxx, eps)\n",
    "\n",
    "        # plt.figure(figsize=(20,20))\n",
    "        # plt.imshow(20*np.log10(Sxx), origin='lower')\n",
    "        # plt.colorbar()\n",
    "        # plt.show()\n",
    "\n",
    "        mel_filters = librosa.filters.mel(sr=fs, n_fft=nsc, n_mels=n_mels)\n",
    "        mel_specgram = np.matmul(mel_filters, Sxx)\n",
    "\n",
    "    #   log_specgram = 20*np.log10(Sxx)\n",
    "    #   norm_log_specgram = (log_specgram + db_ref) / db_ref\n",
    "\n",
    "        log_mel_specgram = 20 * np.log10(np.maximum(mel_specgram, eps))\n",
    "        norm_log_mel_specgram = (log_mel_specgram + db_ref) / db_ref\n",
    "\n",
    "    #   np.save(specgram_path, norm_log_specgram)\n",
    "        np.save(mel_path, norm_log_mel_specgram)\n",
    "    #   np.save(specgram_path, Sxx)\n",
    "\n",
    "    #     print(norm_log_mel_specgram.shape[1])\n",
    "\n",
    "    #     if i % 1000 == 0:\n",
    "    #         plt.figure(figsize=(8, 4))\n",
    "    #         plt.imshow(20 * np.log10(Sxx), origin='lower', aspect='auto')\n",
    "    #         plt.colorbar()\n",
    "    #         plt.show()\n",
    "\n",
    "    #         plt.figure(figsize=(8, 4))\n",
    "    #         plt.imshow(norm_log_mel_specgram, origin='lower', aspect='auto')\n",
    "    #         plt.colorbar()\n",
    "    #         plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGRU(nn.Module):\n",
    "    def __init__(self, D_in, bidirectional=True):\n",
    "        super(ResidualGRU, self).__init__()\n",
    "        self.gru = nn.GRU(D_in, int(D_in/2), bidirectional=bidirectional, batch_first=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, input_tensor):\n",
    "        \n",
    "#         print(\"Residual Input: {}\".format(input_tensor.shape))\n",
    "        gru_output, _ = self.gru(input_tensor)\n",
    "        activated = self.relu(gru_output)  \n",
    "#         print(\"Residual Output: {}\".format(activated.shape))\n",
    "        output_tensor = torch.add(activated, input_tensor)\n",
    "        \n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.H = H\n",
    "        \n",
    "        self.fc = torch.nn.Linear(D_in, H)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.gru = nn.GRU(H, D_out, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "            \n",
    "        output_tensor = self.fc(input_tensor)\n",
    "        output_tensor = self.relu(output_tensor)\n",
    "        output_tensor = self.dropout(output_tensor)\n",
    "        \n",
    "        output_tensor, _ = self.gru(output_tensor)\n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "class CTC_Decoder(nn.Module):\n",
    "    def __init__(self, H, D_out):\n",
    "        super(CTC_Decoder, self).__init__()\n",
    "        self.H = H\n",
    "        \n",
    "        self.fc_embed = nn.Linear(512, 512)\n",
    "        self.relu_embed = torch.nn.ReLU()\n",
    "        self.dropout_embed = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.gru = nn.GRU(H, H, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(512, 75)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "\n",
    "        output_tensor = self.fc_embed(input_tensor)\n",
    "        output_tensor = self.relu_embed(output_tensor)\n",
    "        output_tensor = self.dropout_embed(output_tensor)\n",
    "        \n",
    "        output_tensor,_ = self.gru(output_tensor)\n",
    "\n",
    "        output_tensor = self.fc(output_tensor)\n",
    "        \n",
    "#         print(\"Output Tensor Shape: {}\".format(output_tensor.shape))\n",
    "        \n",
    "        prediction_tensor = self.log_softmax(output_tensor)\n",
    "\n",
    "        return prediction_tensor\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, H, D_out):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.H = H\n",
    "        \n",
    "        self.fc_embed = nn.Linear(256, 512)\n",
    "        self.relu_embed = torch.nn.ReLU()\n",
    "        self.dropout_embed = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.gru = nn.GRU(H, H, batch_first=True)\n",
    "        self.attention = ForcedAttentionModule(D_out * 2)\n",
    "        \n",
    "        self.fc = nn.Linear(512 * 2, 74)\n",
    "        self.log_softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_tensor, query, pos):\n",
    "\n",
    "        output_tensor = self.fc_embed(input_tensor)\n",
    "        output_tensor = self.relu_embed(output_tensor)\n",
    "        output_tensor = self.dropout_embed(output_tensor)\n",
    "        \n",
    "        output_tensor, hidden_tensor = self.gru(output_tensor, hidden_tensor)\n",
    "        \n",
    "        context_vector, alpha = self.attention(query, output_tensor, pos)\n",
    "        output_tensor = torch.cat([output_tensor, context_vector], dim=2)\n",
    "\n",
    "        output_tensor = self.fc(output_tensor)\n",
    "        prediction_tensor = self.softmax(output_tensor)\n",
    "\n",
    "        return prediction_tensor, hidden_tensor, context_vector, alpha\n",
    "\n",
    "class AdditiveAttentionModule(torch.nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(AdditiveAttentionModule, self).__init__()\n",
    "        self.fc_alpha = nn.Linear(H, 1)\n",
    "        self.W = nn.Linear(H, H)\n",
    "        self.V = nn.Linear(H, H)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, query, key):\n",
    "        output_tensor = torch.tanh(torch.add(self.W(query), self.V(key)))\n",
    "        e = self.fc_alpha(output_tensor)\n",
    "        e_sig = self.sigmoid(e)\n",
    "        alpha = self.softmax(e_sig).transpose(1, 2)\n",
    "        \n",
    "        context_vector = torch.bmm(alpha, query)\n",
    "        \n",
    "        return context_vector, alpha\n",
    "    \n",
    "            \n",
    "        alpha_forced = torch.zeros(alpha.shape)\n",
    "    \n",
    "class MultiplicativeAttentionModule(torch.nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(MultiplicativeAttentionModule, self).__init__()\n",
    "        self.Wa = nn.Linear(H, H)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, query, key):\n",
    "        input_tensor = self.Wa(key)\n",
    "        output_tensor = torch.bmm(query, input_tensor.transpose(1, 2))\n",
    "        alpha = self.softmax(output_tensor).transpose(1, 2)\n",
    "        context_vector = torch.bmm(alpha, query)\n",
    "        \n",
    "        return context_vector, alpha\n",
    "    \n",
    "class ForcedAttentionModule(torch.nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(ForcedAttentionModule, self).__init__()\n",
    "        self.Wa = nn.Linear(H, H)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "#         self.force_variable = torch.rand(1, requires_grad=True)\n",
    "        self.force_variable = Variable(torch.tensor(1.0), requires_grad=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, query, key, pos):\n",
    "        input_tensor = self.Wa(key)\n",
    "        output_tensor = torch.bmm(query, input_tensor.transpose(1, 2))\n",
    "        alpha = self.softmax(output_tensor).transpose(1, 2)\n",
    "        \n",
    "        alpha_forced = torch.zeros(alpha.shape).to(device)\n",
    "        \n",
    "        if (pos < 5):\n",
    "            alpha_forced[:, 0, pos:pos + 5] = torch.tensor([5.0, 4.0, 3.0, 2.0 ,1.0]) / 15\n",
    "            \n",
    "        elif (pos > alpha.shape[2] - 5):\n",
    "            alpha_forced[:, 0, pos - 5:pos] = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0]) / 15\n",
    "            \n",
    "        else:\n",
    "            alpha_forced[:, 0, pos - 4 : pos + 5] = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 4.0, 3.0, 2.0 ,1.0]) / 25\n",
    "            \n",
    "        force_rate = self.sigmoid(self.force_variable).to(device)\n",
    "        pass_rate = Variable(1 - force_rate).to(device)\n",
    "        \n",
    "        alpha = torch.add(pass_rate * alpha, force_rate * alpha_forced)\n",
    "        \n",
    "        context_vector = torch.bmm(alpha, query)\n",
    "        \n",
    "        return context_vector, alpha\n",
    "\n",
    "class Mel2SeqNet():\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(Mel2SeqNet, self).__init__()\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.H = H\n",
    "        \n",
    "        self.encoder = Encoder(D_in, H, D_out).to(device)\n",
    "        self.embedding_layer = nn.Embedding(len(jamo_tokens), 256).to(device)\n",
    "        self.decoder = CTC_Decoder(H, D_out).to(device)\n",
    "\n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=0.001)\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=0.001)\n",
    "        self.embedding_optimizer = optim.Adam(self.embedding_layer.parameters(), lr=0.001)\n",
    "\n",
    "#         self.criterion = nn.CrossEntropyLoss(reduction='none').to(device)\n",
    "        self.ctc_loss = nn.CTCLoss().to(device)\n",
    "        \n",
    "        for param in self.encoder.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "        for param in self.embedding_layer.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "        for param in self.decoder.parameters():\n",
    "            param.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def train(self, input_tensor, ground_truth, loss_mask, target_lengths):\n",
    "        \n",
    "        batch_size = input_tensor.shape[0]\n",
    "\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        self.embedding_optimizer.zero_grad()\n",
    "\n",
    "        encoded_tensor = self.encoder(input_tensor)\n",
    "\n",
    "        pred_tensor = self.decoder(encoded_tensor)\n",
    "            \n",
    "        truth = ground_truth\n",
    "        truth = truth.type(torch.cuda.LongTensor)\n",
    "        \n",
    "        pred_tensor = pred_tensor.permute(1, 0, 2)\n",
    "        \n",
    "        input_lengths = torch.full(size=(batch_size,), fill_value=pred_tensor.shape[0], dtype=torch.long)\n",
    "        \n",
    "#         print(pred_tensor)\n",
    "\n",
    "        loss = self.ctc_loss(pred_tensor, truth, input_lengths, target_lengths)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        self.embedding_optimizer.step()\n",
    "        \n",
    "        return pred_tensor, loss.item() / ground_truth.shape[1]\n",
    "    \n",
    "    def save(self, check_point_name):\n",
    "        torch.save({\n",
    "            'embedding_layer_state_dict': self.embedding_layer.state_dict(),\n",
    "            'encoder_state_dict': self.encoder.state_dict(),\n",
    "            'decoder_state_dict': self.decoder.state_dict(),\n",
    "            'embedding_optimizer_state_dict': self.embedding_optimizer.state_dict(),\n",
    "            'encoder_optimizer_state_dict': self.encoder_optimizer.state_dict(),\n",
    "            'decoder_optimizer_state_dict': self.decoder_optimizer.state_dict(),\n",
    "            }, check_point_name)\n",
    "    \n",
    "    def load(self, check_point_name):\n",
    "        checkpoint = torch.load(check_point_name)\n",
    "        self.embedding_layer.load_state_dict(checkpoint['embedding_layer_state_dict'])\n",
    "        self.encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        self.decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "        self.embedding_optimizer.load_state_dict(checkpoint['embedding_optimizer_state_dict'])\n",
    "        self.encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "        self.decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n",
    "    \n",
    "        self.embedding_layer.eval()\n",
    "        self.encoder.eval()\n",
    "        self.decoder.eval()\n",
    "        \n",
    "        self.embedding_layer.train()\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "    \n",
    "net = Mel2SeqNet(80, 512, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")      \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batched_Preloader():\n",
    "    def __init__(self, mel_path_list, ground_truth_list, batch_size):\n",
    "        super(Batched_Preloader).__init__()\n",
    "        self.mel_path_list = mel_path_list\n",
    "        self.total_num_input = len(mel_path_list)\n",
    "        self.tensor_input_list = [None] * self.total_num_input\n",
    "        self.ground_truth_list = ground_truth_list\n",
    "        self.sentence_length_list = np.asarray(list(map(len, ground_truth_list)))\n",
    "#         self.shuffle_step = 4\n",
    "        self.shuffle_step = 12\n",
    "        self.loading_sequence = None\n",
    "        self.end_flag = True\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def load(self, i):\n",
    "        norm_log_mel_specgram = np.load(self.mel_path_list[i])\n",
    "        input_spectrogram = norm_log_mel_specgram.T\n",
    "        tensor_input = torch.tensor(input_spectrogram).view(1, input_spectrogram.shape[0], input_spectrogram.shape[1])\n",
    "        self.tensor_input_list[i] = tensor_input\n",
    "        \n",
    "    def get(self, i):\n",
    "        if type(self.tensor_input_list[i]) == type(None):\n",
    "            self.load(i)\n",
    "        return self.tensor_input_list[i]  \n",
    "    \n",
    "    def initialize_batch(self):\n",
    "        loading_sequence = np.argsort(self.sentence_length_list)\n",
    "        bundle = np.stack([self.sentence_length_list[loading_sequence], loading_sequence])\n",
    "        \n",
    "        for seq_len in range(self.shuffle_step, np.max(self.sentence_length_list), self.shuffle_step):\n",
    "            idxs = np.where((bundle[0, :] > seq_len) & (bundle[0, :] <= seq_len + self.shuffle_step))[0]\n",
    "            idxs_origin = copy.deepcopy(idxs)\n",
    "            random.shuffle(idxs)\n",
    "            bundle[:, idxs_origin] = bundle[:, idxs]\n",
    "            \n",
    "        loading_sequence = bundle[1, :]\n",
    "        \n",
    "        self.loading_sequence = loading_sequence\n",
    "        self.current_loading_index = 0\n",
    "        self.end_flag = False\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def get_batch(self):\n",
    "        \n",
    "        tensor_list = list()\n",
    "        ground_truth_list = list()\n",
    "        tensor_size_list = list()\n",
    "        ground_truth_size_list = list()\n",
    "        \n",
    "        count = 0\n",
    "        max_seq_len = 0\n",
    "        max_sen_len = 0\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            if self.current_loading_index >= self.total_num_input:\n",
    "                self.end_flag = True\n",
    "                break\n",
    "            \n",
    "            tensor = self.get(self.loading_sequence[self.current_loading_index])\n",
    "            tensor_list.append(tensor)\n",
    "            tensor_size_list.append(tensor.shape[1])\n",
    "            \n",
    "            ground_truth = self.ground_truth_list[self.loading_sequence[self.current_loading_index]]\n",
    "            ground_truth_list.append(ground_truth)\n",
    "            ground_truth_size_list.append(len(ground_truth))\n",
    "            \n",
    "            \n",
    "            if (tensor.shape[1] > max_seq_len):\n",
    "                max_seq_len = tensor.shape[1]\n",
    "            if (len(ground_truth) > max_sen_len):\n",
    "                max_sen_len = len(ground_truth)  \n",
    "            \n",
    "            self.current_loading_index += 1\n",
    "            count += 1\n",
    "            \n",
    "        batched_tensor = torch.zeros(count, max_seq_len + 5, n_mels)\n",
    "        batched_ground_truth = torch.zeros(count, max_sen_len)\n",
    "        batched_loss_mask = torch.zeros(count, max_sen_len)\n",
    "        ground_truth_size_list = torch.tensor(np.asarray(ground_truth_size_list), dtype=torch.long)\n",
    "        \n",
    "        for order in range(count):\n",
    "            \n",
    "            target = tensor_list[order]\n",
    "        \n",
    "            pad_random = np.random.randint(0, 5)\n",
    "            \n",
    "            if pad_random > 0:\n",
    "                offset = torch.zeros(target.shape[0], pad_random, target.shape[2])\n",
    "                target = torch.cat((offset, target), 1)\n",
    "                \n",
    "            target = target + (torch.rand(target.shape) - 0.5) / 20\n",
    "        \n",
    "            target = torch.clamp(target, min=0.0, max=1.0)\n",
    "            \n",
    "            batched_tensor[order, :tensor_size_list[order] + pad_random, :] = target\n",
    "            batched_ground_truth[order, :ground_truth_size_list[order]] = torch.tensor(ground_truth_list[order])\n",
    "            batched_loss_mask[order, :ground_truth_size_list[order]] = torch.ones(ground_truth_size_list[order])\n",
    "        \n",
    "        return batched_tensor, batched_ground_truth, batched_loss_mask, ground_truth_size_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_list = [(tokenizer.word2num(list(metadata[i, 3]) + ['<eos>'])) for i in range(len(metadata))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preloader = Batched_Preloader(mel_path_list, ground_truth_list, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decode_CTC_Prediction(prediction):\n",
    "    CTC_pred = prediction.detach().cpu().numpy()\n",
    "    result = list()\n",
    "    last_elem = 0\n",
    "    for i, elem in enumerate(CTC_pred):\n",
    "        if elem != last_elem and elem != 0:\n",
    "            result.append(elem)\n",
    "        \n",
    "        last_elem = elem\n",
    "        \n",
    "    \n",
    "    result = np.asarray(result)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_history = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# preloader.initialize_batch()\n",
    "# preloader.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Error\n",
      "08-31 00:40:03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-9e441185f27e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mpreloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_flag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtensor_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREPEAT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-0cc909a711b2>\u001b[0m in \u001b[0;36mget_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloading_sequence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_loading_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[0mtensor_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[0mtensor_size_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-0cc909a711b2>\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_input_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_input_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-0cc909a711b2>\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mnorm_log_mel_specgram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmel_path_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0minput_spectrogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm_log_mel_specgram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtensor_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_spectrogram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_spectrogram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_spectrogram\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCH = 5 * 60 * 4\n",
    "REPEAT = 1\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    try:\n",
    "         net.load('model_saved/CTC_with_noise_and_pad_5')\n",
    "    except:\n",
    "        print(\"Loading Error\")\n",
    "    preloader.initialize_batch()\n",
    "    counter = 0\n",
    "    loss_list = list()\n",
    "    print(datetime.now().strftime('%m-%d %H:%M:%S'))\n",
    "    \n",
    "    while preloader.end_flag == False:\n",
    "        tensor_input, ground_truth, loss_mask, length_list = preloader.get_batch()\n",
    "        \n",
    "        for i in range(REPEAT):\n",
    "            pred_tensor, loss = net.train(tensor_input.to(device), ground_truth.to(device), loss_mask.to(device), length_list.to(device))\n",
    "        \n",
    "        counter += 1\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "#         if (counter % 300 == 0):\n",
    "#             print('Calculated {} Batches'.format(counter))\n",
    "#             print('Loss {}: {}'.format(counter, loss))\n",
    "#             print(datetime.now().strftime('%m-%d %H:%M:%S'))\n",
    "\n",
    "#             _, index = pred_tensor[:, 0, :].max(-1)\n",
    "    \n",
    "#             sentence = tokenizer.num2word(index.view(-1))\n",
    "#             print(''.join(sentence))\n",
    "#             index_ = Decode_CTC_Prediction(index)\n",
    "#             sentence_ = tokenizer.num2word(index_)\n",
    "#             print(''.join(sentence_))\n",
    "#             true_sentence = tokenizer.num2word(ground_truth[0, :].detach().numpy().astype(int))\n",
    "#             print(''.join(true_sentence))\n",
    "            \n",
    "#             plt.figure()\n",
    "#             plt.imshow(pred_tensor[:, 0, :].detach().cpu().numpy())\n",
    "#             plt.colorbar()\n",
    "#             plt.show()\n",
    "\n",
    "    print(\"Mean Loss: {}\".format(np.mean(np.asarray(loss_list))))\n",
    "    lost_history.append(np.mean(np.asarray(loss_list)))\n",
    "    net.save('model_saved/CTC_with_noise_and_pad_5')\n",
    "    \n",
    "    if ((epoch != 0) and (epoch % 50 == 0)):\n",
    "        plt.figure()\n",
    "        plt.plot(lost_history)\n",
    "        plt.show()\n",
    "        \n",
    "        _, index = pred_tensor[:, 0, :].max(-1)\n",
    "\n",
    "        sentence = tokenizer.num2word(index.view(-1))\n",
    "        print(''.join(sentence))\n",
    "        index_ = Decode_CTC_Prediction(index)\n",
    "        sentence_ = tokenizer.num2word(index_)\n",
    "        print(''.join(sentence_))\n",
    "        true_sentence = tokenizer.num2word(ground_truth[0, :].detach().numpy().astype(int))\n",
    "        print(''.join(true_sentence))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.imshow(pred_tensor[:, 0, :].detach().cpu().numpy())\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "                        \n",
    "    print(\"----------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
