{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joovvhan/korean-stt/blob/master/kang/rnnUnit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GF1Tm7bfglc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2771c88-56eb-4e7a-8b3d-cbed097ac9ad"
      },
      "source": [
        "import torch\n",
        "import torch.optim\n",
        "import numpy as np \n",
        "char_set = ['h', 'i', 'e', 'l','o']\n",
        "x_data = [[0,1,0,2,3,3]]\n",
        "x_one_hot = [[[1,0,0,0,0], [0,1,0,0,0], [1,0,0,0,0], [0,0,1,0,0],\n",
        "             [0,0,0,1,0], [0,0,0,1,0]]]\n",
        "y_data = [[1,0,2,3,3,4]]\n",
        "\n",
        "input_size = len(char_set)\n",
        "hidden_size = len(char_set)\n",
        "learning_rate = 0.1\n",
        "\n",
        "\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), learning_rate)\n",
        "\n",
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  outputs, _status = rnn(X)\n",
        "  loss = criterion(outputs.view(-1, input_size), Y.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  result = outputs.data.numpy().argmax(axis=2)\n",
        "  result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
        "  print(i, \"loss : \",loss.item(), \" prediction : \", result, \" true Y : \", y_data, \" prediction str : \", result_str)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 loss :  1.5023871660232544  prediction :  [[3 3 3 3 3 3]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  llllll\n",
            "1 loss :  1.3443511724472046  prediction :  [[3 3 2 3 3 3]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  llelll\n",
            "2 loss :  1.2347151041030884  prediction :  [[1 3 2 3 3 3]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ilelll\n",
            "3 loss :  1.1592987775802612  prediction :  [[1 3 2 3 3 3]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ilelll\n",
            "4 loss :  1.047301173210144  prediction :  [[1 3 2 3 3 3]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ilelll\n",
            "5 loss :  1.0057692527770996  prediction :  [[1 3 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ilello\n",
            "6 loss :  0.9408038258552551  prediction :  [[1 0 2 3 3 3]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihelll\n",
            "7 loss :  0.8864296078681946  prediction :  [[1 0 2 3 3 3]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihelll\n",
            "8 loss :  0.8458597660064697  prediction :  [[1 0 2 3 3 3]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihelll\n",
            "9 loss :  0.7989758849143982  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "10 loss :  0.7691277861595154  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "11 loss :  0.7294049263000488  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "12 loss :  0.7045612335205078  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "13 loss :  0.6787621378898621  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "14 loss :  0.66154944896698  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "15 loss :  0.6513826251029968  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "16 loss :  0.6382997035980225  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "17 loss :  0.6232197880744934  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "18 loss :  0.6097817420959473  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "19 loss :  0.5986859798431396  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "20 loss :  0.5894530415534973  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "21 loss :  0.5782266855239868  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "22 loss :  0.5691977739334106  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "23 loss :  0.5647203326225281  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "24 loss :  0.5582225918769836  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "25 loss :  0.5492643713951111  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "26 loss :  0.5430987477302551  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "27 loss :  0.5402432084083557  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "28 loss :  0.5364091992378235  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "29 loss :  0.5310035347938538  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "30 loss :  0.5259178876876831  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "31 loss :  0.5218471884727478  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "32 loss :  0.5188473463058472  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "33 loss :  0.5159956216812134  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "34 loss :  0.512993574142456  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "35 loss :  0.5102345943450928  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "36 loss :  0.5074352622032166  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "37 loss :  0.505329430103302  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "38 loss :  0.5041186213493347  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "39 loss :  0.5027334094047546  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "40 loss :  0.5009785294532776  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "41 loss :  0.49943485856056213  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "42 loss :  0.4980681836605072  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "43 loss :  0.49681782722473145  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "44 loss :  0.49548450112342834  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "45 loss :  0.4941275119781494  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "46 loss :  0.4931812584400177  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "47 loss :  0.49235105514526367  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "48 loss :  0.49149462580680847  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "49 loss :  0.4905942380428314  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "50 loss :  0.48982834815979004  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "51 loss :  0.4891912043094635  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "52 loss :  0.48843392729759216  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "53 loss :  0.48767349123954773  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "54 loss :  0.486919641494751  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "55 loss :  0.4863002300262451  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "56 loss :  0.4856642782688141  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "57 loss :  0.48502957820892334  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "58 loss :  0.48441824316978455  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "59 loss :  0.4838869571685791  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "60 loss :  0.48340678215026855  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "61 loss :  0.48292839527130127  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "62 loss :  0.4824490547180176  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "63 loss :  0.48198404908180237  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "64 loss :  0.4815526008605957  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "65 loss :  0.4811340272426605  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "66 loss :  0.4807148873806  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "67 loss :  0.4802861511707306  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "68 loss :  0.47987231612205505  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "69 loss :  0.4794750511646271  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "70 loss :  0.4790990352630615  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "71 loss :  0.47872909903526306  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "72 loss :  0.47836366295814514  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "73 loss :  0.47800660133361816  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "74 loss :  0.4776592254638672  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "75 loss :  0.47732582688331604  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "76 loss :  0.4769962728023529  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "77 loss :  0.47667255997657776  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "78 loss :  0.47634753584861755  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "79 loss :  0.4760306775569916  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "80 loss :  0.47571754455566406  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "81 loss :  0.4754147529602051  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "82 loss :  0.47511616349220276  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "83 loss :  0.47482433915138245  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "84 loss :  0.47453752160072327  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "85 loss :  0.474257230758667  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "86 loss :  0.47398510575294495  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "87 loss :  0.4737192690372467  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "88 loss :  0.47346141934394836  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "89 loss :  0.473207950592041  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "90 loss :  0.47296062111854553  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "91 loss :  0.47271761298179626  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "92 loss :  0.47248029708862305  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "93 loss :  0.47224846482276917  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "94 loss :  0.472022145986557  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "95 loss :  0.47180137038230896  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "96 loss :  0.4715850353240967  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "97 loss :  0.47137388586997986  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "98 loss :  0.4711667597293854  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n",
            "99 loss :  0.47096434235572815  prediction :  [[1 0 2 3 3 4]]  true Y :  [[1, 0, 2, 3, 3, 4]]  prediction str :  ihello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCpzRgkrklZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "4aa3dc0a-326e-442c-802f-980439d562c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OjdqqjfxOYW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4cfe790a-3d45-4f84-fc15-ccfd573f8a68"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters = len(all_letters) + 1 # Plus EOS marker\n",
        "\n",
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "    c for c in unicodedata.normalize('NFD', s)\n",
        "    if unicodedata.category(c) != 'Mn'\n",
        "    and c in all_letters\n",
        "  )\n",
        "\n",
        "def readLines(filename):\n",
        "  lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "  return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "for filename in findFiles('gdrive/My Drive/data/data/names/*.txt'):\n",
        "  category = os.path.splitext(os.path.basename(filename))[0]\n",
        "  all_categories.append(category)\n",
        "  lines = readLines(filename)\n",
        "  category_lines[category] = lines\n",
        "  \n",
        "n_categories = len(all_categories)\n",
        "\n",
        "if n_categories == 0:\n",
        "  raise RuntimeError('Data not found.')\n",
        "  \n",
        "print('# categories : ', n_categories, all_categories)\n",
        "print(unicodeToAscii(\"O'Néàl\"))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# categories :  18 ['Korean', 'Polish', 'Scottish', 'Vietnamese', 'Spanish', 'Portuguese', 'Russian', 'English', 'Chinese', 'Dutch', 'Czech', 'French', 'Irish', 'Greek', 'German', 'Italian', 'Japanese', 'Arabic']\n",
            "O'Neal\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwI4JXSKg81f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    \n",
        "    self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size)\n",
        "    self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size)\n",
        "    self.o2o = nn.Linear(hidden_size + output_size, output_size)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "  def forward(self, category, input, hidden):\n",
        "    input_combined = torch.cat((category, input, hidden), 1)\n",
        "    hidden = self.i2h(input_combined)\n",
        "    output = self.i2o(input_combined)\n",
        "    output_combined = torch.cat((hidden, output), 1)\n",
        "    output = self.o2o(output_combined)\n",
        "    output = self.dropout(output)\n",
        "    output = self.softmax(output)\n",
        "    return output, hidden\n",
        "  \n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2ZRXupJ2ZlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "#무작위 반환\n",
        "def randomChoice(l):\n",
        "  return l[random.randint(0, len(l) - 1)]\n",
        "\n",
        "#임의의 category, 임의의 line\n",
        "def randomTrainingPair():\n",
        "  category = randomChoice(all_categories)\n",
        "  line = randomChoice(category_lines[category])\n",
        "  return category, line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxtonUpE4CPr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#category one-hot vector\n",
        "def categoryTensor(category):\n",
        "  li = all_categories.index(category)\n",
        "  tensor = torch.zeros(1, n_categories)\n",
        "  tensor[0][li] = 1\n",
        "  return tensor\n",
        "\n",
        "#처음부터 마지막(EOS빼고)까지의 one-hot 행렬\n",
        "def inputTensor(line):\n",
        "  tensor = torch.zeros(len(line), 1, n_letters)\n",
        "  for li in range(len(line)):\n",
        "    letter = line[li]\n",
        "    tensor[li][0][all_letters.find(letter)] = 1\n",
        "  return tensor\n",
        "\n",
        "#두번째 문자부터 마지막(EOS)가지의 longTensor\n",
        "def targetTensor(line):\n",
        "  letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
        "  letter_indexes.append(n_letters - 1)\n",
        "  return torch.LongTensor(letter_indexes)\n",
        "\n",
        "#임의의 category에서 category, input, target Tensor를 생성\n",
        "def randomTrainingExample():\n",
        "  category, line = randomTrainingPair()\n",
        "  category_tensor = categoryTensor(category)\n",
        "  input_line_tensor = inputTensor(line)\n",
        "  target_line_tensor = targetTensor(line)\n",
        "  return category_tensor, input_line_tensor, target_line_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSMCWzt89EXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "\n",
        "learning_rate = 0.0005\n",
        "\n",
        "#모든 단계에서 예측을 수행 (모든 단계에서 손실 계산)\n",
        "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
        "  target_line_tensor.unsqueeze_(-1)\n",
        "  hidden = rnn.initHidden()\n",
        "  \n",
        "  rnn.zero_grad()\n",
        "  loss = 0\n",
        "  \n",
        "  #손실함수, loss\n",
        "  for i in range(input_line_tensor.size(0)):\n",
        "    output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
        "    l = criterion(output, target_line_tensor[i])\n",
        "    loss += l\n",
        "    \n",
        "  loss.backward()\n",
        "  \n",
        "  for p in rnn.parameters():\n",
        "    p.data.add_(-learning_rate, p.grad.data)\n",
        "    \n",
        "  return output, loss.item() / input_line_tensor.size(0)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWfbjhNQ-g9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "#시간을 알기 위해 시간 문자열 반환\n",
        "def timeSince(since):\n",
        "  now = time.time()\n",
        "  s = now - since\n",
        "  m = math.floor(s/60)\n",
        "  s -= m * 60\n",
        "  return '%dm %ds' % (m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97wEE6jv-zTc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "c1215578-b42a-4d20-838b-5efe3d8bf28e"
      },
      "source": [
        "rnn = RNN(n_letters, 128, n_letters)\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 500\n",
        "all_losses = []\n",
        "total_loss = 0\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "#train 호출, print_Every마다 시간과 손실 출력\n",
        "for iter in range(1, n_iters + 1):\n",
        "  output, loss = train(*randomTrainingExample())\n",
        "  total_loss += loss\n",
        "  \n",
        "  if iter % print_every == 0:\n",
        "    print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
        "    \n",
        "  #all_losses에 평균 손실 저장\n",
        "  if iter % plot_every == 0:\n",
        "    all_losses.append(total_loss / plot_every)\n",
        "    total_loss = 0"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 19s (5000 5%) 2.6444\n",
            "0m 38s (10000 10%) 2.4867\n",
            "0m 58s (15000 15%) 3.9912\n",
            "1m 17s (20000 20%) 2.2756\n",
            "1m 36s (25000 25%) 2.1359\n",
            "1m 55s (30000 30%) 3.2848\n",
            "2m 14s (35000 35%) 2.6175\n",
            "2m 33s (40000 40%) 2.2653\n",
            "2m 52s (45000 45%) 2.3834\n",
            "3m 11s (50000 50%) 2.2265\n",
            "3m 30s (55000 55%) 1.6757\n",
            "3m 49s (60000 60%) 2.8886\n",
            "4m 9s (65000 65%) 2.7583\n",
            "4m 28s (70000 70%) 2.8790\n",
            "4m 47s (75000 75%) 2.3494\n",
            "5m 6s (80000 80%) 2.2666\n",
            "5m 25s (85000 85%) 1.1808\n",
            "5m 44s (90000 90%) 2.8207\n",
            "6m 4s (95000 95%) 3.4138\n",
            "6m 23s (100000 100%) 1.7016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px3ygFOoBDAZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "a00251c9-d9f5-4669-ecf6-6677b48bfbfd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f210a167080>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd81PX9wPHX+y6XRQIJISFAQsII\nexMQZSgiCDjQaitarau1w1ltnW21+mvV2qrV1qoVq3XviiguRARlhb3CniEkYWQA2ff+/XGXGJJc\nBoRcuLyfj0ce3H3u872875vjfZ/7rK+oKsYYY1oPh78DMMYY07ws8RtjTCtjid8YY1oZS/zGGNPK\nWOI3xphWxhK/Mca0Mpb4jTGmlbHEb4wxrYwlfmOMaWWC/B1AbTp06KDJycn+DsMYY04Zy5Yt26+q\nsQ2p2yITf3JyMmlpaf4OwxhjThkisrOhda2rxxhjWpkGJ34RcYrIChGZVctjISLylohsEZHFIpJc\n5bF7vOUbReTcpgnbGGPM8WpMi/9WYIOPx64HDqlqT+AJ4FEAEekHTAf6A5OBZ0TEefzhGmOMOVEN\nSvwikgCcB7zgo8o04GXv7XeBCSIi3vI3VbVYVbcDW4CRJxayMcaYE9HQFv+TwJ2A28fjXYDdAKpa\nBuQBMVXLvfZ4y2oQkRtEJE1E0nJychoYljHGmMaqN/GLyPlAtqouO5mBqOrzqpqqqqmxsQ2akWSM\nMeY4NKTFPxq4UER2AG8CZ4vIq9XqZACJACISBLQDDlQt90rwlhljjPGTehO/qt6jqgmqmoxnoPYr\nVb2yWrWZwNXe25d666i3fLp31k83IAVY0mTRV/PUnM3M22TdRMYYU5fjnscvIg+KyIXeuzOAGBHZ\nAtwO3A2gquuAt4H1wKfAjapafmIh+/bcvK3Mt8RvjDF1atTKXVX9Gvjae/sPVcqLgB/6OOZPwJ+O\nO8JGCAt2Ulh60j5XjDEmIATUyt1QlyV+Y4ypT0Al/jCXkyJL/MYYU6fASvzBTgpLLPEbY0xdAirx\nW1ePMcbUL6ASf5jLSWGpr8XFxhhjIAATf7G1+I0xpk6BlfhtOqcxxtQroBJ/qMsGd40xpj4BlfjD\nbHDXGGPqFViJP9hh8/iNMaYegZX4XU5Ky5XScpvZY4wxvgRU4g91ea7qaK1+Y4zxLaASf1iwJ/Fb\nP78xxvgWWIm/osVfYl09xhjjS0Al/oquHmvxG2OMbwGV+MMs8RtjTL0CKvFXtvhtEZcxxvgUUIm/\nYnDXZvUYY4xvgZX4ravHGGPqFZiJ37p6jDHGp3ovti4iocA3QIi3/ruqen+1Ok8A4713w4E4VY3y\nPlYOrPE+tktVL2yi2GsIDfZ8jlmL3xhjfKs38QPFwNmqelhEXMACEZmtqosqKqjqrytui8jNwNAq\nxxeq6pAmi7gOYbZy1xhj6lVvV496HPbedXl/tI5DLgfeaILYGs1m9RhjTP0a1McvIk4RWQlkA1+o\n6mIf9ZKAbsBXVYpDRSRNRBaJyEUnHHEdXE4HLqdYV48xxtShQYlfVcu93TUJwEgRGeCj6nQ8YwBV\nM2+SqqYCVwBPikiP2g4UkRu8HxBpOTk5jXgJx7ILrhtjTN0aNatHVXOBucBkH1WmU62bR1UzvP9u\nA77m2P7/qvWeV9VUVU2NjY1tTFjHCHM5rY/fGGPqUG/iF5FYEamYoRMGTATSa6nXB4gGFlYpixaR\nEO/tDsBoYH3ThF67sGC7/KIxxtSlIbN6OgEvi4gTzwfF26o6S0QeBNJUdaa33nTgTVWtOvDbF3hO\nRNzeYx9R1ZOb+F1Oikptd05jjPGl3sSvqquppXtGVf9Q7f4DtdT5Dhh4AvE1mvXxG2NM3QJq5S7Y\nBdeNMaY+gZf4g21w1xhj6hJ4id9lg7vGGFOXgEv81sdvjDF1C7jEHxbssK4eY4ypQ+AlfuvqMcaY\nOgVm4i8t59jlBMYYYyoEXOIPcTlxK5SU2yIuY4ypTcAlfrsKlzHG1C3gEn/FnvwlZdbiN8aY2gRc\n4g8J8rykYkv8xhhTq8BL/K6KxG9dPcYYU5vAS/xBFdfdtRa/McbUJgATv7X4jTGmLoGb+K3Fb4wx\ntQq8xO+d1WODu8YYU7vAS/zW1WOMMXUK4MRvLX5jjKlNwCX+igVc1sdvjDG1C7jEb109xhhTt8BL\n/C6bx2+MMXWpN/GLSKiILBGRVSKyTkT+WEuda0QkR0RWen9+WuWxq0Vks/fn6qZ+AdVZi98YY+oW\n1IA6xcDZqnpYRFzAAhGZraqLqtV7S1VvqlogIu2B+4FUQIFlIjJTVQ81RfC1CXIIDrHBXWOM8aXe\nFr96HPbedXl/GnqVk3OBL1T1oDfZfwFMPq5IG0hECAlyWuI3xhgfGtTHLyJOEVkJZONJ5ItrqXaJ\niKwWkXdFJNFb1gXYXaXOHm/ZSRXiclBs1901xphaNSjxq2q5qg4BEoCRIjKgWpWPgGRVHYSnVf9y\nYwMRkRtEJE1E0nJychp7+DFCghzW4jfGGB8aNatHVXOBuVTrrlHVA6pa7L37AjDcezsDSKxSNcFb\nVttzP6+qqaqaGhsb25iwarCuHmOM8a0hs3piRSTKezsMmAikV6vTqcrdC4EN3tufAZNEJFpEooFJ\n3rKTytPit64eY4ypTUNm9XQCXhYRJ54PirdVdZaIPAikqepM4BYRuRAoAw4C1wCo6kEReQhY6n2u\nB1X1YFO/iOpCXA6bx2+MMT7Um/hVdTUwtJbyP1S5fQ9wj4/jXwRePIEYGy00yGktfmOM8SHgVu5C\nxawea/EbY0xtAjPx2+CuMcb4FKCJ3wZ3jTHGlwBO/NbiN8aY2gRo4ndaH78xxvgQmInfZV09xhjj\nS2AmfuvqMcYYnwI08TspKi1HtaGbiBpjTOsRoInfgVuhzG2J3xhjqgvMxO+quAqXdfcYY0x1AZn4\nQ73X3bU9+Y0xpqaATPzfX3fXWvzGGFNdgCZ+b4vfEr8xxtQQoIm/osVvXT3GGFNdYCb+isFdW71r\njDE1BGbi93b1FNngrjHG1BCgid8Gd40xxpcATfw2uGuMMb4EZuJ32eCuMcb4EpiJP8gGd40xxpcA\nTfzW1WOMMb7Um/hFJFRElojIKhFZJyJ/rKXO7SKyXkRWi8gcEUmq8li5iKz0/sxs6hdQm1Dr6jHG\nGJ+CGlCnGDhbVQ+LiAtYICKzVXVRlTorgFRVPSoivwT+AlzmfaxQVYc0bdh1sxa/Mcb4Vm+LXz0O\ne++6vD9arc5cVT3qvbsISGjSKBsp2NvHb/P4jTGmpgb18YuIU0RWAtnAF6q6uI7q1wOzq9wPFZE0\nEVkkIhfV8Ttu8NZLy8nJaVDwvjgdgssp1uI3xphaNCjxq2q5t7smARgpIgNqqyciVwKpwGNVipNU\nNRW4AnhSRHr4+B3Pq2qqqqbGxsY26kXUxi64bowxtWvUrB5VzQXmApOrPyYi5wD3AReqanGVYzK8\n/24DvgaGnkC8DRYS5KDIBneNMaaGhszqiRWRKO/tMGAikF6tzlDgOTxJP7tKebSIhHhvdwBGA+ub\nLnzf4tqGkplb2By/yhhjTikNmdXTCXhZRJx4PijeVtVZIvIgkKaqM/F07UQA74gIwC5VvRDoCzwn\nIm7vsY+oarMk/h6xbVi1J7c5fpUxxpxS6k38qrqaWrpnVPUPVW6f4+PY74CBJxLg8eoZF8HHazIp\nKi2vvBSjMcaYAF25C57Erwpbcw7XX9kYY1qRgE78AFuyLfEbY0xVAZv4u3Vog0NgqyV+Y4w5RsAm\n/pAgJ13bh7M154i/QzHGmBYlYBM/eLp7rKvHGGOOFdCJv0dcBNv3H6Gs3FbwGmNMhYBO/ClxkZSU\nu9lxwLp7jDGmQkAn/iGJUQAs23nIz5EYY0zLEdCJv0dsG6LDXZb4jTGmioBO/CLC8KRo0izxG2NM\npYBO/ADDkqLZlnOEg0dK/B2KMca0CAGf+FOT2gOw3Fr9xhgDtILEPyihHS6nsGyXJX5jjIFWkPhD\nXU76d27Hsh2W+I0xBlpB4gdITYpm1Z5cSuwavMYY0zoS//CkaIrL3Kzbm+fvUIwxxu9aTeIHW8hl\njDHQShJ/XNtQEtuHWeI3xhhaSeIHz7TOpTsO8pt3VvHSt9v9HY4xxvhNq0n8w5Ki2X+4hHeX7eG5\nb7ahqv4OyRhj/KLexC8ioSKyRERWicg6EfljLXVCROQtEdkiIotFJLnKY/d4yzeKyLlNG37DXTSk\nM7dP7MWN43uQmVfEzgNH/RWKMcb4VUNa/MXA2ao6GBgCTBaRUdXqXA8cUtWewBPAowAi0g+YDvQH\nJgPPiIizqYJvjMhQF7dMSOEHwxIA+G7rAX+EYYwxfldv4lePistYubw/1ftJpgEve2+/C0wQEfGW\nv6mqxaq6HdgCjGySyI9T9w5tiIsMYeE2S/zGmNapQX38IuIUkZVANvCFqi6uVqULsBtAVcuAPCCm\narnXHm+Z34gIp/eIYeHWA9bPb4xplRqU+FW1XFWHAAnASBEZ0NSBiMgNIpImImk5OTlN/fTHOKNH\nDPsPF7Nqjy3oMsa0Po2a1aOqucBcPP31VWUAiQAiEgS0Aw5ULfdK8JbV9tzPq2qqqqbGxsY2JqxG\nm9gvng4RIfz6rZXkF5We1N9ljDEtTUNm9cSKSJT3dhgwEUivVm0mcLX39qXAV+rpR5kJTPfO+ukG\npABLmir449W+TTDP/HgYuw8e5dr/LGVrzuH6DzLGmADRkBZ/J2CuiKwGluLp458lIg+KyIXeOjOA\nGBHZAtwO3A2gquuAt4H1wKfAjapa3tQv4niM7Naexy8bwqasAqY8OZ811u1jjGklpCUOcKampmpa\nWlqz/K7s/CLGPTaXHw5P5KGLmnzowhhjmoWILFPV1IbUbTUrd32JaxvKhD4dmb02k7Jy27bZGBP4\nWn3iBzh/UCf2Hy5hyfaD/g7FGGNOOkv8wFm94wgPdvLR6kx/h2KMMSddkL8DaAnCgp1MGdCJt9N2\nMzQxirZhLkJdDs7qHefv0IwxpslZ4vd6cFp/MvMKufO91QCEBDlYdf8kQl1+2VrIGGNOGuvq8WoT\nEsSL14zg3ql9uO2cFIrL3CzdYX3+xpjAY4m/ilCXkxvG9eCGcd0Jdjr4ZtPJ3TrCGGP8wRJ/LcKD\ng0hNjmb+5v3+DsUYY5qcJX4fxvWKJX1fAVn5Rf4OxRhjmpQlfh/GpXg2intkdrpt5GaMCSiW+H3o\n2ymSn5/ZnQ9XZjDlyfnsPui5VGNL3OLCGGMao9Xv1VOfFbsOcfWLS2gX7qJ9eDAZuUXMvGk0naPC\n/B2aMcZUsr16mtDQrtG8fN1IDh0p5XBxGUdLyvj1Wys5cLi48luAMcacSqzF30CFJeWEBDl4d/ke\n7nzXs8jL5RS+vfts4iJD/RydMaa1a0yL31buNlBYsGcF7w+HJ1BQVEZmbiEvLNjOgs37+cGwBD9H\nZ4wxDWddPY0kIlw/phv3Tu1LTJtgm+tvjDnlWOI/Tg6HMCalA/M378ftbnndZcYY44sl/hMwNiWW\n/YeLSd9X4O9QjDGmwSzxn4CxKR0AmGd7+hhjTiGW+E9Ax7ahDE+K5uXvdnC0pMzf4RhjTINY4j9B\nd0/pw778Iv79zfbKsuxq+/tsyT7Mil2Hmjs0Y4ypVb2JX0QSRWSuiKwXkXUicmstdX4rIiu9P2tF\npFxE2nsf2yEia7yPtazJ+U1gRHJ7pg6M59l5W9l14CivLtrJaQ/PIX1fPgBl5W6ufWkJFz/zHbe9\nuYKi0nI/R2yMae0a0uIvA+5Q1X7AKOBGEelXtYKqPqaqQ1R1CHAPME9Vq17FZLz38QYtLjjV3Hde\nP4Kcwq9eX8Yjs9NRhXkbPf3+s1ZnsvtgIef278j/Vu7lveV7/BytMaa1qzfxq2qmqi733i4ANgBd\n6jjkcuCNpgnv1NAlKow/XzyQtRn5lLuVTu1C+W7rAdxu5V9fbyUlLoJ//Xg4XaLC+HqjDQQbY/yr\nUSt3RSQZGAos9vF4ODAZuKlKsQKfi4gCz6nq8z6OvQG4AaBr166NCatFuGBwZ7Lyi0iIDmfh1v28\ns2wPn6zNZGNWAX/74WAcDuGs3rF8sCKD4rJyQoLsWr7GGP9o8OCuiEQA7wG3qWq+j2oXAN9W6+YZ\no6rDgCl4uonG1Xagqj6vqqmqmhobG9vQsFqUn47tzuQB8ZzeowNHS8q59/01dI9tw7QhnQEY3zuO\noyXlpO2wgV5jjP80KPGLiAtP0n9NVd+vo+p0qnXzqGqG999s4ANg5PGFeuoY1b09IpBfVMYdE3sT\n5PSc5jN6xhDsdPD1xmw/R2iMac0aMqtHgBnABlV9vI567YAzgQ+rlLURkciK28AkYO2JBt3SRYUH\nM6xrNIMT2jFlQHxleXhwEKd1b8/n67NsmwdjjN80pI9/NHAVsEZEVnrL7gW6Aqjqs96yi4HPVfVI\nlWM7Ah94PjsIAl5X1U+bIvCW7sWrRyAOz54+VV0yLIHb3lrJt1v3ExUWzM6DRzh/UGc/RWmMaY1s\nP/5mVlxWzhkPf0WP2Ag2ZxeQX1TGwntsT39jzImxK3C1YCFBTi4bkciSHQc5WlJOuVv5cMVef4dl\njGlFLPH7wZWjkujaPpxHLhnI0K5RvLNsNx+t2stDs9bbyl5jzElnV+Dyg85RYcz77VmICIUlbu79\nYA03v7ECgFW7c5lx9Qjahbv8HKUxJlBZi99PvAPenD+4E33iI/n5md156vKhrNqTyx9nrausV1BU\nyoHDxf4K0xgTgKzF72dtQ118etv3a9rWZuTx7/nbOH9QJx6dvZGNWZ6LvJzdJ47fndeX7rER/grV\nGBMgrMXfwvzqrB5EhARx3UtpZOYVcufk3tx8dk+Wbj/IAx+t93d4xpgAYC3+FiYqPJjfTOrNv77e\nygtXpzKgSzsASsrdzJi/nbyjpdb/b4w5Idbib4GuPiOZhfecXZn0AaYO6ESZW/liQ1ajn6/crTZb\nyBhTyRJ/C1Ux+FthUEI7ukSFMXtNZqOeR1W55c0VnP/0gqYMzxhzCrPEf4oQESYPiGf+5v3sP1zM\n/M05jP/r12zI9GyUeqTYc83fsnI3s1bv5bD3/nvLM/h4dSZbsg+zN7fQb/EbY1oO6+M/hVw+MpFX\nFu7krndXk76vgIzcQu56bzXnDezEo5+m88gPBpGRW8jf52xmSGIUV4zsyh8/Wkdi+zB2Hyxk+a5D\ndI4K8/fLMMb4mbX4TyE94yK5Y1Iv5qRnk5FbyPVjurF6Tx4Pz04nIiSI+/63hn/M3UJqUjTr9uZx\n53ur6dkxkteuH0Woy8Hynbk+nzv3aAkfr87kaElZM74iY4w/WIv/FPPTsd1Zn5lPj9gIbj67J4eL\nylCUuyb34dJnF1LmdvPitSPYsDef3YcKuXhoF5wOYVBCFMt21X4BmKMlZVz94hJW7ckjMjSIW85O\n4adju9UYZzDGBAbbnTOAHC0po8yttA2tOd3z0U/TeWH+NtY8cC7BTkfldtGqys/+u4yv0rO4d2pf\nvtt6gK/Ss5kyIJ6nLx9aeREZY0zLZrtztlLhwUG1Jn2AYV2jKS1Xpv59PqMensN3W/YD8Nm6LL7c\nkMU9U/ry07HdmXF1Kr89tzez1+7jk7X7mjN8Y0wzscTfSgxPiibU5aDMrUSEBnHljMX86eP1PDJ7\nAylxEVw7OhnwzB765Zk96NahDS8u2O7foI0xJ4Ul/laifZtgvrt7Al/dcSYf3TSGy0Yk8sKC7ew4\ncJT7zut7TJeOwyFcOzqZlbtzWbaz9nGBg0dKuP3tlWzJLmiul2CMaSKW+FuR9m2CCXI6aBMSxMM/\nGMSHN47mrz8czFm942rUvWRYAm1Dg7j3/TVs33+kxuPPzN3C+8szuPmNlRSX2apgY04llvhbsUEJ\nUVw6PKHWx9qEBPH0FcPIKiji/Kfms2jbAQDcbiUrv4hXFu2kX6e2bMjM56+fbaw8LvdoCec+8Q2v\nLtrZLK/BGNN4lviNT2f2iuWTW8bSKSqM615aynUvLSXld7OZ8Ld5lLuVZ68czlWjkvj3/O3M8I4H\nvJO2h41ZBdw/cx0Ltx44rt9bUFTKXz/bSGaerTQ25mSoN/GLSKKIzBWR9SKyTkRuraXOWSKSJyIr\nvT9/qPLYZBHZKCJbROTupn4B5uTqHBXG6z87jU7tQlm5O5crT+vKOX3juO+8vnSNCef+C/oxZUA8\nD81azxtLdvHKop0MTmhHckw4N7+xnENHShr9O99O28M/5m7hsucWsefQ0ZPwqoxp3eqdxy8inYBO\nqrpcRCKBZcBFqrq+Sp2zgN+o6vnVjnUCm4CJwB5gKXB51WNrY/P4W56SMjci4KplXn9xWTk3/HcZ\n8zblAPD05UNJ6RjB+U8t4MIhnXn8R0Mq6+7NLWTFrlymDoyvsUCstNyNy+nggqcXkFdYSu7REgYn\nRvHK9aed3BdnTABo0nn8qpqpqsu9twuADUCXBsYyEtiiqttUtQR4E5jWwGNNCxIc5Kg16QOEBDl5\n9srhnNatPV3bh3Nu/3j6xLfll2f14P3lGSzY7FkzsGjbAc5/egE3vr6cpTu+ny1UWu7moVnr6X//\nZ/xz7hbWZORx9RnJTOofz+asw83y+oxpTRrVxy8iycBQYHEtD58uIqtEZLaI9PeWdQF2V6mzh4Z/\naJhTSFiwkzdvGMVnt40jOMjztrpxfE86twvln3O3cOBwMde9tJSocBdtQ4N4eeGOymNvfXMFMxZs\np314MI99thGHwAWDO9E5KoysgiJKy91NEuPOA0fILihqkucy5lTW4MQvIhHAe8Btqppf7eHlQJKq\nDgaeBv7X2EBE5AYRSRORtJycnMYebloAESEs2Fl5P9Tl5KrTk1m47QD3vL+GwtJynr8qlctGJPLp\n2n3syysi92gJn67dx8/GdmP2rWPpEx/JlIGdiIsMpXO7UFRhX57vZL1mTx5PfrmJkrL6Pxx++nIa\n932wtkleqzGnsgYlfhFx4Un6r6nq+9UfV9V8VT3svf0J4BKRDkAGkFilaoK3rAZVfV5VU1U1NTY2\ntpEvw7RU00ckEhLk4PP1WUwd2ImecRFcNSoZtyqvLtrJ/M37cStMGdiJ6DbBfHLLWP5+mWdMoGIL\n6cxqiT+7oIh1e/MAePabrTz55WZ+9t80Ckt8rycoKXOzbf8Rlmw/iNvte1xrQ2Y+33q3szAmUDVk\nVo8AM4ANqvq4jzrx3nqIyEjv8x7AM5ibIiLdRCQYmA7MbKrgTcsX3SaYaUM6A3DT+J4AdI0JZ2Lf\njryyaCcfr84kOtzF4IQowLNquGIVcUXi35tbSFm5u/Lykfd/uI7pzy+itNzNip2HSGwfxjebc3hw\n1vdzBkrL3WzJ/n58YNfBo5S7lbzCUrbt9z1u8OBH67n1zRW0xM0LjWkqDWnxjwauAs6uMl1zqoj8\nQkR+4a1zKbBWRFYBTwHT1aMMuAn4DM+g8Nuquu4kvA7Tgt03tR9v/GwUfTu1rSz7+Zk9yCss5dN1\n+xjXKxano+YW0J2jQgHIyC3kDzPXcdE/v+VIcRlzN2ZTUFTGJ2sy2ZtXxLVndOPaM7rx5tJdrM3w\nfBP49/xtnPvkN+w64JkOui3n+2SftuPYbShmLNjO37/c7Pkg2X2I/YdLanzLqM3ibQfIzrcxA3Pq\nacisngWqKqo6SFWHeH8+UdVnVfVZb51/qGp/VR2sqqNU9bsqx3+iqr1UtYeq/ulkvhjTMrULd3F6\nj5hjyoYnRTMiORqAs3rX3rUXHhxEVLiLzLxC5m/OIX1fAX/8aB1FpZ7+/KfmbK58rlvPSaF9eDD3\nz1yH2628t2wP5W7lo9V7ASq3nYgICaqx/9B/vt3OM19vYemOg5XPvcb7AeLLnkNHueKFxfz18+9X\nLWfmFfLYZ+nHjDfsyyuyAWXT4tjKXeM3d0zqTZ/4SMbXsldQhc7twliTkc/ug55VvG+n7SE63MXQ\nrlFszTlCSJCDvp3a0i7MxV2T+7Bs5yH+7+MNbM05gsspfLTq+8TfISKYUd3bH3NBmuz8IvYcKqS4\nzM3jn28CwCGeQeOq8gpLWb/3+zkNL327g3K3snDb96uT//b5Jv45d2vl7wS49qWl/GTGkjrHFYxp\nbpb4jd+M6h7Dp7eNIyo82GedzlFhrNrtuWTkmb083wwm9Yuv/LAYnBBVOX300uEJDE5ox4vfbifY\n6eDms1NI31fA5qwCtuUcoXuHCIYlRbMt5wg5BcUALPd+CDgE0nYeIikmnN7xbVldrcV/7/tr+MG/\nvqWotJyColLeXLqbiJAgdh8sZM+ho+zNLeR/KzzzFv7z3XZUlV0HjrIhM5/0fQV8lZ5NabnbNrQz\nLYIlftOiVfTzOwT+/IOBpCZFc/lpXRnn/RAYmhRVWdfhEB640LOEZHyfWKaPTMQh8P6KDLbtP0K3\nDm2Y0KcjTodw/8y1qCrLdh4i2OnggsGeAejUpPYM6tKOtRl5lQO8W7IP88naTIpK3SzfeYj3lu3h\ncHEZ91/QD4CFWw8wY8F2FLhxfA/WZuSTtvMQX27IAiCmTTCPfprO+L9+zZQn51d+6FRXMXhtzMlm\nid+0aBUze3p1jKRLVBjv/vIMhiRGMahLO26f2Isfj0w6pv7QrtG88JNUfndeP+IiQ5kyoBP/+XY7\n+w8X0y22Db3jI7nz3N58smYf//l2B8t35TIwoR3nD/Ik/hHJ0QxIaMfBIyVk5Hq6l/719VZCghw4\nxLP6eNbqTPrER3LJsATatwnm1cW7eGXhTqYN6cxN41NoF+biL5+m8+nafaTERfDrib3YnH0Yp0PI\nzCviqhmLj5lxBJ7FZan/9yV//mRDM5xV09pZ4jctWkXiH9o16phyh0O4ZUIKXWPCaxxzTr+OJLb3\nlN81uQ9u71hr9w5tAPjZ2O5M6teRhz5ez8rduQxPiubsPnH85ZJBXDS0C4O6tAPg4U/SeWDmOj5Y\nsYcrRiYxsEs7Pl6TybJdh5gyoBMOhzCqe3tW7c4lNjKE35/Xj7BgJw9O68/SHYdYsuMg5/TryOUj\nu/L8VcOZfetYnv/JcHYcOMI5j8/jgZnfT3B77LONHC4u4/lvtnHT68sZeP9nvL10Ny3F1pzDXP78\nIvYfrv3bijm1WOI3LVoXb+J25C2kAAAS6klEQVQfkhhVT83adY0Jr7ysZErHSMDzofHU5UMZ07MD\n5W5lWNdonA7hRyMSCXU5GdilHTeM686XG7J4eeEOrjitK3dM6sWo7jFszTmCKkwZGA/AhD4dCXU5\n+NeVw4hu4xmrmDakCzeN74lDYMqAeJwOYVL/eMKDgxibEsuCu85m6sB4Xlm0k/yiUlbtzmXW6kx+\neVYPRveMYdbqTIrL3Hyz+fsV7K8u2smkJ+b57CY6GfKLSnl76W7K3cpL3+5g4bYDfLbOrsMcCOrd\nndMfbHdOU8HtVt5YuotLhiUQ6nLWf0AtSsrcrNh1iNO6HzultLCknK83ZnNu/3gctawjOHC4mKMl\n5ZXfHuamZ3PtS0vp3qENc+44ExFBVSkqdR+zVQWAqpJzuJi4yNBaY1q87QCXPb+IZ68cxvvLM1i6\n4yDf3Dkel9NBTkExD81az5acw3x1x1l8lZ7FT19Ow63w49O68qeLB9b4XU98uRmnCDef3ROHQygo\nKuXVRbu4clRXIkNddZ6fotJysvKLSIppc0z5P+du4bHPNvLQtP789fNN5BWWck7fOF64ekSdz2f8\no0l35zTGnxwO4cenJR130gfPzqLVkz54NpabMrBTrUkfICYipDLpA6QmRxPmcnL+oE6VW0pX35+o\ngoj4TPoAw5KiiQwJ4u20PcxJz+ZHIxKJDHUR6nKS2D6cfp3bsn3/EfKOlvLrt1bRt1NbLktN5I0l\n3y9SA0/S/+NH63lqzmae+HITv3ptORv3FfCz/6bx6KfpvLtsD/sPF3P6w3OYWWWaaYXCknJ+MmMJ\nE/42r8YlNj/3tu4f+Gg9eYWl9O4YybdbDtggdACwxG9MA0WGuvji9nHcdHbKCT+Xy+lgTEoHvkrP\nptytTB/R9ZjH+3Vqiyq8ungneYWl3Dohhbum9CEiJIjzn17A1L/PJzu/iHfS9vDSdzu4fkw3fn9+\nPz5fv49zn/yGRdsOEhESxLdbDvBVejaZeUXc9/6aygvbuN3Kwq0HuP7lpSzdeRCHCE9/tbny9+/L\nK2LVnjzO7d+RcrcS3zaUOyf3prC0nMXbD9Z4PW8u2cU/qhzfGEeKyyq37vaX1jbNNsjfARhzKkmI\nrjmYfLzO6h3L7LX7OL17DN06HNvN0t87wDxjgWdNwuieHWgTEsRHN4/h07X7ePyLTdz53mrWZuQz\nPCma353XFxFh6sB4Zq/ZR1zbEL7dsp9ZqzJxiGdKaXGZm9veXMmrPz2Nez9Yw/vLMwgPdvLwxQPZ\nmnOYGQu2k3e0lDUZeZVrJn57bm+GJEbTJTqM0T07EOpyMDc9mzN7xfLw7A2s35vP+N5xPDhrPSJw\n/qDOOB3Cp2v3EepyMGVgJzpEhPg8B+Vu5VevLWfephzeumFUrd/MfMnILWTZzkOkxEXQJz6yxoV9\nGip9Xz4XPL2AD341mgHe8x7oLPEb4yfj+8QRFe7ip2O71Xisc7tQ2oW5OHikhLEpnqQPkBTThp+f\n2QOAh2enA/DiNamVSa9TuzCuG+N5PlV4Y8luvtiQxfQRiYzu2YGb31jBpCe+YdfBo/zizB7cOiGF\nsGAnOQXFvLZ4F0t2HKRtqIt3lu2he4c29IiNoOdZkZVxndUrjpmr9nLDuO78Z8EOSsrdzN+8n36d\n2rIpq4DnvtnGwq372eHdI+mZr7fyzI+HMbRrdK3n4C+fpjNvUw4OgQ9X7WVkt/Z8smYff/t8IyO7\nteeRSwZ5BsELS7nRu8mf57Upt76xgjTv9htn9Ijh0UsGHdM1V1BUSpjLWbnpny9Ltx+ktFz5but+\nS/zGmJMrLjKUlX+YVOtjIkK/Tm1ZuO0AZ/epuaXF9WO6sXj7QVLiIhiUUPuMpzO8+yOpwvjecUzq\nH8/hojLufn8NUwfGc9fk3pUfGLGRIXz+63FEhrooK3fzm3dWMbFfzctjXn1GMp+u28cNr6RRUu7m\n1etPY9nOQ1x+WiL/N2sDbyzZBcDL142kXZiLG19bzsXPfEdSTDgPThtQ+U0CICu/iBcWbOdHqQkU\nlbqZvSaT5Jhw/vxJOi6nsP9wMX+6eCDPfr2VrPwifpSaSGyk59vDgi37Sdt5iFsneNZNPP7FJqb9\n81s+//U4OkSEUFLmZsLf5jF9RCK3T+p9zGsoKi3H5XRUbgy4PrMAgFW7fe/PNGdDFr//31oeumgA\nE/p2rPH49v1HeHPpLn4zqbfPK9VVOFJcRrkqbesZdD+ZrI/fmBaqX2fPbqa17WUU5HTw4jUjuGdq\nX5/Hx0SE0Cc+srKrCGD6yK7MueNM/j59aI2knhAdTrswFzERIfzn2pFccVrXGs85qnt7+nZqy9qM\nfFKTohmT0oFbz0khLjKUa7zTZq8+PYkze8UyJDGKWTeP4YEL+uFyOrj59eWVi+IA3liyi3K3cuP4\nnlw4uDOHjpby50/SOadvHH++eCD5RWV8uSGLjNxCytzK22medQ2qyhNfbKJzu1B+Nb4H143pxru/\nPJ2ColL+/LFnAdzi7QfILihmTno24EncGzLzKS13M+0f33Luk99ULqLbkOnZg2mld2uQqtxu5fEv\nNnH9y2nszSs6Zjrre8v2MPHxeRSXlfPqop08N28bry7a6fPvUeHWN1fyo2cX+nX/JmvxG9NCXTs6\nmd4dI0mu1v/fGLdMSGHPoaOVXUUAPWIjjvv5RITrx3TjN++sqvHBMKxrNF/ePo5uHb5//ug2wVwz\nuhvj+8Rx3lMLuGrGYgYnRDGmZwfeXLKbM3vFkhTThk7twmgX5sLldPDoJYMo9M4cqtiBtWv7cF5f\nvItfnNmDD1dmsHxXLn+6eAAhQZ4ZVX3i2/KLM3vw9FdbuGR4Al+s92yXsT4zn90Hj/LL15YT0yaY\nq05PYmNWAREhQUz7xwJm3zqOjfsKCHM5ycgtZFNWAY9/vom7pvQhITqMn7+yjK/Ss7l0eAJ7cwsr\nPxzcbuXprzaz48BRFm87yHdbPZv1PfnlZi4a0qVyTUd1peVuvtu6n6Ml5czfsv+Yb0DNyebxG2Ma\nxe1W5m/Zz9ieHXxOha3NF+uz+OtnG8ktLCEr37MQ7d8/SWViP0/XydIdB4kMDaJPvOebzuhHviIj\nt5CYNsE8OG0AN76+nIuHdmHOhix6dYzkrZ+ffsx1HIpKy5n85DcAFJe5EWBvXhHn9I3jyw3ZlfVG\nJrfnL5cOYvzfvmba4M78b+Vefjg8gXeW7aF3x0g2ZhXwg2FdOL17DL99dzW/P78f141O5u9zNvP3\nOZtZff8k0nYc4tqXlgIwbUhnPly5lwsHd2bW6r1069CGif3iKSot55y+HRmT0oEPV2YQ7HQQ3y6U\ni5/x7Fo/rlcst07oSbkbRnZrf/x/EK/GzOO3Fr8xplEcDjmulurEfh2Z2K8jqspn67JYtzfvmPGL\nEcnHJr+R3drzwYoMRnWPYfKAeH4+rjsvLNiOyyk89sPBNS7eE+py8n8XDeTKGYsB+PPFA3lo1nq+\n3JBNUkw4lw5L4Mk5m7lrSh+SO7ThjB4x/G+lZ23DpcMTeH9FBhuzCggPdvLRqr0s2X6Q/p3bct3o\nZESEIYlRqHq27H7pux3ERYbQOz6SD73Pcc3oZCb178iLC7bz7LytOB3CnPQsPrllLPd9sJYgp3Dt\nGZ6B96tPT+LlhTv5ZlMOQQ7hpWtHMialQ6PP6fGyPn5jTLMSESYPiOeOSb1rvfJahYpW8KgeMTgd\nwj1T+/LZbWN59xdn1Jj+WmFMSgcuHtoFl1M4t39HUr0X+5k2uDM3T0hh6X3nMDzJU3bx0ATAs/Pr\n4MQoenX0jIe8eM0Iyt3KnkOF/OqsnpVjIRXbhrz47XbmbcrhqlFJTPJ+W4kICWJQF89mf+//ajRb\n/zyVJy4bwu6Dhdz93hoOF5eRe7SU577ZSvfYNvx6Yi+mDIjn/gv60TMugl++uow5G7Ka7ZKf1uI3\nxrRIk/p15Nst+5kyIL6yrGdcZB1HeDxyyUBuHN+DmIgQxqZ0YP7m/Vw4pAsA7av0vU8eEM/v/reG\nhOhwQl1O7pjYi/yiUkZ1j+GioV1YvzefyVV+d1R4MMkx4Xy5IZvO7UK5fmw3Dh4pgQ/XcVq39sdM\nG3U6PB88MW2C+XhNJr06RuBWzxbfp3WLISo8mH9dOdzzOvvHc9ULi7n+5TRGJEfz3+tOq3U1eFOy\nxG+MaZFiIkL4xxXDGn1cSJCz8gPiJ6cnM6p7DD3jag5oR4QEcdfkPoR7k+w5/b6fpvnYpYMpd2uN\nbyRDEqPYceAovz+/H+HBQYQHB3HLhJTKqbPV4/hhaiLPztvKFSO7IiLcP3Mdo7of26XVJSqMz349\njreW7mbd3vyTnvTBEr8xJoCFupw+1zkAXDu65uI58LTYa+uGun5Md3rHtz3mm8DtE3v5fP7rx3Sj\nuKycS1MTCXIIblXO7R9fo57L6eDKUUm1PMPJYbN6jDEmADTp7pwikigic0VkvYisE5Fba6nzYxFZ\nLSJrROQ7ERlc5bEd3vKVImLZ3Bhj/KwhXT1lwB2qulxEIoFlIvKFqq6vUmc7cKaqHhKRKcDzwGlV\nHh+vqv7dfs8YYwzQgMSvqplApvd2gYhsALoA66vU+a7KIYuAhCaO0xhjTBNp1Dx+EUkGhgKL66h2\nPTC7yn0FPheRZSJyQ2MDNMYY07QaPKtHRCKA94DbVDXfR53xeBL/mCrFY1Q1Q0TigC9EJF1Vv6nl\n2BuAGwC6dq25OZQxxpim0aAWv4i48CT911T1fR91BgEvANNU9UBFuapmeP/NBj4ARtZ2vKo+r6qp\nqpoaG+ufjYuMMaY1aMisHgFmABtU9XEfdboC7wNXqeqmKuVtvAPCiEgbYBKwtikCN8YYc3wa0tUz\nGrgKWCMiK71l9wJdAVT1WeAPQAzwjHdfizLvfNKOwAfesiDgdVX9tElfgTHGmEZpkQu4RCQHqP+K\nBrXrALTEqaMWV+O11NgsrsaxuBrveGJLUtUG9ZO3yMR/IkQkraGr15qTxdV4LTU2i6txLK7GO9mx\n2bbMxhjTyljiN8aYViYQE//z/g7AB4ur8VpqbBZX41hcjXdSYwu4Pn5jjDF1C8QWvzHGmDoETOIX\nkckislFEtojI3X6Mo9ZtrEXkARHJ8G5PvVJEpvopvhrbZItIexH5QkQ2e/+NbuaYelc5LytFJF9E\nbvPHORORF0UkW0TWVimr9fyIx1Pe99xqEWn85aJOPLbHRCTd+/s/EJEob3myiBRWOXfPNnNcPv92\nInKP95xtFJFzmzmut6rEtKNibVIzny9fOaL53meqesr/AE5gK9AdCAZWAf38FEsnYJj3diSwCegH\nPAD8pgWcqx1Ah2plfwHu9t6+G3jUz3/LfUCSP84ZMA4YBqyt7/wAU/FsSCjAKGCxH2KbBAR5bz9a\nJbbkqvX8EFetfzvv/4VVQAjQzfv/1tlccVV7/G/AH/xwvnzliGZ7nwVKi38ksEVVt6lqCfAmMM0f\ngahqpqou994uACq2sW7JpgEve2+/DFzkx1gmAFtV9XgX8J0Q9WwgeLBasa/zMw34r3osAqJEpFNz\nxqaqn6tqmfeuX7ZE93HOfJkGvKmqxaq6HdiCj/27TmZc3q1ofgS8cTJ+d13qyBHN9j4LlMTfBdhd\n5f4eWkCylZrbWN/k/ar2YnN3p1RR2zbZHdVz3QXwtLY71n5os5jOsf8ZW8I583V+Wtr77jqO3RK9\nm4isEJF5IjLWD/HU9rdrKedsLJClqpurlDX7+aqWI5rtfRYoib/FkZrbWP8L6AEMwXNhm7/5KbQx\nqjoMmALcKCLjqj6onu+WfpnqJSLBwIXAO96ilnLOKvnz/NRFRO7Dc7W817xFmUBXVR0K3A68LiJt\nmzGkFve3q+Zyjm1gNPv5qiVHVDrZ77NASfwZQGKV+wneMr+QWraxVtUsVS1XVTfwb07S19v6aO3b\nZGdVfHX0/pvtj9jwfBgtV9Usb4wt4pzh+/y0iPediFwDnA/82Jsw8HalHPDeXoanL71Xc8VUx9/O\n7+dMRIKAHwBvVZQ19/mqLUfQjO+zQEn8S4EUEenmbTVOB2b6IxBv32GNbayr9cldjB+2pxbf22TP\nBK72Vrsa+LC5Y/M6phXWEs6Zl6/zMxP4iXfWxSggr8pX9WYhIpOBO4ELVfVolfJYEXF6b3cHUoBt\nzRiXr7/dTGC6iISISDdvXEuaKy6vc4B0Vd1TUdCc58tXjqA532fNMYrdHD94Rr434fmkvs+PcYzB\n8xVtNbDS+zMVeAVY4y2fCXTyQ2zd8cyoWAWsqzhPeLbUngNsBr4E2vshtjbAAaBdlbJmP2d4Pngy\ngVI8fanX+zo/eGZZ/NP7nlsDpPohti14+n8r3mvPeute4v0brwSWAxc0c1w+/3bAfd5zthGY0pxx\nectfAn5RrW5zni9fOaLZ3me2ctcYY1qZQOnqMcYY00CW+I0xppWxxG+MMa2MJX5jjGllLPEbY0wr\nY4nfGGNaGUv8xhjTyljiN8aYVub/AW9332SkcPPLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIBNtcmeG56c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "ae309595-b688-4510-866c-2eaf83c535f2"
      },
      "source": [
        "max_length = 20\n",
        "\n",
        "def sample(category, start_letter='A'):\n",
        "  with torch.no_grad(): #history 추적 안해도 됨\n",
        "    category_tensor = categoryTensor(category)\n",
        "    input = inputTensor(start_letter)\n",
        "    hidden = rnn.initHidden()\n",
        "    output_name = start_letter\n",
        "    \n",
        "    for i in range(max_length):\n",
        "      output, hidden = rnn(category_tensor, input[0], hidden)\n",
        "      topv, topi = output.topk(1)\n",
        "      topi = topi[0][0]\n",
        "      if topi == n_letters-1:\n",
        "        break;\n",
        "      else:\n",
        "        letter = all_letters[topi]\n",
        "        output_name += letter\n",
        "      input = inputTensor(letter)\n",
        "    return output_name\n",
        "  \n",
        "#한 카테고리와 여러 시작 문자들에서 샘플 얻기\n",
        "def samples(category, start_letters='ABC'):\n",
        "  for start_letter in start_letters:\n",
        "    print(sample(category, start_letter))\n",
        "    \n",
        "samples('Russian', 'RUS')\n",
        "samples('German', 'GER')\n",
        "samples('Spanish', 'SPA')\n",
        "samples('Chinese', 'CHI')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Roveri\n",
            "Uarisher\n",
            "Shavavav\n",
            "Gerter\n",
            "Eonter\n",
            "Roste\n",
            "Sara\n",
            "Paner\n",
            "Artaras\n",
            "Cha\n",
            "Han\n",
            "Iu\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}