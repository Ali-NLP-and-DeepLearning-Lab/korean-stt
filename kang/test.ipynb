{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joovvhan/korean-stt/blob/master/kang/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gbeFIm5hMQE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61e23656-cc54-42b5-97d7-44739d32fb52"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
        "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
        "x = np.random.randn(N, D_in)\n",
        "y = np.random.randn(N, D_out)\n",
        "\n",
        "# 무작위로 가중치를 초기화합니다.\n",
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # 순전파 단계: 예측값 y를 계산합니다.\n",
        "    h = x.dot(w1)\n",
        "    h_relu = np.maximum(h, 0)\n",
        "    y_pred = h_relu.dot(w2)\n",
        "\n",
        "    # 손실(loss)을 계산하고 출력합니다.\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    print(t, loss)\n",
        "\n",
        "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "    grad_h = grad_h_relu.copy()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.T.dot(grad_h)\n",
        "\n",
        "    # 가중치를 갱신합니다.\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 41832335.30509984\n",
            "1 36659546.10710899\n",
            "2 31464385.59063548\n",
            "3 23284578.956471503\n",
            "4 14574993.27878956\n",
            "5 8225704.577213904\n",
            "6 4637565.87903685\n",
            "7 2839556.013365996\n",
            "8 1934737.9324014485\n",
            "9 1439285.113040689\n",
            "10 1134150.0683799302\n",
            "11 924705.3514179615\n",
            "12 769458.2728699917\n",
            "13 648460.3534682828\n",
            "14 551482.3706505819\n",
            "15 472262.89832708926\n",
            "16 406720.3413523219\n",
            "17 352024.64369796857\n",
            "18 306014.89563696296\n",
            "19 267060.41010983137\n",
            "20 233892.26639135875\n",
            "21 205500.68140246323\n",
            "22 181084.54202584352\n",
            "23 159995.88580379676\n",
            "24 141718.6599335046\n",
            "25 125836.14994598397\n",
            "26 111979.32524611353\n",
            "27 99842.69733852013\n",
            "28 89184.48242688786\n",
            "29 79794.5287836997\n",
            "30 71515.00021266015\n",
            "31 64199.99417566564\n",
            "32 57711.4155954245\n",
            "33 51953.32790200056\n",
            "34 46830.7061399202\n",
            "35 42263.85525856863\n",
            "36 38181.03227405987\n",
            "37 34530.422433641696\n",
            "38 31262.611078694397\n",
            "39 28335.9421720134\n",
            "40 25707.583682197124\n",
            "41 23345.65447278812\n",
            "42 21223.5457415115\n",
            "43 19308.73218150901\n",
            "44 17580.146817755347\n",
            "45 16018.797211841971\n",
            "46 14606.608431779472\n",
            "47 13327.879834612748\n",
            "48 12171.675121238335\n",
            "49 11123.482138539484\n",
            "50 10171.506727378424\n",
            "51 9306.35178743589\n",
            "52 8519.649661575593\n",
            "53 7803.999552183215\n",
            "54 7152.254300696846\n",
            "55 6558.242084940061\n",
            "56 6016.88892170506\n",
            "57 5522.574459090995\n",
            "58 5071.471949000141\n",
            "59 4659.546134727768\n",
            "60 4282.97224071203\n",
            "61 3938.6219937040223\n",
            "62 3623.7510731465195\n",
            "63 3335.5566378596377\n",
            "64 3071.546867341348\n",
            "65 2829.5109192016976\n",
            "66 2607.4545425654064\n",
            "67 2403.7500554940534\n",
            "68 2216.8111041948746\n",
            "69 2045.1389864000482\n",
            "70 1887.3679316410646\n",
            "71 1742.4782621550726\n",
            "72 1609.259236390796\n",
            "73 1486.6753902723742\n",
            "74 1373.8684393158408\n",
            "75 1270.0314192730052\n",
            "76 1174.3959622725538\n",
            "77 1086.2948264285362\n",
            "78 1005.1037719256487\n",
            "79 930.2687963011587\n",
            "80 861.2462331467473\n",
            "81 797.6194228830451\n",
            "82 738.8512705799626\n",
            "83 684.5938369028515\n",
            "84 634.5119778564592\n",
            "85 588.2263802281991\n",
            "86 545.4550704530789\n",
            "87 505.9357945417829\n",
            "88 469.4093256327245\n",
            "89 435.6249267189556\n",
            "90 404.3622679984751\n",
            "91 375.4530540604433\n",
            "92 348.68757613288045\n",
            "93 323.8956540092656\n",
            "94 300.9345943050672\n",
            "95 279.6569776091453\n",
            "96 259.94123857598305\n",
            "97 241.67609797620383\n",
            "98 224.7411126238709\n",
            "99 209.0346605013283\n",
            "100 194.46933152051173\n",
            "101 180.95358451971788\n",
            "102 168.41673855653113\n",
            "103 156.7785313873747\n",
            "104 145.96749340370332\n",
            "105 135.92724271591285\n",
            "106 126.60534355725218\n",
            "107 117.94400222676624\n",
            "108 109.89630582682223\n",
            "109 102.41581996902937\n",
            "110 95.46160920409434\n",
            "111 88.99354270730527\n",
            "112 82.9787804112554\n",
            "113 77.38362806209737\n",
            "114 72.17937256435346\n",
            "115 67.33792791311916\n",
            "116 62.83226769150936\n",
            "117 58.63449067085035\n",
            "118 54.72610955726982\n",
            "119 51.08576050616463\n",
            "120 47.694916122488436\n",
            "121 44.53676128135887\n",
            "122 41.594524923684986\n",
            "123 38.85107395005684\n",
            "124 36.293598369622856\n",
            "125 33.91039055687508\n",
            "126 31.688594542834487\n",
            "127 29.61532559078834\n",
            "128 27.682084196964794\n",
            "129 25.878182249875863\n",
            "130 24.19686093276588\n",
            "131 22.627840672352264\n",
            "132 21.161698013335528\n",
            "133 19.79353827699975\n",
            "134 18.51651817731609\n",
            "135 17.323819976444877\n",
            "136 16.209830813738666\n",
            "137 15.16915500029003\n",
            "138 14.197314002647587\n",
            "139 13.289147118457482\n",
            "140 12.440549189911557\n",
            "141 11.647570184157587\n",
            "142 10.90632121668676\n",
            "143 10.213421806048014\n",
            "144 9.565755706796686\n",
            "145 8.960153625629339\n",
            "146 8.393513484577067\n",
            "147 7.863645759630693\n",
            "148 7.3686237090818345\n",
            "149 6.905222660704274\n",
            "150 6.471349232039014\n",
            "151 6.065469554974115\n",
            "152 5.68558875985591\n",
            "153 5.330040275872348\n",
            "154 4.997147569232708\n",
            "155 4.685647968923106\n",
            "156 4.393815151262281\n",
            "157 4.120622092131363\n",
            "158 3.8648556673798113\n",
            "159 3.625249181342768\n",
            "160 3.4007217597334813\n",
            "161 3.1905131647300147\n",
            "162 2.99353991700146\n",
            "163 2.8088969258516245\n",
            "164 2.635875978841591\n",
            "165 2.4737313785789445\n",
            "166 2.321846777205656\n",
            "167 2.1794402067405656\n",
            "168 2.0458463219139507\n",
            "169 1.920783252385232\n",
            "170 1.8034293913251118\n",
            "171 1.693328724421507\n",
            "172 1.590039621858808\n",
            "173 1.4931975429965179\n",
            "174 1.4024004529263652\n",
            "175 1.3171749980071854\n",
            "176 1.2372048829587605\n",
            "177 1.1622235090173703\n",
            "178 1.0918489426679967\n",
            "179 1.0257777145859812\n",
            "180 0.9637922749798202\n",
            "181 0.9056435637194113\n",
            "182 0.8510351894823872\n",
            "183 0.7997560385977955\n",
            "184 0.7516253739269355\n",
            "185 0.7064622475967532\n",
            "186 0.6640200781621906\n",
            "187 0.6241790824213544\n",
            "188 0.5867897643209405\n",
            "189 0.5516616609679035\n",
            "190 0.5186646965278845\n",
            "191 0.487669494047169\n",
            "192 0.45856489141611456\n",
            "193 0.4312142334099317\n",
            "194 0.4055303677759851\n",
            "195 0.3814055640759997\n",
            "196 0.3587272461149845\n",
            "197 0.3374055864809482\n",
            "198 0.3173698122940576\n",
            "199 0.2985487439182699\n",
            "200 0.2808566396571367\n",
            "201 0.26422415195689436\n",
            "202 0.24858820960005865\n",
            "203 0.23389494265632588\n",
            "204 0.22007716305414818\n",
            "205 0.207085701339176\n",
            "206 0.19487643058299634\n",
            "207 0.18339219299559414\n",
            "208 0.17259214510460008\n",
            "209 0.16243925136091708\n",
            "210 0.15289136072041035\n",
            "211 0.14391245647254253\n",
            "212 0.13546584748474913\n",
            "213 0.1275241520750497\n",
            "214 0.12004845943407366\n",
            "215 0.11301486795623891\n",
            "216 0.10640388495394042\n",
            "217 0.10018004084135512\n",
            "218 0.09432333124962658\n",
            "219 0.08881598897055523\n",
            "220 0.0836333069696326\n",
            "221 0.0787543423949027\n",
            "222 0.07416391308278232\n",
            "223 0.06984257281173659\n",
            "224 0.06577392381206679\n",
            "225 0.06195169775991413\n",
            "226 0.05834914637305707\n",
            "227 0.0549567581514879\n",
            "228 0.051764536461621935\n",
            "229 0.04876033245031866\n",
            "230 0.04593078106579244\n",
            "231 0.043267462528737734\n",
            "232 0.04076068274220453\n",
            "233 0.038400642709606914\n",
            "234 0.03617831460115494\n",
            "235 0.03408582799971287\n",
            "236 0.032115400391861795\n",
            "237 0.03025974521847046\n",
            "238 0.028512420676070224\n",
            "239 0.02686761912203411\n",
            "240 0.02531789109490515\n",
            "241 0.023858498650795562\n",
            "242 0.022484340408958255\n",
            "243 0.021189255493009387\n",
            "244 0.019969558282163156\n",
            "245 0.018821632269850334\n",
            "246 0.017739397095864744\n",
            "247 0.016719986737117554\n",
            "248 0.015760232384970183\n",
            "249 0.014855270204907794\n",
            "250 0.014002798801496867\n",
            "251 0.013200220899121085\n",
            "252 0.012443402110616633\n",
            "253 0.011730407329099064\n",
            "254 0.011058971306577356\n",
            "255 0.01042581089406101\n",
            "256 0.009829386033282487\n",
            "257 0.0092676875840246\n",
            "258 0.00873798703044689\n",
            "259 0.008238793109417875\n",
            "260 0.007769001977465622\n",
            "261 0.007325550920892494\n",
            "262 0.00690758874499967\n",
            "263 0.0065139645368605\n",
            "264 0.0061425928907343115\n",
            "265 0.005792554839611341\n",
            "266 0.0054627756108840825\n",
            "267 0.005151759156082874\n",
            "268 0.004858576162388905\n",
            "269 0.0045823040900926214\n",
            "270 0.004321782116751132\n",
            "271 0.004076174886620558\n",
            "272 0.0038446794907436572\n",
            "273 0.003626314216951103\n",
            "274 0.003420439192877029\n",
            "275 0.0032264362261095634\n",
            "276 0.0030434209962447107\n",
            "277 0.0028708609856835567\n",
            "278 0.00270818484547538\n",
            "279 0.0025547099231149227\n",
            "280 0.0024100553251653607\n",
            "281 0.0022736253035271882\n",
            "282 0.0021449369773440174\n",
            "283 0.0020236249943276136\n",
            "284 0.0019091837274859617\n",
            "285 0.0018012648706995655\n",
            "286 0.0016995311756902748\n",
            "287 0.0016034974444091897\n",
            "288 0.0015129546014044696\n",
            "289 0.001427583948671359\n",
            "290 0.001346998752149299\n",
            "291 0.001271017510121637\n",
            "292 0.0011993602730320805\n",
            "293 0.0011317245548355115\n",
            "294 0.0010679755817451985\n",
            "295 0.001007816172084443\n",
            "296 0.000951043877552555\n",
            "297 0.0008975220842786771\n",
            "298 0.000846996276456143\n",
            "299 0.0007993670239535974\n",
            "300 0.0007544589573788127\n",
            "301 0.0007120259510157499\n",
            "302 0.0006720076466022269\n",
            "303 0.0006342563503552594\n",
            "304 0.0005986204530767422\n",
            "305 0.0005650151898941081\n",
            "306 0.0005332960652789138\n",
            "307 0.0005033597463411492\n",
            "308 0.00047512818362664954\n",
            "309 0.00044847208527803246\n",
            "310 0.00042332383050376935\n",
            "311 0.0003995988505152443\n",
            "312 0.00037719913882175123\n",
            "313 0.0003560725347914916\n",
            "314 0.0003361320625210975\n",
            "315 0.00031731118456189606\n",
            "316 0.0002995536278905901\n",
            "317 0.00028279218459125637\n",
            "318 0.00026697367632163535\n",
            "319 0.00025204692558239834\n",
            "320 0.0002379498040391768\n",
            "321 0.0002246515313464359\n",
            "322 0.00021209757887368644\n",
            "323 0.0002002457851107078\n",
            "324 0.00018906541096167938\n",
            "325 0.00017850587466808882\n",
            "326 0.00016854000408829205\n",
            "327 0.00015913551253930904\n",
            "328 0.00015025440682835605\n",
            "329 0.00014187412195367324\n",
            "330 0.00013396334490300266\n",
            "331 0.00012649446381471355\n",
            "332 0.00011944630305103473\n",
            "333 0.00011278930674821336\n",
            "334 0.00010650485687581853\n",
            "335 0.00010057518903308724\n",
            "336 9.497351275311675e-05\n",
            "337 8.968721460705567e-05\n",
            "338 8.469547188529542e-05\n",
            "339 7.998114594368847e-05\n",
            "340 7.553404510962238e-05\n",
            "341 7.13320299114009e-05\n",
            "342 6.736769443757862e-05\n",
            "343 6.36260986043317e-05\n",
            "344 6.008848508214747e-05\n",
            "345 5.6750861596234785e-05\n",
            "346 5.359747347727821e-05\n",
            "347 5.0620667611307355e-05\n",
            "348 4.7811167736229766e-05\n",
            "349 4.5157026014339935e-05\n",
            "350 4.265122779548941e-05\n",
            "351 4.0284478979869814e-05\n",
            "352 3.8049369528891225e-05\n",
            "353 3.593961942134314e-05\n",
            "354 3.3946382584326636e-05\n",
            "355 3.206454162814909e-05\n",
            "356 3.028755075691602e-05\n",
            "357 2.8608893975399653e-05\n",
            "358 2.7024317312077822e-05\n",
            "359 2.5527490404054088e-05\n",
            "360 2.4113734548570743e-05\n",
            "361 2.277875973791862e-05\n",
            "362 2.1517593205726772e-05\n",
            "363 2.0327012723308813e-05\n",
            "364 1.9202233755919027e-05\n",
            "365 1.8140130594820005e-05\n",
            "366 1.7137034930914483e-05\n",
            "367 1.6189293964612354e-05\n",
            "368 1.5294627002669053e-05\n",
            "369 1.444912790418731e-05\n",
            "370 1.365103093496189e-05\n",
            "371 1.289699614445962e-05\n",
            "372 1.2184357603899411e-05\n",
            "373 1.151159625881653e-05\n",
            "374 1.0875840827873516e-05\n",
            "375 1.0275438424792753e-05\n",
            "376 9.708450799862973e-06\n",
            "377 9.172637489773939e-06\n",
            "378 8.666628540006192e-06\n",
            "379 8.188627379779508e-06\n",
            "380 7.736949732287872e-06\n",
            "381 7.31042852751371e-06\n",
            "382 6.9073562430327394e-06\n",
            "383 6.526720625024217e-06\n",
            "384 6.16704436728576e-06\n",
            "385 5.827229875061282e-06\n",
            "386 5.5062896897654115e-06\n",
            "387 5.202991003639111e-06\n",
            "388 4.916706135743479e-06\n",
            "389 4.646112169570413e-06\n",
            "390 4.390374594479176e-06\n",
            "391 4.148762543515214e-06\n",
            "392 3.92048626388075e-06\n",
            "393 3.7048605403071484e-06\n",
            "394 3.5011127933735775e-06\n",
            "395 3.3085471267179987e-06\n",
            "396 3.1266943720153537e-06\n",
            "397 2.9547524480552e-06\n",
            "398 2.792377799730124e-06\n",
            "399 2.638941807196569e-06\n",
            "400 2.4939640752534026e-06\n",
            "401 2.3569919328294625e-06\n",
            "402 2.2275269603169807e-06\n",
            "403 2.1052311005457877e-06\n",
            "404 1.9896378442928553e-06\n",
            "405 1.8804389411776105e-06\n",
            "406 1.7772433611652555e-06\n",
            "407 1.6797128845764558e-06\n",
            "408 1.587590863463904e-06\n",
            "409 1.5005120165653124e-06\n",
            "410 1.4182242752932389e-06\n",
            "411 1.3404945993769327e-06\n",
            "412 1.2669770675612328e-06\n",
            "413 1.1975570569050202e-06\n",
            "414 1.1319142419714731e-06\n",
            "415 1.0698907525857974e-06\n",
            "416 1.0112807889515053e-06\n",
            "417 9.559016290714374e-07\n",
            "418 9.035606340580431e-07\n",
            "419 8.540831996751903e-07\n",
            "420 8.073308275660518e-07\n",
            "421 7.631695424815658e-07\n",
            "422 7.214010757901063e-07\n",
            "423 6.81947537561903e-07\n",
            "424 6.446284120273945e-07\n",
            "425 6.093770212346758e-07\n",
            "426 5.760558040533727e-07\n",
            "427 5.445537306832896e-07\n",
            "428 5.147940748172566e-07\n",
            "429 4.866554398163652e-07\n",
            "430 4.600578465798553e-07\n",
            "431 4.34920565686255e-07\n",
            "432 4.1115715231803565e-07\n",
            "433 3.887088726116937e-07\n",
            "434 3.6747970234003204e-07\n",
            "435 3.474335820639764e-07\n",
            "436 3.284692778210567e-07\n",
            "437 3.105379924371013e-07\n",
            "438 2.935942921558603e-07\n",
            "439 2.775727363946811e-07\n",
            "440 2.6243088393476537e-07\n",
            "441 2.481203818861568e-07\n",
            "442 2.345853314915018e-07\n",
            "443 2.217967786871372e-07\n",
            "444 2.097013509573382e-07\n",
            "445 1.9827374363946955e-07\n",
            "446 1.874680942701151e-07\n",
            "447 1.7725140441574742e-07\n",
            "448 1.675982751195851e-07\n",
            "449 1.5846829219854718e-07\n",
            "450 1.4983665721696836e-07\n",
            "451 1.4168027555659665e-07\n",
            "452 1.3396431992378378e-07\n",
            "453 1.2667350824441663e-07\n",
            "454 1.1977738683711546e-07\n",
            "455 1.1326059193253218e-07\n",
            "456 1.0709808265676206e-07\n",
            "457 1.0127327184676069e-07\n",
            "458 9.576558147891917e-08\n",
            "459 9.05568713054288e-08\n",
            "460 8.563287459820154e-08\n",
            "461 8.09782199409986e-08\n",
            "462 7.657629931461978e-08\n",
            "463 7.241704090877759e-08\n",
            "464 6.848180345193434e-08\n",
            "465 6.476179590084689e-08\n",
            "466 6.124360435774185e-08\n",
            "467 5.791718260016718e-08\n",
            "468 5.4774158006285886e-08\n",
            "469 5.180054911113057e-08\n",
            "470 4.8988888801294115e-08\n",
            "471 4.633024735252377e-08\n",
            "472 4.381590548638315e-08\n",
            "473 4.1439261381141075e-08\n",
            "474 3.919139967865749e-08\n",
            "475 3.7066027000786915e-08\n",
            "476 3.505612476098686e-08\n",
            "477 3.315499187208221e-08\n",
            "478 3.135808667698538e-08\n",
            "479 2.9658103161376644e-08\n",
            "480 2.8052383097029035e-08\n",
            "481 2.653232829687075e-08\n",
            "482 2.5094735649922245e-08\n",
            "483 2.373562784079575e-08\n",
            "484 2.2450125782894695e-08\n",
            "485 2.123447664525098e-08\n",
            "486 2.008510059228684e-08\n",
            "487 1.89977304271701e-08\n",
            "488 1.7969991099570528e-08\n",
            "489 1.6997364519949196e-08\n",
            "490 1.6077745587889724e-08\n",
            "491 1.5208151026537305e-08\n",
            "492 1.4385549873627144e-08\n",
            "493 1.3607706465101157e-08\n",
            "494 1.2871915795548523e-08\n",
            "495 1.2176111749836588e-08\n",
            "496 1.1518002690153487e-08\n",
            "497 1.0895539298540326e-08\n",
            "498 1.0306924452113209e-08\n",
            "499 9.75009687146706e-09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvXT0SI3hCIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "dtype = torch.FloatTensor\n",
        "dtype = torch.cuda.FloatTensor # GPU에서 실행하려면 이 주석을 제거하세요.\n",
        "\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
        "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# 무작위 Tensor 생성\n",
        "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
        "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
        "# 변화도 계산이 필요없음\n",
        "\n",
        "# 얘네는 갱신해서 최적값 찾아야함\n",
        "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
        "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # forward로 y값 예측해보기\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "\n",
        "    # Variable 연산을 사용하여 손실을 계산하고 출력합니다.\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    print(t, loss.data)\n",
        "\n",
        "    # autograde를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를\n",
        "    # 갖는 모든 Variable에 대한 손실의 변화도를 계산합니다. 이후 w1.grad와 w2.grad는\n",
        "    # w1과 w2 각각에 대한 손실의 변화도를 갖는 Variable이 됩니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # 경사하강법(Gradient Descent)을 사용하여 가중치를 갱신합니다; w1.data와\n",
        "    # w2.data는 Tensor이며, w1.grad와 w2.grad는 Variable이고, w1.grad.data와\n",
        "    # w2.grad.data는 Tensor입니다.\n",
        "    w1.data -= learning_rate * w1.grad.data\n",
        "    w2.data -= learning_rate * w2.grad.data\n",
        "\n",
        "    # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
        "    w1.grad.data.zero_()\n",
        "    w2.grad.data.zero_()\n",
        "    \n",
        "    \n",
        "# 1. Variable로 y값 예측(forward)\n",
        "# 2. Variable 연산으로 loss 계산\n",
        "# 3. autograde로 loss의 변화도 계산 (backward)\n",
        "# 4. 가중치 갱신"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6VW2YNDkRLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOPCBp3ajh2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class MyReLU(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    torch.autograd.Function을 상속받아 사용자 정의 autograd 함수를 구현하고,\n",
        "    Tensor 연산을 하는 순전파와 역전파 단계를 구현하겠습니다.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        \"\"\"\n",
        "        순전파 단계에서는 입력을 갖는 Tensor를 받아 출력을 갖는 Tensor를 반환합니다.\n",
        "        ctx는 역전파 연산을 위한 정보를 저장하기 위해 사용하는 Context Object입니다.\n",
        "        ctx.save_for_backward method를 사용하여 역전파 단계에서 사용할 어떠한\n",
        "        객체(object)도 저장(cache)해 둘 수 있습니다.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        역전파 단계에서는 출력에 대한 손실의 변화도를 갖는 Tensor를 받고, 입력에\n",
        "        대한 손실의 변화도를 계산합니다.\n",
        "        \"\"\"\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[input < 0] = 0\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "dtype = torch.FloatTensor\n",
        "# dtype = torch.cuda.FloatTensor # GPU에서 실행하려면 이 주석을 제거하세요.\n",
        "\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
        "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로\n",
        "# 감쌉니다.\n",
        "x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n",
        "y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n",
        "\n",
        "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로\n",
        "# 감쌉니다.\n",
        "w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n",
        "w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # 사용자 정의 함수를 적용하기 위해 Function.apply method를 사용합니다.\n",
        "    # 이를 'relu'라고 이름(alias) 붙였습니다.\n",
        "    relu = MyReLU.apply\n",
        "\n",
        "    # 순전파 단계: Variable 연산을 사용하여 y 값을 예측합니다;\n",
        "    # 사용자 정의 autograd 연산을 사용하여 ReLU를 계산합니다.\n",
        "    y_pred = relu(x.mm(w1)).mm(w2)\n",
        "\n",
        "    # 손실(loss)을 계산하고 출력합니다.\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    print(t, loss.data)\n",
        "\n",
        "    # autograde를 사용하여 역전파 단계를 계산합니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # 경사하강법(Gradient Descent)을 사용하여 가중치를 갱신합니다.\n",
        "    w1.data -= learning_rate * w1.grad.data\n",
        "    w2.data -= learning_rate * w2.grad.data\n",
        "\n",
        "    # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
        "    w1.grad.data.zero_()\n",
        "    w2.grad.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMd1QCuYmUcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# 먼저 연산 그래프를 구성하겠습니다:\n",
        "\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
        "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# 입력과 정답(target) 데이터를 위한 Placeholder를 생성합니다; 이는 우리가 그래프를\n",
        "# 실행할 때 실제 데이터로 채워질 것입니다.\n",
        "x = tf.placeholder(tf.float32, shape=(None, D_in))\n",
        "y = tf.placeholder(tf.float32, shape=(None, D_out))\n",
        "\n",
        "# 가중치를 저장하기 위한 Variable을 생성하고 무작위 데이터로 초기화합니다.\n",
        "# Tensorflow의 Variable은 그래프가 실행되는 동안 그 값이 유지됩니다.\n",
        "w1 = tf.Variable(tf.random_normal((D_in, H)))\n",
        "w2 = tf.Variable(tf.random_normal((H, D_out)))\n",
        "\n",
        "# 순전파 단계: Tensorflow의 Tensor 연산을 사용하여 y 값을 예측합니다.\n",
        "# 이 코드가 어떠한 수치 연산을 실제로 수행하지는 않는다는 것을 유의하세요;\n",
        "# 이 단계에서는 나중에 실행할 연산 그래프를 구성하기만 합니다.\n",
        "h = tf.matmul(x, w1)\n",
        "h_relu = tf.maximum(h, tf.zeros(1))\n",
        "y_pred = tf.matmul(h_relu, w2)\n",
        "\n",
        "# Tensorflow의 Tensor 연산을 사용하여 손실(loss)을 계산합니다.\n",
        "loss = tf.reduce_sum((y - y_pred) ** 2.0)\n",
        "\n",
        "# 손실에 따른 w1, w2의 변화도(Gradient)를 계산합니다.\n",
        "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
        "\n",
        "# 경사하강법(Gradient Descent)을 사용하여 가중치를 갱신합니다. 실제로 가중치를\n",
        "# 갱신하기 위해서는 그래프가 실행될 때 new_w1과 new_w2 계산(evaluate)해야 합니다.\n",
        "# Tensorflow에서 가중치의 값을 갱신하는 작업은 연산 그래프의 일부임을 유의하십시오;\n",
        "# PyTorch에서는 이 작업이 연산 그래프의 밖에서 일어납니다.\n",
        "learning_rate = 1e-6\n",
        "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
        "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
        "\n",
        "# 지금까지 우리는 연산 그래프를 구성하였으므로, 실제로 그래프를 실행하기 위해 이제\n",
        "# Tensorflow 세션(Session)에 들어가보겠습니다.\n",
        "with tf.Session() as sess:\n",
        "    # 그래프를 한 번 실행하여 Variable w1과 w2를 초기화합니다.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # 입력 데이터 x와 정답 데이터 y를 저장하기 위한 NumPy 배열을 생성합니다.\n",
        "    x_value = np.random.randn(N, D_in)\n",
        "    y_value = np.random.randn(N, D_out)\n",
        "    for _ in range(500):\n",
        "        # 그래프를 여러 번 실행합니다. 매번 그래프가 실행할 때마다 feed_dict\n",
        "        # 인자(argument)로 명시하여 x_value를 x에, y_value를 y에 할당(bind)하고자\n",
        "        # 합니다. 또한, 그래프를 실행할 때마다 손실과 new_w1, new_w2 값을\n",
        "        # 계산하려고 합니다; 이러한 Tensor들의 값은 NumPy 배열로 반환됩니다.\n",
        "        loss_value, _, _ = sess.run([loss, new_w1, new_w2],\n",
        "                                    feed_dict={x: x_value, y: y_value})\n",
        "        print(loss_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d4JOrLRnyxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
        "# H는 은닉 계층의 차원이며, D_out은 출력 차원입니다:\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성하고, Variable로\n",
        "# 감쌉니다.\n",
        "x = Variable(torch.randn(N, D_in))\n",
        "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
        "\n",
        "# nn 패키지를 사용하여 모델을 순차적인 계층(Sequence of layers)으로 정의합니다.\n",
        "# nn.Sequential은 다른 모듈들을 포함하는 모듈로, 그 모듈들을 순차적으로 적용하여\n",
        "# 출력을 생성합니다. 각각의 선형(Linear) 모듈은 선형 함수를 사용하여 입력으로부터\n",
        "# 출력을 계산하고, 가중치와 편향(Bias)을 저장하기 위해 내부적인 Variable을 갖습니다.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        ")\n",
        "\n",
        "# 또한, nn 패키지에는 널리 사용하는 손실 함수들에 대한 정의도 포함하고 있습니다;\n",
        "# 여기에서는 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용하겠습니다.\n",
        "loss_fn = torch.nn.MSELoss(size_average=False)\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(500):\n",
        "    # 순전파 단계: 모델에 x를 전달하여 예상하는 y 값을 계산합니다. 모듈 객체는\n",
        "    # __call__ 연산자를 덮어써서(Override) 함수처럼 호출할 수 있게 합니다.\n",
        "    # 그렇게 함으로써 입력 데이터의 Variable을 모듈에 전달하고 출력 데이터의\n",
        "    # Variable을 생성합니다.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # 손실을 계산하고 출력합니다. 예측한 y값과 정답 y를 갖는 Variable들을 전달하고,\n",
        "    # 손실 함수는 손실(loss)을 갖는 Variable을 반환합니다.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    print(t, loss.data[0])\n",
        "\n",
        "    # 역전파 단계를 실행하기 전에 변화도를 0으로 만듭니다.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해서 손실의 변화도를\n",
        "    # 계산합니다. 내부적으로 각 모듈의 매개변수는 requires_grad=True 일 때\n",
        "    # Variable 내에 저장되므로, 이 호출은 모든 모델의 모든 학습 가능한 매개변수의\n",
        "    # 변화도를 계산하게 됩니다.\n",
        "    loss.backward()\n",
        "\n",
        "    # 경사하강법(Gradient Descent)를 사용하여 가중치를 갱신합니다. 각 매개변수는\n",
        "    # Variable이므로 이전에 했던 것과 같이 데이터와 변화도에 접근할 수 있습니다.\n",
        "    for param in model.parameters():\n",
        "        param.data -= learning_rate * param.grad.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKg9PMpAmvsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}